{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Atlassian Data Center Helm Charts","text":"<p>This project contains Helm charts for installing Atlassian's Jira Data Center, Confluence Data Center, Bitbucket Data Center, Bamboo Data Center and Crowd Data Center on Kubernetes.</p> <p>Use the charts to install and operate Data Center products within a Kubernetes cluster of your choice. It can be a managed environment, such as Amazon EKS, Azure Kubernetes Service, Google Kubernetes Engine, or a custom on-premise system.</p>"},{"location":"#support-disclaimer","title":"Support disclaimer","text":"<p>Support Disclaimer</p> <p>Helm is a Kubernetes package manager that orchestrates the provisioning of applications onto existing Kubernetes infrastructure. The requirements for this infrastructure are described in Prerequisites. The Kubernetes cluster remains your responsibility; we do not provide direct support for Kubernetes or the underlying hardware it runs on.</p> <p>If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation after running the <code>helm install</code> command. </p> <p>We don\u2019t officially support the functionality described in the examples or the documented platforms. You should use them for reference only.</p> <p>Read more about what we support and what we don\u2019t. </p> <p>Certain product limitations listed below:</p> <ul> <li>Jira currently has limitations with scaling.</li> <li>Bamboo has a number of limitations, particularly with deployment and clustering.</li> </ul> <p>Read more about these product and platform limitations.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The diagram below provides a high level overview of what a typical deployment might look like when using the Atlassian Data Center Helm charts:</p> <p></p>"},{"location":"#installing-the-helm-charts","title":"Installing the Helm charts","text":"<ul> <li>Prerequisites and setup - everything you need to do before installing the Helm charts</li> <li>Verification - verify the integrity of the Helm charts</li> <li>Installation - the steps to install the Helm charts</li> <li>Migration - what you have to do if you're migrating an existing deployment to Kubernetes</li> </ul>"},{"location":"#additional-content","title":"Additional content","text":"<ul> <li>Operation - how to upgrade applications, scale your cluster, and update resources</li> <li>Configuration - a deep dive into the configuration parameters</li> <li>Platforms support - how to allow support for different platforms</li> <li>Examples - various configuration examples</li> <li>Troubleshooting - how to debug issues with installation</li> </ul>"},{"location":"#product-versions","title":"Product versions","text":"<p>The minimum versions that we support for each product are:</p> Jira DC Confluence DC Bitbucket DC Bamboo DC Crowd DC 8.19 7.13 7.12 8.1 4.3"},{"location":"#feedback","title":"Feedback","text":"<p>If you find any issues, raise a ticket. If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space.</p>"},{"location":"#contributions","title":"Contributions","text":"<p>Contributions are welcome. Find out how to contribute. </p>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 licensed, see license file.</p>"},{"location":"containers/BAMBOO-AGENT/","title":"Bamboo Agent","text":""},{"location":"containers/BAMBOO-AGENT/#overview","title":"Overview","text":"<p>A Bamboo Agent is a service that can run job builds. Each agent has a defined set of capabilities and can run builds only for jobs whose requirements match the agent's capabilities. </p> <p>If you are looking for Bamboo Docker Image it can be found here. To learn more about Bamboo, see: https://www.atlassian.com/software/bamboo</p> <p>This Docker container makes it easy to get a Bamboo Remote Agent up and running. It is intended to be used as a base to  build from, and as such contains limited built-in capabilities:</p> <ul> <li>JDK 11, JDK 17 (from v9.4.0), JDK 21 (from v10.1.0)</li> <li>Git &amp; Git LFS</li> <li>Maven 3</li> <li>Python 3</li> </ul> <p>Using this image as a base, you can create a custom remote agent image with your desired build tools installed. Note that Bamboo Agent Docker Image does not include a Bamboo server.</p> <p>Use docker version &gt;= 20.10.9.</p>"},{"location":"containers/BAMBOO-AGENT/#quick-start","title":"Quick Start","text":"<p>For the <code>BAMBOO_AGENT_HOME</code> directory that is used to store the repository data (amongst other things) we recommend mounting a host directory as a data volume, or via a named volume.</p> <p>To get started you can use a data volume, or named volumes. In this example we'll use named volumes.</p> <p>Run an Agent: <pre><code>docker volume create --name bambooAgentVolume\ndocker run -e BAMBOO_SERVER=http://bamboo.mycompany.com/agentServer/ -v bambooAgentVolume:/var/atlassian/application-data/bamboo-agent --name=\"bambooAgent\" --hostname=\"bambooAgent\" -d atlassian/bamboo-agent-base\n</code></pre></p> <p>The Bamboo remote agent is now available to be approved in your Bamboo administration.</p>"},{"location":"containers/BAMBOO-AGENT/#verbose-container-entrypoint-logging","title":"Verbose container entrypoint logging","text":"<p>During the startup process of the container, various operations and checks are performed to ensure that the application is configured correctly and ready to run. To help in troubleshooting and to provide transparency into this process, you can enable verbose logging. The <code>VERBOSE_LOGS</code> environment variable enables detailed debug messages to the container's log, offering insights into the actions performed by the entrypoint script.</p> <ul> <li><code>VERBOSE_LOGS</code> (default: false)</li> </ul> <p>Set to <code>true</code> to enable detailed debug messages during the container initialization.</p>"},{"location":"containers/BAMBOO-AGENT/#configuration","title":"Configuration","text":"<ul> <li><code>BAMBOO_SERVER</code> (required)</li> </ul> <p>The URL of the Bamboo Server the remote agent should connect to, e.g. <code>http://bamboo.mycompany.com/agentServer/</code></p> <ul> <li><code>SECURITY_TOKEN</code> (default: NONE)</li> </ul> <p>If security token verification is enabled, this value specifies the token required to authenticate to the Bamboo server</p> <ul> <li><code>WRAPPER_JAVA_INITMEMORY</code> (default: 256)</li> </ul> <p>The minimum heap size of the JVM. This value is in MB and should be specified as an integer</p> <ul> <li><code>WRAPPER_JAVA_MAXMEMORY</code> (default: 512)</li> </ul> <p>The maximum heap size of the JVM. This value is in MB and should be specified as an integer</p> <ul> <li><code>IGNORE_SERVER_CERT_NAME</code> (default: false)</li> </ul> <p>Ignore SSL certificate hostname if it's issued to a different host than the one under your Bamboo Base URL hostname</p> <ul> <li><code>ALLOW_EMPTY_ARTIFACTS</code> (default: false)</li> </ul> <p>Allow empty directories to be published as artifacts</p> <ul> <li><code>BAMBOO_AGENT_PERMISSIVE_READINESS</code> (default: unset/false)</li> </ul> <p>If set to 'true', the readiness probe will be more permissive and not expect    the agent to be fully configured, only that the startup wrapper is    running. This is primarily intended for use when deploying agents into    environments where the server may not yet be configured.</p> <ul> <li><code>BAMBOO_AGENT_CLASSPATH_DIR</code> (default: NONE)</li> </ul> <p>If set, agent startup process will copy agent classpath from designated location instead of downloading it from the server.    This can speed up the process and reduce the load on the Bamboo server.</p>"},{"location":"containers/BAMBOO-AGENT/#dedicated-agent-specific-configuration","title":"Dedicated agent specific configuration","text":"<ul> <li><code>AGENT_EPHEMERAL_FOR_KEY</code> (default: NONE)</li> </ul> <p>The value specifies the purpose for spawning the agent. It needs to be a valid ResultKey.</p> <ul> <li><code>KUBE_NUM_EXTRA_CONTAINERS</code> (default: 0) </li> </ul> <p>The number of extra containers that run in parallel with the Bamboo Agent. We make sure these extra containers are run before the Agent kick in.</p> <ul> <li><code>EXTRA_CONTAINERS_REGISTRATION_DIRECTORY</code> (default: /pbc/kube)</li> </ul> <p>The directory where extra containers should register their readiness by creating any file. The image waits for having <code>KUBE_NUM_EXTRA_CONTAINERS</code> number of files inside this directory (if the one exists) before processing further and running the actual agent.</p>"},{"location":"containers/BAMBOO-AGENT/#ephemeral-agent-specific-configuration","title":"Ephemeral agent specific configuration","text":"<ul> <li><code>BAMBOO_EPHEMERAL_AGENT_DATA</code> (default: NONE)</li> </ul> <p>The Bamboo Ephemeral Agents specific configuration. It was designed to pass multiple key-value properties separated by the <code>#</code>. Example: <code>BAMBOO_SERVER=http://localhost#SECURITY_TOKEN=123456789#bamboo.agent.ephemeral.for.key=PROJ-PLAN-JOB1-1</code></p>"},{"location":"containers/BAMBOO-AGENT/#additional-agent-wrapper-properties","title":"Additional agent wrapper properties","text":"<ul> <li><code>BAMBOO_WRAPPER_JAVA_ADDITIONAL_PROPERTIES</code> (default: NONE)</li> </ul> <p>Adds <code>wrapper.java.additional.X</code> entries to the Agent's <code>conf/wrapper.conf</code>. It was designed to pass multiple key-value properties separated by the <code>#</code>. Example: <code>log4j2.configurationFile=/path/to/log4j2.properties#javax.net.ssl.keyStore=/var/atlassian/application-data/bamboo-agent/ssl/bamboo.secure.client.ks</code></p>"},{"location":"containers/BAMBOO-AGENT/#extending-base-image","title":"Extending base image","text":"<p>This Docker image contains only minimal setup to run a Bamboo agent which might not be sufficient to run your builds. If you need additional capabilities you can extend the image to suit your needs.</p> <p>Example of extending the agent base image by Maven and Git: <pre><code>FROM atlassian/bamboo-agent-base:9.6.8\nUSER root\nRUN apt-get update &amp;&amp; \\\n    apt-get install maven -y &amp;&amp; \\\n    apt-get install git -y\n\nUSER ${BAMBOO_USER}\nRUN /bamboo-update-capability.sh \"system.builder.mvn3.Maven 3.3\" /usr/share/maven\nRUN /bamboo-update-capability.sh \"system.git.executable\" /usr/bin/git\n</code></pre></p>"},{"location":"containers/BAMBOO-AGENT/#building-your-own-image","title":"Building your own image","text":"<ul> <li>Clone the Atlassian repository at https://bitbucket.org/atlassian-docker/docker-bamboo-agent-base</li> <li>Modify or replace the Jinja templates   under <code>config</code>; NOTE: The files must have the <code>.j2</code> extensions. However you   don't have to use template variables if you don't wish.</li> <li>Build the new image with e.g: <code>docker build --tag my-bamboo-image --platform linux/amd64,linux/arm64 --build-arg BAMBOO_VERSION=X.Y.Z .</code></li> <li>Optionally push to a registry, and deploy.</li> </ul>"},{"location":"containers/BAMBOO-AGENT/#issue-tracker","title":"Issue tracker","text":"<ul> <li>You can view know issues here.</li> <li>Please contact our support if you encounter any problems with this Dockerfile.</li> </ul>"},{"location":"containers/BAMBOO-AGENT/#supported-jdk-versions-and-base-images","title":"Supported JDK versions and base images","text":"<p>Bamboo Docker images are based on JDK 11, JDK 17 (from Bamboo 9.4), and JDK 21 (from Bamboo 10.1) and generated from the official Eclipse Temurin OpenJDK Docker images and Red Hat Universal Base Images. Starting in Bamboo 9.4, UBI tags are available in 2 formats: <code>&lt;version&gt;-ubi9</code>, <code>&lt;version&gt;-ubi9-jdk17</code>, or <code>&lt;version&gt;-ubi9-jdk21&gt;</code></p> <p>The Docker images follow the Atlassian Support end-of-life policy; images for unsupported versions of the products remain available but will no longer receive updates or fixes.</p> <p>However, Bamboo is an exception to this. Due to the need to support JDK 11 and Kubernetes, we currently only generate new images for Bamboo 9.1 and up. Legacy builds for JDK 8 are still available in Docker Hub, and building custom images is available (see above).</p> <p>Historically, we have also generated other versions of the images, including JDK 8, Alpine, and 'slim' versions of the JDK. These legacy images still exist in Docker Hub, however they should be considered deprecated, and do not receive updates or fixes.</p> <p>If for some reason you need a different version, see \"Building your own image\".</p>"},{"location":"containers/BAMBOO-AGENT/#migration-to-ubi","title":"Migration to UBI","text":"<p>If you have been mounting any files to <code>${JAVA_HOME}</code> directory in <code>eclipse-temurin</code> based container, <code>JAVA_HOME</code> in UBI container is set to <code>/usr/lib/jvm/java-17</code> (JDK17) and <code>/usr/lib/jvm/java-21</code> (JDK21).</p> <p>Also, if you have been mounting and running any custom scripts in the container, UBI-based images may lack some tools and utilities that are available out of the box in eclipse-temurin tags. If that's the case, see Building your own image.</p>"},{"location":"containers/BAMBOO-AGENT/#supported-architectures","title":"Supported architectures","text":"<p>Currently, the Atlassian Docker images are built for the <code>linux/amd64</code> and <code>linux/arm64</code> target platforms; we do not have other architectures on our roadmap at this point. However, the Dockerfiles and support tooling have now had all architecture-specific components removed, so if necessary it is possible to build images for any platform supported by Docker.</p>"},{"location":"containers/BAMBOO-AGENT/#building-on-the-target-architecture","title":"Building on the target architecture","text":"<p>The simplest method of getting a platform image is to build it on a target machine; and specify either <code>linux/amd64</code> or <code>linux/arm64</code>. See Building your own image above.</p> <p>Note: This method is known to work on Mac M1 and AWS ARM64 machines, but has not been extensively tested.</p>"},{"location":"containers/BAMBOO-AGENT/#support","title":"Support","text":"<p>For product support, go to https://support.atlassian.com</p> <p>You can also visit the Atlassian Data Center forum for discussion on running Atlassian Data Center products in containers.</p>"},{"location":"containers/BAMBOO/","title":"Bamboo","text":"<p>Server image deprecation</p> <p>This Docker image has been published as both <code>atlassian/bamboo</code> and <code>atlassian/bamboo-server</code> up until February 15, 2024. Both names refer to the same image. However, post-February 15, 2024, the <code>atlassian/bamboo-server</code> version ceased  receiving updates, including both existing and new tags. If you have been using <code>atlassian/bamboo-server</code>,  switch to the <code>atlassian/bamboo</code> image to ensure access to the latest updates and new tags.</p>"},{"location":"containers/BAMBOO/#overview","title":"Overview","text":"<p>Bamboo is a continuous integration and deployment tool that ties automated builds, tests and releases together in a single workflow.</p> <p>Learn more about Bamboo: https://www.atlassian.com/software/bamboo</p> <p>This Docker container makes it easy to get an instance of Bamboo up and running.</p> <p>Use docker version &gt;= 20.10.10</p>"},{"location":"containers/BAMBOO/#quick-start","title":"Quick Start","text":"<p>For the <code>BAMBOO_HOME</code> directory that is used to store the repository data (amongst other things) we recommend mounting a host directory as a data volume, or via a named volume.</p> <p>Additionally, if running Bamboo in Data Center mode it is required that a shared filesystem is mounted.</p> <p>To get started you can use a data volume, or named volumes. In this example we'll use named volumes. <pre><code>docker volume create --name bambooVolume\ndocker run -v bambooVolume:/var/atlassian/application-data/bamboo --name=\"bamboo\" -d -p 8085:8085 -p 54663:54663 atlassian/bamboo\n</code></pre></p> <p>Bamboo is now available on http://localhost:8085.</p> <p>Please ensure your container has the necessary resources allocated to it. We recommend 2GiB of memory allocated to accommodate the application server. See System Requirements for further information.</p> <p>If you are using <code>docker-machine</code> on Mac OS X, please use <code>open http://$(docker-machine ip default):8085</code> instead.</p>"},{"location":"containers/BAMBOO/#common-settings","title":"Common settings","text":""},{"location":"containers/BAMBOO/#verbose-container-entrypoint-logging","title":"Verbose container entrypoint logging","text":"<p>During the startup process of the container, various operations and checks are performed to ensure that the application is configured correctly and ready to run. To help in troubleshooting and to provide transparency into this process, you can enable verbose logging. The <code>VERBOSE_LOGS</code> environment variable enables detailed debug messages to the container's log, offering insights into the actions performed by the entrypoint script.</p> <ul> <li><code>VERBOSE_LOGS</code> (default: false)</li> </ul> <p>Set to <code>true</code> to enable detailed debug messages during the container initialization.</p>"},{"location":"containers/BAMBOO/#memory-heap-size","title":"Memory / Heap Size","text":"<p>If you need to override Bamboo's default memory allocation, you can control the minimum heap (Xms) and maximum heap (Xmx) via the below environment variables.</p> <ul> <li><code>JVM_MINIMUM_MEMORY</code> (default: 512m)</li> </ul> <p>The minimum heap size of the JVM</p> <ul> <li><code>JVM_MAXIMUM_MEMORY</code> (default: 1024m)</li> </ul> <p>The maximum heap size of the JVM</p>"},{"location":"containers/BAMBOO/#tomcat-and-reverse-proxy-settings","title":"Tomcat and Reverse Proxy Settings","text":"<p>If Bamboo is run behind a reverse proxy server as described here, then you need to specify extra options to make Bamboo aware of the setup. They can be controlled via the below environment variables.</p> <ul> <li><code>ATL_PROXY_NAME</code> (default: NONE)</li> </ul> <p>The reverse proxy's fully qualified hostname. <code>CATALINA_CONNECTOR_PROXYNAME</code>    is also supported for backwards compatibility.</p> <ul> <li><code>ATL_PROXY_PORT</code> (default: NONE)</li> </ul> <p>The reverse proxy's port number via which Bamboo is    accessed. <code>CATALINA_CONNECTOR_PROXYPORT</code> is also supported for backwards    compatibility.</p> <ul> <li><code>ATL_TOMCAT_PORT</code> (default: 8085)</li> </ul> <p>The port for Tomcat/Bamboo to listen on. Depending on your container    deployment method this port may need to be    exposed and published.</p> <ul> <li><code>ATL_TOMCAT_PROTOCOL</code> (default: HTTP/1.1)</li> </ul> <p>The protocol to be used by Tomcat. Bamboo provides additional customized protocols that will support encryption    along with <code>ATL_TOMCAT_BAMBOO_ENCRYPTION_KEY</code>. For more information, see Encrypting passwords in server.</p> <ul> <li><code>ATL_TOMCAT_SCHEME</code> (default: http)</li> </ul> <p>The protocol via which the application is accessed. <code>CATALINA_CONNECTOR_SCHEME</code> is also    supported for backwards compatibility.</p> <ul> <li><code>ATL_TOMCAT_SECURE</code> (default: false)</li> </ul> <p>Set 'true' if <code>ATL_TOMCAT_SCHEME</code> is 'https'. <code>CATALINA_CONNECTOR_SECURE</code> is    also supported for backwards compatibility.</p> <ul> <li><code>ATL_TOMCAT_CONTEXTPATH</code> (default: NONE)</li> </ul> <p>The context path the application is served over. <code>CATALINA_CONTEXT_PATH</code> is    also supported for backwards compatibility.</p> <p>The following Tomcat/Catalina options are also supported. For more information, see https://tomcat.apache.org/tomcat-9.0-doc/config/index.html.</p> <ul> <li><code>ATL_TOMCAT_ACCEPTCOUNT</code> (default: 100)</li> <li><code>ATL_TOMCAT_CONNECTIONTIMEOUT</code> (default: 20000)</li> <li><code>ATL_TOMCAT_ENABLELOOKUPS</code> (default: false)</li> <li><code>ATL_TOMCAT_MAXHTTPHEADERSIZE</code> (default: 8192)</li> <li><code>ATL_TOMCAT_MAXTHREADS</code> (default: 150)</li> <li><code>ATL_TOMCAT_MGMT_PORT</code> (default: 8007)</li> <li><code>ATL_TOMCAT_MINSPARETHREADS</code> (default: 25)</li> <li><code>ATL_TOMCAT_URIENCODING</code> (default: UTF-8)</li> <li><code>ATL_TOMCAT_STUCKTHREADDETECTIONVALVE_THRESHOLD</code> (default: 60)</li> </ul> <p>The standard HTTP connectors (NIO, NIO2 and APR/native) settings</p> <ul> <li><code>ATL_TOMCAT_ADDRESS</code></li> </ul> <p>For servers with more than one IP address, this attribute specifies which     address will be used for listening on the specified port.</p> <ul> <li><code>ATL_TOMCAT_SECRET</code> (default: null)</li> </ul> <p>Only requests from workers with this secret keyword will be accepted. The    default value is null. This attribute must be specified with a non-null,     non-zero length value unless secretRequired is explicitly configured to be false.     If this attribute is configured with a non-null, non-zero length value then the workers    must provide a matching value else the request will be rejected irrespective of the    setting of secretRequired.</p> <ul> <li><code>ATL_TOMCAT_SECRET_REQUIRED</code> (default: false)</li> </ul> <p>If this attribute is true, the AJP Connector will only start if the secret     attribute is configured with a non-null, non-zero length value. This attribute only     controls whether the secret attribute is required to be specified for the AJP Connector    to start. It does not control whether workers are required to provide the secret. The    default value is true. This attribute should only be set to false when the Connector     is used on a trusted network.</p> <ul> <li><code>ATL_TOMCAT_BAMBOO_ENCRYPTION_KEY</code></li> </ul> <p>File which contains encryption key used for Bamboo-specific connectors.</p> <ul> <li><code>ATL_TOMCAT_SSL_ENABLED</code></li> </ul> <p>Set this attribute to <code>true</code> to enable SSL traffic on a connector.</p> <ul> <li><code>ATL_TOMCAT_SSL_PROTOCOL</code></li> </ul> <p>JSSE only.  The SSL protocol(s) to use (a single value may enable multiple protocols    - see the JVM documentation for details).</p> <ul> <li><code>ATL_TOMCAT_SSL_CERTIFICATE_FILE</code></li> </ul> <p>Name of the file that contains the server certificate. The format is PEM-encoded.     Relative paths will be resolved against $CATALINA_BASE.</p> <ul> <li><code>ATL_TOMCAT_SSL_CERTIFICATE_KEY_FILE</code></li> </ul> <p>Name of the file that contains the server private key. The format is PEM-encoded.     The default value is the value of certificateFile and in this case both certificate    and private key have to be in this file (NOT RECOMMENDED). Relative paths will be     resolved against $CATALINA_BASE.</p> <ul> <li><code>ATL_TOMCAT_SSL_PASS</code></li> </ul> <p>The password used to access the private key associated with the server certificate     from the specified file.</p> <ul> <li><code>ATL_TOMCAT_KEYSTORE_FILE</code></li> </ul> <p>JSSE only. The pathname of the keystore file where you have stored the server certificate    and key to be loaded. By default, the pathname is the file .keystore in the operating     system home directory of the user that is running Tomcat.</p> <ul> <li><code>ATL_TOMCAT_KEYSTORE_PASS</code></li> </ul> <p>JSSE only. The password to use to access the keystore containing the server's private    key and certificate. If not specified, a default of changeit will be used.</p> <ul> <li><code>ATL_TOMCAT_KEY_PASS</code></li> </ul> <p>The password used to access the private key associated with the server certificate     from the specified file.</p> <ul> <li><code>ATL_TOMCAT_KEY_ALIAS</code></li> </ul> <p>Specify the key alias to be loaded in case the provided keystore contains multiple     private key aliases.</p> <ul> <li><code>ATL_TOMCAT_CLIENT_AUTH</code></li> </ul> <p>Set to required if you want the SSL stack to require a valid certificate chain from    the client before accepting a connection. Set to optional if you want the SSL stack    to request a client Certificate, but not fail if one isn't presented. Set to optionalNoCA    if you want client certificates to be optional and you don't want Tomcat to check them     against the list of trusted CAs. If the TLS provider doesn't support this option (OpenSSL    does, JSSE does not) it is treated as if optional was specified. A none value (which is     the default) will not require a certificate chain unless the client requests a resource     protected by a security constraint that uses CLIENT-CERT authentication.</p> <ul> <li><code>ATL_TOMCAT_TRUSTSTORE_FILE</code></li> </ul> <p>JSSE only. The trust store file to use to validate client certificates. The default is    the value of the javax.net.ssl.trustStore system property. If neither this attribute nor    the default system property is set, no trust store will be configured. Relative paths will    be resolved against $CATALINA_BASE. A URL may also be used for this attribute.</p> <ul> <li><code>ATL_TOMCAT_TRUSTSTORE_PASS</code></li> </ul> <p>JSSE only. The password to access the trust store. The default is the value of the     javax.net.ssl.trustStorePassword system property. If that property is null, no trust    store password will be configured. If an invalid trust store password is specified,     a warning will be logged and an attempt will be made to access the trust store without    a password which will skip validation of the trust store contents.</p> <ul> <li><code>ATL_TOMCAT_COMPRESSION</code></li> </ul> <p>Enables HTTP compression. The acceptable values for the parameter are:</p> <ul> <li><code>off</code> or <code>0</code> - disabled compression</li> <li><code>on</code> - enabled compression</li> <li><code>force</code> - forces compression in all cases</li> <li><code>numerical integer value</code>,   e.g. <code>100</code> - which is equivalent to <code>on</code>, but specifies the     minimum amount of data before the output is compressed. If the content length is not known    and compression is set to <code>on</code> or more aggressive, the output will also be compressed.</li> </ul> <p>If not specified, compression will remain disabled.</p> <ul> <li><code>ATL_TOMCAT_COMPRESSIBLEMIMETYPE</code></li> </ul> <p>A comma-separated list of MIME types for which HTTP compression may be used.    Only applicable if <code>ATL_TOMCAT_COMPRESSION</code> is set to <code>on</code> or <code>force</code>.    If not specified, this attribute defaults to     <code>text/html,text/xml,text/plain,text/css,text/javascript,application/javascript,application/json,application/xml</code>.</p> <ul> <li><code>ATL_TOMCAT_COMPRESSIONMINSIZE</code></li> </ul> <p>The minimum amount of data before the output is compressed. Only applicable if     <code>ATL_TOMCAT_COMPRESSION</code> is set to <code>on</code> or <code>force</code>. If not specified, this attribute     defaults to <code>2048</code>.</p> <ul> <li><code>ATL_TOMCAT_REQUESTATTRIBUTESENABLED</code></li> </ul> <p>Checks for the existence of request attributes (typically set by the RemoteIpValve and similar)     that should be used to override the values returned by the request for remote address,     remote host, server port and protocol. This property is usually combined with <code>ATL_TOMCAT_TRUSTEDPROXIES</code>    and <code>ATL_TOMCAT_INTERNALPROXIES</code> to show IP address of the remote host instead of the load balancer's.    If not declared, the default value of <code>false</code> will be used.</p> <ul> <li><code>ATL_TOMCAT_TRUSTEDPROXIES</code></li> </ul> <p>A list of IP addresses separated by a pipe character e.g. <code>10.0.9.6|10.0.9.32</code>.    Trusted proxies that appear in the <code>remoteIpHeader</code> will be trusted and will appear     in the <code>proxiesHeader</code> value. By adding a list of Trusted Proxies, Bamboo will remove the     load balancers' IP addresses from Bamboo's view of the incoming connection. This could be desired    in a clustered load balancer architecture where the load balancer address changes depending on     which node proxies the connection, requiring re-approval of Agents.     If not specified, no trusted proxies will be trusted.</p> <ul> <li><code>ATL_TOMCAT_INTERNALPROXIES</code></li> </ul> <p>A list of IP addresses separated by a pipe character e.g. <code>10.0.9.6|10.0.9.32</code>.    Trusted proxies that appear in the <code>remoteIpHeader</code> will be trusted and will not appear     in the <code>proxiesHeader</code> value. By adding a list of Internal Proxies, Bamboo will remove the     load balancers' IP addresses from Bamboo's view of the incoming connection. This could be desired    in a clustered load balancer architecture where the load balancer address changes depending on     which node proxies the connection, requiring re-approval of Agents.    If not specified, no internal proxies will be trusted.</p>"},{"location":"containers/BAMBOO/#access-log-settings","title":"Access Log Settings","text":"<p>You can set the maximum number of days for access logs to be retained before being deleted. The default value of -1 means never delete old files.</p> <ul> <li><code>ATL_TOMCAT_ACCESS_LOGS_MAXDAYS</code> (default: -1)</li> </ul>"},{"location":"containers/BAMBOO/#jvm-configuration","title":"JVM configuration","text":"<p>If you need to pass additional JVM arguments to Bamboo, such as specifying a custom trust store, you can add them via the below environment variable</p> <ul> <li><code>JVM_SUPPORT_RECOMMENDED_ARGS</code></li> </ul> <p>Additional JVM arguments for Bamboo. </p> Example <p><code>docker run -e JVM_SUPPORT_RECOMMENDED_ARGS=-Djavax.net.ssl.trustStore=/var/atlassian/application-data/bamboo/cacerts -v bambooVolume:/var/atlassian/application-data/bamboo --name=\"bamboo\" -d -p 8085:8085 -p 54663:54663 atlassian/bamboo</code></p>"},{"location":"containers/BAMBOO/#bamboo-specific-settings","title":"Bamboo-specific settings","text":"<ul> <li><code>ATL_AUTOLOGIN_COOKIE_AGE</code> (default: 1209600; two weeks, in seconds)</li> </ul> <p>The maximum time a user can remain logged-in with 'Remember Me'.</p> <ul> <li><code>BAMBOO_HOME</code></li> </ul> <p>The Bamboo home directory. This may be on an mounted volume; if so it    should be writable by the user <code>bamboo</code>. See note below about UID    mappings.</p> <ul> <li><code>ATL_BROKER_URI</code> (default: nio://0.0.0.0:54663)</li> </ul> <p>The ActiveMQ Broker URI to listen on for in-bound remote agent communication.</p> <ul> <li><code>ATL_BROKER_CLIENT_URI</code></li> </ul> <p>The ActiveMQ Broker Client URI that remote agents will use to attempt to establish a connection to the ActiveMQ Broker on the Bamboo server.</p> <ul> <li><code>ATL_BAMBOO_SKIP_CONFIG</code> (defaults to <code>False</code>)</li> </ul> <p>If <code>true</code> skip the generation of <code>bamboo.cfg.xml</code>. This is only really useful    for Bamboo versions &gt;= 8.1, which added environment-based configuration (see    next section).</p>"},{"location":"containers/BAMBOO/#optional-configuration-pre-seeding","title":"Optional configuration pre-seeding","text":"<p>Optionally, for new deployments, the setup flow can be skipped by provided the required values via the environment. NOTE: This only work with Bamboo versions &gt;= 8.1.</p> <ul> <li><code>SECURITY_TOKEN</code></li> </ul> <p>The security token to use for server/agent authentication. Additional details    are available here</p> <ul> <li><code>ATL_BAMBOO_DISABLE_AGENT_AUTH</code> (default: false)</li> </ul> <p>Whether to disable agent authentication. Defaults to false.</p> <ul> <li><code>ATL_LICENSE</code></li> </ul> <p>The licence to supply. Licenses can be generated at https://my.atlassian.com.</p> <ul> <li><code>ATL_BASE_URL</code></li> </ul> <p>Bamboo instance Base URL.</p> <ul> <li><code>ATL_ADMIN_USERNAME</code></li> <li><code>ATL_ADMIN_PASSWORD</code></li> <li><code>ATL_ADMIN_FULLNAME</code></li> <li><code>ATL_ADMIN_EMAIL</code></li> </ul> <p>The admin details and credentials.</p> <ul> <li><code>ATL_IMPORT_OPTION</code></li> </ul> <p>Import data from backup file during setup. Default value is 'clean' which skip import step and create Bamboo home     from scratch. If value is 'import' then <code>ATL_IMPORT_PATH</code> should contain path to backup archive.</p> <ul> <li><code>ATL_IMPORT_PATH</code></li> </ul> <p>Full path to backup archive. </p>"},{"location":"containers/BAMBOO/#database-configuration","title":"Database configuration","text":"<p>It is optionally possible to configure the database from the environment, which will pre-fill it for the installation wizard. The password cannot be pre-filled.</p> <p>The following variables are all must all be supplied if using this feature:</p> <ul> <li><code>ATL_JDBC_URL</code></li> </ul> <p>The database URL; this is database-specific.</p> <ul> <li><code>ATL_JDBC_USER</code></li> </ul> <p>The database user to connect as.</p> <ul> <li><code>ATL_JDBC_PASSWORD</code></li> </ul> <p>The database user password to connect with.</p> <ul> <li><code>ATL_DB_TYPE</code></li> </ul> <p>The type of database; valid supported values are:</p> <ul> <li><code>h2</code> - for evaluation needs only</li> <li><code>mssql</code></li> <li><code>mysql</code></li> <li><code>oracle</code></li> <li><code>postgresql</code></li> </ul> MySQL or Oracle JDBC drivers <p>Due to licensing restrictions Bamboo does not ship with a MySQL or Oracle JDBC drivers (since Bamboo 7.0).  To use these databases you will need to copy a suitable driver into the container and restart it.  For example, to copy the MySQL driver into a container named \"bamboo\", you would do the following:</p> <p><code>docker cp mysql-connector-java.x.y.z.jar bambooo:/opt/atlassian/bamboo/lib</code></p> <p><code>docker restart bamboo</code> </p>"},{"location":"containers/BAMBOO/#optional-database-settings","title":"Optional database settings","text":"<p>The following variables are for the database connection pool, and are optional.</p> <ul> <li><code>ATL_DB_POOLMINSIZE</code> (default: 3)</li> <li><code>ATL_DB_POOLMAXSIZE</code> (default: 170)</li> <li><code>ATL_DB_TIMEOUT</code> (default: 120000)</li> <li><code>ATL_DB_CONNECTIONTIMEOUT</code> (default: 30000)</li> <li><code>ATL_DB_LEAKDETECTION</code> (default: 0 / disabled)</li> </ul>"},{"location":"containers/BAMBOO/#container-configuration","title":"Container Configuration","text":"<ul> <li><code>ATL_FORCE_CFG_UPDATE</code> (default: false)</li> </ul> <p>The Docker entrypoint generates application configuration on    first start; not all of these files are regenerated on subsequent    starts. This is deliberate, to avoid race conditions or overwriting manual    changes during restarts and upgrades. However in deployments where    configuration is purely specified through the environment (e.g. Kubernetes)    this behaviour may be undesirable; this flag forces an update of all    generated files.</p> <p>In Bamboo the affected files are: <code>unattended-setup.properties</code>, <code>bamboo.cfg.xml</code></p> <p>See the entrypoint code for the details of how configuration    files are generated.</p> <ul> <li><code>ATL_ALLOWLIST_SENSITIVE_ENV_VARS</code></li> </ul> <p>Define a comma separated list of environment variables containing keywords 'PASS', 'SECRET' or 'TOKEN' to be ignored by the unset function which is executed in the entrypoint. The function uses <code>^</code> regex. For example, if you set <code>ATL_ALLOWLIST_SENSITIVE_ENV_VARS=\"PATH_TO_SECRET_FILE\"</code>, all variables starting with <code>PATH_TO_SECRET_FILE</code> will not be unset.</p> Value exposure on host OS <p>When using this property, the values to sensitive environment variables will be available in clear text on the host OS. As such, this data may be exposed to users or processes running on the host OS.</p> <ul> <li><code>SET_PERMISSIONS</code> (default: true)</li> </ul> <p>Define whether to set home directory permissions on startup. Set to <code>false</code> to disable this behaviour.</p>"},{"location":"containers/BAMBOO/#file-system-permissions-and-user-ids","title":"File system permissions and user IDs","text":"<p>By default, the Bamboo application runs as the user <code>bamboo</code>, with a UID and GID of 2005. Bamboo this UID must have write access to the home directory filesystem. If for some reason a different UID must be used, there are a number of options available:</p> <ul> <li>The Docker image can be rebuilt with a different UID.</li> <li>Under Linux, the UID can be remapped using   user namespace remapping.</li> </ul>"},{"location":"containers/BAMBOO/#upgrade","title":"Upgrade","text":"<p>To upgrade to a more recent version of Bamboo you can simply stop the <code>bamboo</code> container and start a new one based on a more recent image:</p> <pre><code>docker stop bamboo\ndocker rm bamboo\ndocker run ... (See above)\n</code></pre> <p>As your data is stored in the data volume directory on the host it will still be available after the upgrade.</p> <p>Please make sure that you don't accidentally remove the <code>bamboo</code> container and its volumes using the <code>-v</code> option.</p>"},{"location":"containers/BAMBOO/#backup","title":"Backup","text":"<p>For evaluations you can use the built-in database that will store its files in the Bamboo home directory. In that case it is sufficient to create a backup archive of the docker volume.</p> <p>If you're using an external database, you can configure Bamboo to make a backup automatically each night. This will back up the current state, including the database to the <code>bambooVolume</code> docker volume, which can then be archived. Alternatively you can backup the database separately, and continue to create a backup archive of the docker volume to back up the Bamboo Home directory.</p> <p>Read more about data recovery and backups: https://confluence.atlassian.com/display/BAMBOO/Data+and+backups</p>"},{"location":"containers/BAMBOO/#shutdown","title":"Shutdown","text":"<p>Depending on your configuration Bamboo may take a short period to shutdown any active operations to finish before termination. If sending a <code>docker stop</code> this should be taken into account with the <code>--time</code> flag.</p> <p>Alternatively, the script <code>/shutdown-wait.sh</code> is provided, which will initiate a clean shutdown and wait for the process to complete. This is the recommended method for shutdown in environments which provide for orderly shutdown, e.g. Kubernetes via the <code>preStop</code> hook.</p>"},{"location":"containers/BAMBOO/#versioning","title":"Versioning","text":"<p>The <code>latest</code> tag matches the most recent release of Atlassian Bamboo. Thus <code>atlassian/bamboo:latest</code> will use the newest version of Bamboo available.</p> <p>Alternatively you can use a specific major, major.minor, or major.minor.patch version of Bamboo by using a version number tag:</p> <ul> <li><code>atlassian/bamboo:9</code></li> <li><code>atlassian/bamboo:9.6</code></li> <li><code>atlassian/bamboo:9.6.8</code></li> </ul> <p>All versions from 8.0+ are available. Legacy builds for older versions are available but are no longer supported.</p>"},{"location":"containers/BAMBOO/#supported-jdk-versions-and-base-images","title":"Supported JDK versions and base images","text":"<p>Bamboo Docker images are based on JDK 11, JDK 17 (from Bamboo 9.4), and JDK 21 (from Bamboo 10.1) and generated from the official Eclipse Temurin OpenJDK Docker images and  Red Hat Universal Base Images. Starting in Bamboo 9.4, UBI tags are available in 2 formats: <code>&lt;version&gt;-ubi9</code>, <code>&lt;version&gt;-ubi9-jdk17</code>, <code>&lt;version&gt;-ubi9-jdk21</code></p> <p>The Docker images follow the Atlassian Support end-of-life policy; images for unsupported versions of the products remain available but will no longer receive updates or fixes.</p> <p>However, Bamboo is an exception to this. Due to the need to support JDK 11 and Kubernetes, we currently only generate new images for Bamboo 9.1 and up. Legacy builds for JDK 8 are still available in Docker Hub, and building custom images is available (see below).</p> <p>Historically, we have also generated other versions of the images, including JDK 8, Alpine, and 'slim' versions of the JDK. These legacy images still exist in Docker Hub, however they should be considered deprecated, and do not receive updates or fixes.</p> <p>If for some reason you need a different version, see \"Building your own image\".</p>"},{"location":"containers/BAMBOO/#building-your-own-image","title":"Building your own image","text":"<ul> <li>Clone the Atlassian repository at https://bitbucket.org/atlassian-docker/docker-bamboo-server</li> <li>Modify or replace the Jinja templates   under <code>config</code>; NOTE: The files must have the <code>.j2</code> extensions. However, you   don't have to use template variables if you don't wish.</li> <li>Build the new image with e.g: <code>docker build --tag my-bamboo-image --platform linux/amd64,linux/arm64 --build-arg BAMBOO_VERSION=X.Y.Z .</code></li> <li>Optionally push to a registry, and deploy.</li> </ul>"},{"location":"containers/BAMBOO/#migration-to-ubi","title":"Migration to UBI","text":"<p>If you have been mounting any files to <code>${JAVA_HOME}</code> directory in <code>eclipse-temurin</code> based container, <code>JAVA_HOME</code> in UBI container is set to <code>/usr/lib/jvm/java-17</code> (JDK17) and <code>/usr/lib/jvm/java-21</code> (JDK21).</p> <p>Also, if you have been mounting and running any custom scripts in the container, UBI-based images may lack some tools and utilities that are available out of the box in eclipse-temurin tags. If that's the case, see Building your own image.</p>"},{"location":"containers/BAMBOO/#supported-architectures","title":"Supported architectures","text":"<p>Currently, the Atlassian Docker images are built for the <code>linux/amd64</code> and <code>linux/arm64</code> target platforms; we do not have other architectures on our roadmap at this point. However, the Dockerfiles and support tooling have now had all architecture-specific components removed, so if necessary it is possible to build images for any platform supported by Docker.</p>"},{"location":"containers/BAMBOO/#building-on-the-target-architecture","title":"Building on the target architecture","text":"<p>The simplest method of getting a platform image is to build it on a target machine; and specify either <code>linux/amd64</code> or <code>linux/arm64</code>. See Building your own image above.</p> <p>Note: This method is known to work on Mac M1 and AWS ARM64 machines, but has not been extensively tested.</p>"},{"location":"containers/BAMBOO/#troubleshooting","title":"Troubleshooting","text":"<p>These images include built-in scripts to assist in performing common JVM diagnostic tasks.</p>"},{"location":"containers/BAMBOO/#thread-dumps","title":"Thread dumps","text":"<p><code>/opt/atlassian/support/thread-dumps.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of thread dumps from the containerized application. For example:</p> <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh\n</code></pre> <p>By default this script will collect 10 thread dumps at 5 second intervals. This can be overridden by passing a custom value for the count and interval, by using <code>-c</code> / <code>--count</code> and <code>-i</code> / <code>--interval</code> respectively. For example, to collect 20 thread dumps at 3 second intervals:</p> <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh --count 20 --interval 3\n</code></pre> <p>Thread dumps will be written to <code>$APP_HOME/thread_dumps/&lt;date&gt;</code>.</p> Disable capturing output from top run <p>By default this script will also capture output from top run in 'Thread-mode'.  This can be disabled by passing <code>-n</code> / <code>--no-top</code></p>"},{"location":"containers/BAMBOO/#heap-dump","title":"Heap dump","text":"<p><code>/opt/atlassian/support/heap-dump.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of a heap dump from the containerized application. For example:</p> <pre><code>docker exec my_container /opt/atlassian/support/heap-dump.sh\n</code></pre> <p>A heap dump will be written to <code>$APP_HOME/heap.bin</code>. If a file already exists at this location, use <code>-f</code> / <code>--force</code> to overwrite the existing heap dump file.</p>"},{"location":"containers/BAMBOO/#manual-diagnostics","title":"Manual diagnostics","text":"<p>The <code>jcmd</code> utility is also included in these images and can be used by starting a <code>bash</code> shell in the running container:</p> <pre><code>docker exec -it my_container /bin/bash\n</code></pre>"},{"location":"containers/BAMBOO/#support","title":"Support","text":"<p>For product support, go to https://support.atlassian.com</p> <p>You can also visit the Atlassian Data Center forum for discussion on running Atlassian Data Center products in containers.</p>"},{"location":"containers/BAMBOO/#changelog","title":"Changelog","text":"<p>For a detailed list of changes to the Docker image configuration see the Git commit history.</p>"},{"location":"containers/BAMBOO/#license","title":"License","text":"<p>Copyright \u00a9 2020 Atlassian Corporation Pty Ltd. Licensed under the Apache License, Version 2.0.</p>"},{"location":"containers/BITBUCKET-MESH/","title":"Bitbucket Mesh","text":""},{"location":"containers/BITBUCKET-MESH/#overview","title":"Overview","text":"<p>Bitbucket Data Center is an on-premises source code management solution for Git that's secure, fast, and enterprise grade. Create and manage repositories, set up fine-grained permissions, and collaborate on code - all with the flexibility of your servers.</p> <p>Bitbucket Mesh is an optional scalability extension for Bitbucket. For more information see https://confluence.atlassian.com/bitbucketserver/bitbucket-mesh-1128304351.html.</p> <p>This Docker image is published as <code>atlassian/bitbucket-mesh</code>.</p> <p>This Docker container makes it easy to get Mesh nodes for a Bitbucket Data Center up and running. It will only work in conjunction with a Bitbucket Data Center server.</p> <p>For full documentation on running Bitbucket Data Center with Mesh nodes, see the Bitbucket documentation.</p> <p> If running this image in a production environment, we strongly recommend you run this image using a specific version tag instead of latest. This is because the image referenced by the latest tag changes often and we cannot guarantee that it will be backwards compatible. </p> <p>Use docker version &gt;= 20.10.10</p>"},{"location":"containers/BITBUCKET-MESH/#quick-start","title":"Quick Start","text":"<p>For the <code>MESH_HOME</code> directory that is used to store the repository data (amongst other things) we recommend mounting a host directory as a data volume, or via a named volume.</p> <p>Volume permissions are managed by entry scripts. To get started you can use a data volume, or named volumes. In this example we'll use named volumes. <pre><code>docker volume create --name bitbucketMeshVolume\ndocker run -v bitbucketMeshVolume:/var/atlassian/application-data/mesh --name=\"bitbucket-mesh\" -d -p 7777:7777 atlassian/bitbucket-mesh\n</code></pre></p> <p>Note that this command can substitute folder paths with named volumes.</p> <p>Please ensure your container has the necessary resources allocated to it. We recommend 2GiB of memory allocated to accommodate both the application server and the git processes. See Supported Platforms for further information.</p>"},{"location":"containers/BITBUCKET-MESH/#common-settings","title":"Common settings","text":""},{"location":"containers/BITBUCKET-MESH/#verbose-container-entrypoint-logging","title":"Verbose container entrypoint logging","text":"<p>During the startup process of the container, various operations and checks are performed to ensure that the application is configured correctly and ready to run. To help in troubleshooting and to provide transparency into this process, you can enable verbose logging. The <code>VERBOSE_LOGS</code> environment variable enables detailed debug messages to the container's  log, offering insights into the actions performed by the entrypoint script.</p> <ul> <li><code>VERBOSE_LOGS</code> (default: false)</li> </ul> <p>Set to <code>true</code> to enable detailed debug messages during the container initialization.</p>"},{"location":"containers/BITBUCKET-MESH/#mesh-node-configuration","title":"Mesh Node Configuration","text":"<ul> <li><code>MESH_HOME</code></li> </ul> <p>The home directory used by the Mesh node. This should have full read/write   permissions and be persistent \u2013 your Bitbucket Mesh data will be stored here.</p> <ul> <li><code>GRPC_SERVER_PORT</code> (default: 7777)</li> </ul> <p>The port used by the Mesh node to communicate with the server.</p>"},{"location":"containers/BITBUCKET-MESH/#mesh-node-jvm-configuration","title":"Mesh Node JVM Configuration","text":"<p>If you need to override the Mesh node's default memory configuration or pass additional JVM arguments, use the environment variables below</p> <ul> <li><code>JVM_MINIMUM_MEMORY</code> (default: 512m)</li> </ul> <p>The minimum heap size of the JVM</p> <ul> <li><code>JVM_MAXIMUM_MEMORY</code> (default: 1024m)</li> </ul> <p>The maximum heap size of the JVM</p> <ul> <li><code>JVM_SUPPORT_RECOMMENDED_ARGS</code> (default: NONE)</li> </ul> <p>Additional JVM arguments for the Mesh node , such as a custom Java Trust Store</p>"},{"location":"containers/BITBUCKET-MESH/#jmx-monitoring","title":"JMX Monitoring","text":"<p>JMX monitoring can be enabled with <code>JMX_ENABLED=true</code>. Information on additional settings and available metrics is available in the Bitbucket JMX documentation.</p>"},{"location":"containers/BITBUCKET-MESH/#other-settings","title":"Other settings","text":"<p>As well as the above settings, all settings that are available in the mesh.properties file can also be provided via Docker environment variables. For a full explanation of converting Bitbucket properties into environment variables see the relevant Spring Boot documentation.</p> <p>To translate a property name into an environment variable:</p> <ul> <li>dot <code>.</code> becomes underscore <code>_</code></li> <li>dash <code>-</code> becomes underscore <code>_</code></li> <li>Example: <code>this.new-property</code> becomes <code>THIS_NEW_PROPERTY</code></li> </ul>"},{"location":"containers/BITBUCKET-MESH/#container-configuration","title":"Container Configuration","text":"<ul> <li><code>SET_PERMISSIONS</code> (default: true)</li> </ul> <p>Define whether to set home directory permissions on startup. Set to <code>false</code> to disable   this behaviour.</p>"},{"location":"containers/BITBUCKET-MESH/#home-directory-and-user-ids","title":"Home directory and user IDs","text":"<p>By default the Bitbucket application runs as the user <code>bitbucket</code>, with a UID and GID of 2003. If for some reason a different UID must be used, there are a number of options available:</p> <ul> <li>The Docker image can be rebuilt with a different UID.</li> <li>Under Linux, the UID can be remapped using   user namespace remapping.</li> </ul>"},{"location":"containers/BITBUCKET-MESH/#shutdown","title":"Shutdown","text":"<p>The Mesh node allows a configurable grace period for active operations to finish before termination; by default this is 30s. If sending a <code>docker stop</code> this should be taken into account with the <code>--time</code> flag.</p> <p>Alternatively, the script <code>/shutdown-wait.sh</code> is provided, which will initiate a clean shutdown and wait for the process to complete. This is the recommended method for shutdown in environments which provide for orderly shutdown, e.g. Kubernetes via the <code>preStop</code> hook.</p>"},{"location":"containers/BITBUCKET-MESH/#versioning","title":"Versioning","text":"<p>You should ensure you are running the appropriate Mesh version for your Bitbucket Data Center server. A support matrix is available here: Bitbucket Mesh compatibility matrix.</p>"},{"location":"containers/BITBUCKET-MESH/#supported-jdk-versions-and-base-images","title":"Supported JDK versions and base images","text":"<p>The Bitbucket Mesh Docker images support the following JDK versions:</p> <ul> <li>JDK 11: Supported for Mesh versions 2.0.1 to 3.0.0</li> <li>JDK 17: Supported for Mesh versions 2.4 to 3.4.5</li> <li>JDK 21: Supported for Mesh versions 3.4.5 and above (required for Mesh 4.x)</li> </ul> <p>Images are generated from the official Eclipse Temurin OpenJDK Docker images.</p> <p>Starting from 2.4, UBI based tags are published as well. UBI tags are available in the following formats: * For JDK 17: <code>&lt;version&gt;-ubi9</code> and <code>&lt;version&gt;-ubi9-jdk17</code> * For JDK 21: <code>&lt;version&gt;-ubi9</code> and <code>&lt;version&gt;-ubi9-jdk21</code> (from version 3.4.5)</p> <p>The Docker images follow the Atlassian Support end-of-life policy; images for unsupported versions of the products remain available but will no longer receive updates or fixes.</p> <p>If for some reason you need a different version, see \"Building your own image\"</p>"},{"location":"containers/BITBUCKET-MESH/#migration-to-ubi","title":"Migration to UBI","text":"<p>If you have been mounting any files to <code>${JAVA_HOME}</code> directory in <code>eclipse-temurin</code> based container, <code>JAVA_HOME</code> in UBI JDK17 container is set to <code>/usr/lib/jvm/java-17</code>.</p> <p>Also, if you have been mounting and running any custom scripts in the container, UBI-based images may lack some tools and utilities that are available out of the box in eclipse-temurin tags. If that's the case, see \"Building your own image\".</p>"},{"location":"containers/BITBUCKET-MESH/#building-your-own-image","title":"Building your own image","text":"<ul> <li>Clone the Atlassian repository at https://bitbucket.org/atlassian-docker/docker-atlassian-bitbucket-mesh/</li> <li>Modify or replace the Jinja templates   under <code>config</code>; NOTE: The files must have the <code>.j2</code> extensions. However, you   don't have to use template variables if you don't wish.</li> <li>Build the new image with e.g: <code>docker build --tag my-bitbucket-mesh-image --build-arg MESH_VERSION=1.x.x .</code></li> <li>Optionally push to a registry, and deploy.</li> </ul>"},{"location":"containers/BITBUCKET-MESH/#supported-architectures","title":"Supported architectures","text":"<p>Currently, Bitbucket Mesh container images are built for the <code>linux/amd64</code> and <code>linux/arm64</code> target platforms. The Dockerfiles and support tooling have now had all architecture-specific components removed, so if necessary it is possible to build images for any platform supported by OCI-compliant container runtimes.</p>"},{"location":"containers/BITBUCKET-MESH/#building-on-the-target-architecture","title":"Building on the target architecture","text":"<p>The simplest method of getting a platform image is to build it on a target machine; see \"Building your own image\" above.</p> <p>Note: This method is known to work on Mac M1 and AWS ARM64 machines, but has not be extensively tested.</p>"},{"location":"containers/BITBUCKET-MESH/#troubleshooting","title":"Troubleshooting","text":"<p>These images include built-in scripts to assist in performing common JVM diagnostic tasks.</p>"},{"location":"containers/BITBUCKET-MESH/#thread-dumps","title":"Thread dumps","text":"<p><code>/opt/atlassian/support/thread-dumps.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of thread dumps from the containerized application. For example: <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh\n</code></pre></p> <p>By default, this script will collect 10 thread dumps at 5 second intervals. This can be overridden by passing a custom value for the count and interval, by using <code>-c</code> / <code>--count</code> and <code>-i</code> / <code>--interval</code> respectively. For example, to collect 20 thread dumps at 3 second intervals: <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh --count 20 --interval 3\n</code></pre></p> <p>Thread dumps will be written to <code>$APP_HOME/thread_dumps/&lt;date&gt;</code>.</p> Disable capturing output from top run <p>By default this script will also capture output from top run in 'Thread-mode'. This can be disabled by passing <code>-n</code> / <code>--no-top</code></p>"},{"location":"containers/BITBUCKET-MESH/#heap-dump","title":"Heap dump","text":"<p><code>/opt/atlassian/support/heap-dump.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of a heap dump from the containerized application. For example: <pre><code>docker exec my_container /opt/atlassian/support/heap-dump.sh\n</code></pre></p> <p>A heap dump will be written to <code>$APP_HOME/heap.bin</code>. If a file already exists at this location, use <code>-f</code> / <code>--force</code> to overwrite the existing heap dump file.</p>"},{"location":"containers/BITBUCKET-MESH/#manual-diagnostics","title":"Manual diagnostics","text":"<p>The <code>jcmd</code> utility is also included in these images and can be used by starting a <code>bash</code> shell in the running container: <pre><code>docker exec -it my_container /bin/bash\n</code></pre></p>"},{"location":"containers/BITBUCKET-MESH/#support","title":"Support","text":"<p>For product support, go to support.atlassian.com</p> <p>You can also visit the Atlassian Data Center forum for discussion on running Atlassian Data Center products in containers.</p>"},{"location":"containers/BITBUCKET-MESH/#changelog","title":"Changelog","text":"<p>For a detailed list of changes to the Docker image configuration see the Git commit history.</p>"},{"location":"containers/BITBUCKET-MESH/#license","title":"License","text":"<p>Copyright \u00a9 2022 Atlassian Corporation Pty Ltd. Licensed under the Apache License, Version 2.0.</p>"},{"location":"containers/BITBUCKET/","title":"Bitbucket","text":"<p>Server image deprecation</p> <p>This Docker image has been published as both <code>atlassian/bitbucket</code> and <code>atlassian/bitbucket-server</code> up until February 15, 2024. Both names refer to the same image. However, post-February 15, 2024, the <code>atlassian/bitbucket-server</code> version ceased  receiving updates, including both existing and new tags. If you have been using <code>atlassian/bitbucket-server</code>,  switch to the <code>atlassian/bitbucket</code> image to ensure access to the latest updates and new tags.</p>"},{"location":"containers/BITBUCKET/#overview","title":"Overview","text":"<p>Bitbucket Server is an on-premises source code management solution for Git that's secure, fast, and enterprise grade. Create and manage repositories, set up fine-grained permissions, and collaborate on code - all with the flexibility of your servers.</p> <p>Learn more about Bitbucket Server: https://www.atlassian.com/software/bitbucket/server</p> <p>This Docker container makes it easy to get an instance of Bitbucket up and running.</p> Embedded OpenSearch <p>Only Bitbucket versions &lt; 10 include an embedded search server. For backwards-compatibility, by default the image will start both Bitbucket and an embedded OpenSearch.  However, this is not a recommended configuration, especially in a clustered environment, and has known issues with shutdown. Instead, we recommend running a separate OpenSearch instance (possibly in another Docker container);  see below for instructions on connecting to an external OpenSearch cluster.</p> <p> If running this image in a production environment, we strongly recommend you run this image using a specific version tag instead of latest. This is because the image referenced by the latest tag changes often and we cannot guarantee that it will be backwards compatible. </p> <p>Use docker version &gt;= 20.10.10</p>"},{"location":"containers/BITBUCKET/#quick-start","title":"Quick Start","text":"<p>For the <code>BITBUCKET_HOME</code> directory that is used to store the repository data (amongst other things) we recommend mounting a host directory as a data volume, or via a named volume.</p> <p>Additionally, if running Bitbucket in Data Center mode it is required that a shared filesystem is mounted.</p> <p>Volume permission is managed by entry scripts. To get started you can use a data volume, or named volumes. In this example we'll use named volumes. <pre><code>docker volume create --name bitbucketVolume\ndocker run -v bitbucketVolume:/var/atlassian/application-data/bitbucket --name=\"bitbucket\" -d -p 7990:7990 -p 7999:7999 atlassian/bitbucket\n</code></pre> Note that this command can substitute folder paths with named volumes. Start Atlassian Bitbucket Server: <pre><code>docker run -v /data/bitbucket:/var/atlassian/application-data/bitbucket --name=\"bitbucket\" -d -p 7990:7990 -p 7999:7999 atlassian/bitbucket\n</code></pre></p> <p>Bitbucket is now available on http://localhost:7990.</p> <p>Please ensure your container has the necessary resources allocated to it. We recommend 2GiB of memory allocated to accommodate both the application server and the git processes. See Supported Platforms for further information.</p> If you are using <code>docker-machine</code> on Mac OS X, please use <code>open http://$(docker-machine ip default):7990</code> instead."},{"location":"containers/BITBUCKET/#common-settings","title":"Common settings","text":""},{"location":"containers/BITBUCKET/#verbose-container-entrypoint-logging","title":"Verbose container entrypoint logging","text":"<p>During the startup process of the container, various operations and checks are performed to ensure that the application is configured correctly and ready to run. To help in troubleshooting and to provide transparency into this process, you can enable verbose logging. The <code>VERBOSE_LOGS</code> environment variable enables detailed debug messages to the container's log, offering insights into the actions performed by the entrypoint script.</p> <ul> <li><code>VERBOSE_LOGS</code> (default: false)</li> </ul> <p>Set to <code>true</code> to enable detailed debug messages during the container initialization.</p>"},{"location":"containers/BITBUCKET/#reverse-proxy-settings","title":"Reverse Proxy Settings","text":"<p>If Bitbucket is run behind a reverse proxy server as described here, then you need to specify extra options to make Bitbucket aware of the setup. They can be controlled via the below environment variables.</p> <ul> <li><code>SERVER_PROXY_NAME</code> (default: NONE)</li> </ul> <p>The reverse proxy's fully qualified hostname.</p> <ul> <li><code>SERVER_PROXY_PORT</code> (default: NONE)</li> </ul> <p>The reverse proxy's port number via which bitbucket is accessed.</p> <ul> <li><code>SERVER_SCHEME</code> (default: http)</li> </ul> <p>The protocol via which bitbucket is accessed. </p> <p>In certain cloud environments (specifically Kubernetes, Heroku and Cloud Foundry), this setting    will be superseded by the value of the <code>X-Forwarded-Proto</code> request header if sent by a ingress or load balancer.     See <code>SERVER_FORWARD_HEADERS_STRATEGY</code> below to alter this behaviour.</p> <ul> <li><code>SERVER_SECURE</code> (default: false)</li> </ul> <p>Set 'true' if SERVER_SCHEME is 'https'. </p> <ul> <li><code>SERVER_FORWARD_HEADERS_STRATEGY</code> (default: NATIVE in the specified cloud environments, NONE otherwise)</li> </ul> <p>Can be explicitly set to a value of <code>NONE</code> if deploying to a cloud environment (specifically Kubernetes, Heroku and Cloud Foundry) and the preference is for <code>SERVER_SCHEME</code>     to be used over the value of the <code>X-Forwarded-Proto</code> request header. A value of NONE will cause X-Forwarded-* headers to no longer take priority when determining the     origin of a request, which means the system will return to the default expected state.</p>"},{"location":"containers/BITBUCKET/#jvm-configuration-bitbucket-server-50-only","title":"JVM Configuration (Bitbucket Server 5.0 + only)","text":"<p>If you need to override Bitbucket Server's default memory configuration or pass additional JVM arguments, use the environment variables below</p> <ul> <li><code>JVM_MINIMUM_MEMORY</code> (default: 512m)</li> </ul> <p>The minimum heap size of the JVM</p> <ul> <li><code>JVM_MAXIMUM_MEMORY</code> (default: 1024m)</li> </ul> <p>The maximum heap size of the JVM</p> <ul> <li><code>JVM_SUPPORT_RECOMMENDED_ARGS</code> (default: NONE)</li> </ul> <p>Additional JVM arguments for Bitbucket Server, such as a custom Java Trust Store</p>"},{"location":"containers/BITBUCKET/#application-mode-settings-bitbucket-server-50-only","title":"Application Mode Settings (Bitbucket Server 5.0 + only)","text":"<p>This docker image can be run as a Smart Mirror or as part of a Data Center cluster.  You can specify the following properties to start Bitbucket as a mirror or as a Data Center node:</p> <ul> <li><code>SEARCH_ENABLED</code> (default: true)</li> </ul> <p>Note: This property is not applicable for Bitbucket 10.0.0 onwards, as embedded search was removed in Bitbucket 10.   Set 'false' to prevent OpenSearch (previously Elasticsearch) from starting in the   container. This should be used if OpenSearch is running remotely, e.g. for if Bitbucket   is running in a Data Center cluster. You may also use <code>ELASTICSEARCH_ENABLED</code> to   set this property, however this is deprecated in favor of <code>SEARCH_ENABLED</code>.</p> <ul> <li><code>APPLICATION_MODE</code> (default: default)</li> </ul> <p>The mode Bitbucket will run in. This can be set to 'mirror' to start    Bitbucket as a Smart Mirror. This will also disable OpenSearch even if    <code>SEARCH_ENABLED</code> has not been set to 'false'.</p>"},{"location":"containers/BITBUCKET/#database-configuration","title":"Database Configuration","text":"<p>To configure the database automatically on first run, you can provide the following settings:</p> <ul> <li><code>JDBC_DRIVER</code></li> <li><code>JDBC_URL</code></li> <li><code>JDBC_USER</code></li> <li><code>JDBC_PASSWORD</code></li> </ul> <p>Note: Due to licensing restrictions Bitbucket does not ship with a MySQL or Oracle JDBC drivers. To use these databases you will need to copy a suitable driver into the container and restart it. For example, to copy the MySQL driver into a container named \"bitbucket\", you would do the following:</p> <pre><code>docker cp mysql-connector-java.x.y.z.jar bitbucket:/var/atlassian/application-data/bitbucket/lib\ndocker restart bitbucket\n</code></pre> <p>For more information see Connecting Bitbucket Server to an external database.</p>"},{"location":"containers/BITBUCKET/#jdbc-password-encryption","title":"JDBC password encryption","text":"<p>Starting from Bitbucket <code>8.13</code> the <code>JDBC</code> password can now be managed via AWS Secrets Manager. For example, a Bitbucket node with a PostgreSQL database and <code>JDBC</code> password management via AWS Secrets Manager might look like:</p> <pre><code>docker run \\\n    -e JDBC_DRIVER=org.postgresql.Driver \\\n    -e JDBC_USER=atlbitbucket \\\n    -e JDBC_PASSWORD=\"{\\\"region\\\":\\\"us-east-1\\\",\\\"secretId\\\":\\\"mysecret\\\",\\\"secretPointer\\\":\\\"/password\\\"}\" \\\n    -e JDBC_PASSWORD_DECRYPTER_CLASSNAME=\"com.atlassian.secrets.store.aws.AwsSecretsManagerStore\" \\\n    -e JDBC_URL=jdbc:postgresql://my.database.host:5432/bitbucket \\\n    -v /data/bitbucket-shared:/var/atlassian/application-data/bitbucket/shared \\\n    --name=\"bitbucket\" \\\n    -d -p 7990:7990 -p 7999:7999 \\\n    atlassian/bitbucket\n</code></pre> <p>Of note here are the two properties; <code>JDBC_PASSWORD</code> and <code>JDBC_PASSWORD_DECRYPTER_CLASSNAME</code> and their corresponding values, where the Secrets Manager coordinates and decryption class name are supplied respectively. </p>"},{"location":"containers/BITBUCKET/#other-settings","title":"Other settings","text":"<p>As well as the above settings, all settings that are available in the bitbucket.properties file can also be provided via Docker environment variables. For a full explanation of converting Bitbucket properties into environment variables see the relevant Spring Boot documentation.</p> <p>To translate a property name into an environment variable:</p> <ul> <li>dot <code>.</code> becomes underscore <code>_</code></li> <li>dash <code>-</code> becomes underscore <code>_</code></li> <li>Example: <code>this.new-property</code> becomes <code>THIS_NEW_PROPERTY</code></li> </ul> <p>For example, a full command-line for a Bitbucket node with a PostgreSQL database, and an external OpenSearch instance might look like:</p> <pre><code>docker network create --driver bridge --subnet=172.18.0.0/16 myBitbucketNetwork\ndocker run --network=myBitbucketNetwork --ip=172.18.1.1 \\\n    -e SEARCH_ENABLED=false \\\n    -e JDBC_DRIVER=org.postgresql.Driver \\\n    -e JDBC_USER=atlbitbucket \\\n    -e JDBC_PASSWORD=MYPASSWORDSECRET \\\n    -e JDBC_URL=jdbc:postgresql://my.database.host:5432/bitbucket \\\n    -e PLUGIN_SEARCH_CONFIG_BASEURL=http://my.opensearch.host \\\n    -v /data/bitbucket-shared:/var/atlassian/application-data/bitbucket/shared \\\n    --name=\"bitbucket\" \\\n    -d -p 7990:7990 -p 7999:7999 \\\n    atlassian/bitbucket\n</code></pre>"},{"location":"containers/BITBUCKET/#cluster-settings","title":"Cluster settings","text":"<p>If running a clustered Bitbucket DC instance, the cluster settings are specified with <code>HAZELCAST_*</code> environment variables. The main ones to be aware of are:</p> <ul> <li><code>HAZELCAST_PORT</code> (<code>hazelcast.port</code>)</li> <li><code>HAZELCAST_GROUP_NAME</code> (<code>hazelcast.group.name</code>)</li> <li><code>HAZELCAST_GROUP_PASSWORD</code> (<code>hazelcast.group.password</code>)</li> </ul> <p>Each clustering type (e.g. AWS/Azure/Multicast/TCP) has its own settings. For more information on clustering Bitbucket, and other properties see Clustering with Bitbucket Data Center and Clustering with Bitbucket Data Center.</p> Out-of-scope network configuration <p>The underlying network should be configured to support the clustering type you are using. How to do this depends on the container management technology, and is beyond the scope of this documentation.</p>"},{"location":"containers/BITBUCKET/#jmx-monitoring","title":"JMX Monitoring","text":"<p>JMX monitoring can be enabled with <code>JMX_ENABLED=true</code>. Information on additional settings and available metrics is available in the Bitbucket JMX documentation.</p>"},{"location":"containers/BITBUCKET/#container-configuration","title":"Container Configuration","text":"<ul> <li><code>SET_PERMISSIONS</code> (default: true)</li> </ul> <p>Define whether to set home directory permissions on startup. Set to <code>false</code> to disable    this behaviour.</p>"},{"location":"containers/BITBUCKET/#shared-directory-and-user-ids","title":"Shared directory and user IDs","text":"<p>By default the Bitbucket application runs as the user <code>bitbucket</code>, with a UID and GID of 2003. Consequently this UID must have write access to the shared filesystem. If for some reason a different UID must be used, there are a number of options available:</p> <ul> <li>The Docker image can be rebuilt with a different UID.</li> <li>Under Linux, the UID can be remapped using   user namespace remapping.</li> </ul>"},{"location":"containers/BITBUCKET/#upgrade","title":"Upgrade","text":"<p>To upgrade to a more recent version of Bitbucket Server you can simply stop the <code>bitbucket</code> container and start a new one based on a more recent image:</p> <pre><code>docker stop bitbucket\ndocker rm bitbucket\ndocker pull atlassian/bitbucket:&lt;desired_version&gt;\ndocker run ... (See above)\n</code></pre> <p>As your data is stored in the data volume directory on the host it will still be available after the upgrade.</p> <p>Please make sure that you don't accidentally remove the <code>bitbucket</code> container and its volumes using the <code>-v</code> option.</p>"},{"location":"containers/BITBUCKET/#backup","title":"Backup","text":"<p>For evaluations you can use the built-in database that will store its files in the Bitbucket Server home directory. In that case it is sufficient to create a backup archive of the directory on the host that is used as a volume (<code>/data/bitbucket</code> in the example above).</p> <p>The Bitbucket Server Backup Client is currently not supported in the Docker setup. You can however use the Bitbucket Server DIY Backup approach in case you decided to use an external database.</p> <p>Read more about data recovery and backups: https://confluence.atlassian.com/display/BitbucketServer/Data+recovery+and+backups</p>"},{"location":"containers/BITBUCKET/#shutdown","title":"Shutdown","text":"<p>Bitbucket allows a configurable grace period for active operations to finish before termination; by default this is 30s. If sending a <code>docker stop</code> this should be taken into account with the <code>--time</code> flag.</p> <p>Alternatively, the script <code>/shutdown-wait.sh</code> is provided, which will initiate a clean shutdown and wait for the process to complete. This is the recommended method for shutdown in environments which provide for orderly shutdown, e.g. Kubernetes via the <code>preStop</code> hook.</p>"},{"location":"containers/BITBUCKET/#versioning","title":"Versioning","text":"<p>The <code>latest</code> tag matches the most recent version of this repository. Thus using <code>atlassian/bitbucket:latest</code> or <code>atlassian/bitbucket</code> will ensure you are running the most up to date version of this image.</p> <p>Alternatively, you can use a specific minor version of Bitbucket Server by using a version number tag: <code>atlassian/bitbucket:6</code>. This will install the latest <code>6.x.x</code> version that is available.</p>"},{"location":"containers/BITBUCKET/#supported-jdk-versions-and-base-images","title":"Supported JDK versions and base images","text":"<p>All the Atlassian Docker images are now JDK11 and JDK17 (starting from 8.8 version), and generated from the official Eclipse Temurin OpenJDK Docker images.</p> <p>Starting from 8.18 UBI based tags are published as well. UBI tags are available in 2 formats: <code>&lt;version&gt;-ubi9</code> and <code>&lt;version&gt;-ubi9-jdk17</code>.</p> <p>The Docker images follow the Atlassian Support end-of-life policy; images for unsupported versions of the products remain available but will no longer receive updates or fixes.</p> <p>Historically, we have also generated other versions of the images, including JDK8, Alpine, and 'slim' versions of the JDK. These legacy images still exist in Docker Hub, however they should be considered deprecated, and do not receive updates or fixes.</p> <p>If for some reason you need a different version, see \"Building your own image\"</p>"},{"location":"containers/BITBUCKET/#migration-to-ubi","title":"Migration to UBI","text":"<p>If you have been mounting any files to <code>${JAVA_HOME}</code> directory in <code>eclipse-temurin</code> based container, JAVA_HOME in UBI JDK17 container is set to <code>/usr/lib/jvm/java-17</code>.</p> <p>Also, if you have been mounting and running any custom scripts in the container, UBI-based images may lack some tools and utilities that are available out of the box in <code>eclipse-temurin</code> tags. If that's the case, see Building your own image.</p>"},{"location":"containers/BITBUCKET/#building-your-own-image","title":"Building your own image","text":"<ul> <li>Clone the Atlassian repository at https://bitbucket.org/atlassian-docker/docker-atlassian-bitbucket-server/</li> <li>Modify or replace the Jinja templates   under <code>config</code>; NOTE: The files must have the <code>.j2</code> extensions. However, you   don't have to use template variables if you don't wish.</li> <li>Build the new image with e.g: <code>docker build --tag my-bitbucket-image --build-arg BITBUCKET_VERSION=8.x.x .</code></li> <li>Optionally push to a registry, and deploy.</li> </ul>"},{"location":"containers/BITBUCKET/#supported-architectures","title":"Supported architectures","text":"<p>Currently, Bitbucket container images are built for the <code>linux/amd64</code> and <code>linux/arm64</code> target platforms. The Dockerfiles and support tooling have now had all architecture-specific components removed, so if necessary it is possible to build images for any platform supported by OCI-compliant container runtimes.</p>"},{"location":"containers/BITBUCKET/#building-on-the-target-architecture","title":"Building on the target architecture","text":"<p>The simplest method of getting a platform image is to build it on a target machine; see \"Building your own image\" above.</p> <p>Note: This method is known to work on Mac M1 and AWS ARM64 machines, but has not been extensively tested.</p>"},{"location":"containers/BITBUCKET/#troubleshooting","title":"Troubleshooting","text":"<p>These images include built-in scripts to assist in performing common JVM diagnostic tasks.</p>"},{"location":"containers/BITBUCKET/#thread-dumps","title":"Thread dumps","text":"<p><code>/opt/atlassian/support/thread-dumps.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of thread dumps from the containerized application. For example: <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh\n</code></pre></p> <p>By default, this script will collect 10 thread dumps at 5 second intervals. This can be overridden by passing a custom value for the count and interval, by using <code>-c</code> / <code>--count</code> and <code>-i</code> / <code>--interval</code> respectively. For example, to collect 20 thread dumps at 3 second intervals: <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh --count 20 --interval 3\n</code></pre></p> <p>Thread dumps will be written to <code>$APP_HOME/thread_dumps/&lt;date&gt;</code>.</p> Disable capturing output from top run <p>By default this script will also capture output from top run in 'Thread-mode'. This can be disabled by passing <code>-n</code> / <code>--no-top</code></p>"},{"location":"containers/BITBUCKET/#heap-dump","title":"Heap dump","text":"<p><code>/opt/atlassian/support/heap-dump.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of a heap dump from the containerized application. For example: <pre><code>docker exec my_container /opt/atlassian/support/heap-dump.sh\n</code></pre></p> <p>A heap dump will be written to <code>$APP_HOME/heap.bin</code>. If a file already exists at this location, use <code>-f</code> / <code>--force</code> to overwrite the existing heap dump file.</p>"},{"location":"containers/BITBUCKET/#manual-diagnostics","title":"Manual diagnostics","text":"<p>The <code>jcmd</code> utility is also included in these images and can be used by starting a <code>bash</code> shell in the running container: <pre><code>docker exec -it my_container /bin/bash\n</code></pre></p>"},{"location":"containers/BITBUCKET/#support","title":"Support","text":"<p>For product support, go to support.atlassian.com</p> <p>You can also visit the Atlassian Data Center forum for discussion on running Atlassian Data Center products in containers.</p>"},{"location":"containers/BITBUCKET/#changelog","title":"Changelog","text":"<p>For a detailed list of changes to the Docker image configuration see the Git commit history.</p>"},{"location":"containers/BITBUCKET/#license","title":"License","text":"<p>Copyright \u00a9 2019 Atlassian Corporation Pty Ltd. Licensed under the Apache License, Version 2.0.</p>"},{"location":"containers/CONFLUENCE/","title":"Confluence","text":"<p>Server image deprecation</p> <p>This Docker image has been published as both <code>atlassian/confluence</code> and <code>atlassian/confluence-server</code> up until  February 15, 2024. Both names refer to the same image. However, post-February 15, 2024, the  <code>atlassian/confluence-server</code> version ceased receiving updates, including both existing and new tags. If you have  been using <code>atlassian/confluence-server</code>, switch to the <code>atlassian/confluence</code> image to ensure access to the  latest updates and new tags.</p>"},{"location":"containers/CONFLUENCE/#overview","title":"Overview","text":"<p>Confluence Server is where you create, organise and discuss work with your team. Capture the knowledge that's too often lost in email inboxes and shared network drives in Confluence - where it's easy to find, use, and update.  Give every team, project, or department its own space to create the things they need, whether it's meeting notes,  product requirements, file lists, or project plans, you can get more done in Confluence.</p> <p>This Docker container makes it easy to get an instance of Confluence up and running.</p> <p>Learn more about Confluence Server: https://www.atlassian.com/software/confluence</p> <p>You can find the repository with the Dockerfile at https://bitbucket.org/atlassian-docker/docker-atlassian-confluence-server</p> <p>DockerHub repository: https://hub.docker.com/r/atlassian/confluence</p> <p>Use docker version &gt;= 20.10.10</p>"},{"location":"containers/CONFLUENCE/#quick-start","title":"Quick Start","text":"<p>For the directory in the environmental variable <code>CONFLUENCE_HOME</code> that is used to store Confluence data (amongst other things) we recommend mounting a host directory as a data volume:</p> <p>Additionally, if running Confluence in Data Center mode it is required that a shared filesystem is mounted. The mountpoint (inside the container) can be configured with <code>CONFLUENCE_SHARED_HOME</code>.</p> <p>Start Atlassian Confluence Server: <pre><code>docker run -v /data/your-confluence-home:/var/atlassian/application-data/confluence --name=\"confluence\" -d -p 8090:8090 -p 8091:8091 atlassian/confluence\n</code></pre></p> <p>Confluence is now available on http://localhost:8090.</p> <p>Please ensure your container has the necessary resources allocated to it.  We recommend 2GiB of memory allocated to accommodate the application server.  See Supported Platforms for further information.</p> If you are using <code>docker-machine</code> on Mac OS X, please use <code>open http://$(docker-machine ip default):8090</code> instead."},{"location":"containers/CONFLUENCE/#configuring-confluence","title":"Configuring Confluence","text":"<p>This Docker image is intended to be configured from its environment; the provided information is used to generate the application configuration files from templates. This allows containers to be repeatably created and destroyed on-the-fly, as required in advanced cluster configurations. Most aspects of the deployment can be configured in this manner; the necessary environment variables are documented below. However, if your particular deployment scenario is not covered by these settings, it is possible to override the provided templates with your own; see the section Advanced Configuration below.</p>"},{"location":"containers/CONFLUENCE/#verbose-container-entrypoint-logging","title":"Verbose container entrypoint logging","text":"<p>During the startup process of the container, various operations and checks are performed to ensure that the application is configured correctly and ready to run. To help in troubleshooting and to provide transparency into this process, you can enable verbose logging. The <code>VERBOSE_LOGS</code> environment variable enables detailed debug messages to the container's log, offering insights into the actions performed by the entrypoint script.</p> <ul> <li><code>VERBOSE_LOGS</code> (default: false)</li> </ul> <p>Set to <code>true</code> to enable detailed debug messages during the container initialization.</p>"},{"location":"containers/CONFLUENCE/#memory-heap-size","title":"Memory / Heap Size","text":"<p>If you need to override Confluence Server's default memory allocation, you can control the minimum heap (Xms) and maximum heap (Xmx) via the below environment variables.</p> <ul> <li><code>JVM_MINIMUM_MEMORY</code> (default: 1024m)</li> </ul> <p>The minimum heap size of the JVM</p> <ul> <li><code>JVM_MAXIMUM_MEMORY</code> (default: 1024m)</li> </ul> <p>The maximum heap size of the JVM</p> <ul> <li> <p><code>JVM_RESERVED_CODE_CACHE_SIZE</code> (default: 256m)</p> <p>The reserved code cache size of the JVM</p> </li> </ul>"},{"location":"containers/CONFLUENCE/#tomcat-and-reverse-proxy-settings","title":"Tomcat and Reverse Proxy Settings","text":"<p>If Confluence is run behind a reverse proxy server (e.g. a load-balancer or nginx server), then you need to specify extra options to make Confluence aware of the setup. They can be controlled via the below environment variables.</p> <ul> <li><code>ATL_PROXY_NAME</code> (default: NONE)</li> </ul> <p>The reverse proxy's fully qualified hostname. <code>CATALINA_CONNECTOR_PROXYNAME</code>    is also supported for backwards compatability.</p> <ul> <li><code>ATL_PROXY_PORT</code> (default: NONE)</li> </ul> <p>The reverse proxy's port number via which Confluence is    accessed. <code>CATALINA_CONNECTOR_PROXYPORT</code> is also supported for backwards    compatability.</p> <ul> <li><code>ATL_TOMCAT_PORT</code> (default: 8090)</li> </ul> <p>The port for Tomcat/Confluence to listen on. Depending on your container    deployment method this port may need to be    [exposed and published][docker-expose].</p> <ul> <li><code>ATL_TOMCAT_SCHEME</code> (default: http)</li> </ul> <p>The protocol via which Confluence is accessed. <code>CATALINA_CONNECTOR_SCHEME</code> is also    supported for backwards compatability.</p> <ul> <li><code>ATL_TOMCAT_SECURE</code> (default: false)</li> </ul> <p>Set 'true' if <code>ATL_TOMCAT_SCHEME</code> is 'https'. <code>CATALINA_CONNECTOR_SECURE</code> is    also supported for backwards compatability.</p> <ul> <li><code>ATL_TOMCAT_CONTEXTPATH</code> (default: NONE)</li> </ul> <p>The context path the application is served over. <code>CATALINA_CONTEXT_PATH</code> is    also supported for backwards compatability.</p> <ul> <li><code>ATL_TOMCAT_ACCESS_LOG</code> (default: false [version &lt; 7.11.0] and true [version &gt;=7.11.0])</li> </ul> <p>Whether to enable Tomcat access logging; set to <code>true</code> to enable. NOTE:    These logs are written to the Container internal volume by default (under    <code>/opt/atlassian/confluence/logs/</code>); these are rotated but not removed, and    will grow indefinitely. If you enable this functionality it is recommended    that you map the directory to a volume and perform log ingestion/cleanup with    external tools.</p> <ul> <li><code>ATL_TOMCAT_REQUESTATTRIBUTESENABLED</code></li> </ul> <p>Checks for the existence of request attributes (typically set by the RemoteIpValve and similar)   that should be used to override the values returned by the request for remote address,   remote host, server port and protocol. This property is usually combined with <code>ATL_TOMCAT_TRUSTEDPROXIES</code>   and <code>ATL_TOMCAT_INTERNALPROXIES</code> to show IP address of the remote host instead of the load balancer's.   If not declared, the default value of <code>false</code> will be used.</p> <ul> <li><code>ATL_TOMCAT_TRUSTEDPROXIES</code></li> </ul> <p>A list of IP addresses separated by a pipe character e.g. <code>10.0.9.6|10.0.9.32</code>.   Trusted proxies that appear in the <code>remoteIpHeader</code> will be trusted and will appear   in the <code>proxiesHeader</code> value. By adding a list of Trusted Proxies, Confluence will remove the   load balancers' IP addresses from Confluence's view of the incoming connection. This could be desired   in a clustered load balancer architecture where the load balancer address changes depending on   which node proxies the connection.   If not specified, no trusted proxies will be trusted.</p> <ul> <li><code>ATL_TOMCAT_INTERNALPROXIES</code></li> </ul> <p>A list of IP addresses separated by a pipe character e.g. <code>10.0.9.6|10.0.9.32</code>.   Trusted proxies that appear in the <code>remoteIpHeader</code> will be trusted and will not appear   in the <code>proxiesHeader</code> value. By adding a list of Internal Proxies, Confluence will remove the   load balancers' IP addresses from Confluence's view of the incoming connection. This could be desired   in a clustered load balancer architecture where the load balancer address changes depending on   which node proxies the connection. If not specified, no internal proxies will be trusted.</p> <ul> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_PORT</code> - If unset, no additional connector is defined in server.xml. No default.</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_CONNECTION_TIMEOUT</code> (default: 20000)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_MAX_THREADS</code> (default: 50)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_MIN_SPARE_THREADS</code> (default: 10)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_ENABLE_LOOKUPS</code> (default: false)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_ACCEPT_COUNT</code> (default: 10)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_URI_ENCODING</code> (default: UTF-8)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_SECURE</code> (no default, if undefined secure is not set in the connector)</li> </ul> <p>The following Tomcat/Catalina options are also supported. For more information, see https://tomcat.apache.org/tomcat-7.0-doc/config/index.html. </p> <ul> <li><code>ATL_TOMCAT_MGMT_PORT</code> (default: 8000)</li> <li><code>ATL_TOMCAT_MAXTHREADS</code> (default: 48)</li> <li><code>ATL_TOMCAT_MINSPARETHREADS</code> (default: 10)</li> <li><code>ATL_TOMCAT_CONNECTIONTIMEOUT</code> (default: 20000)</li> <li><code>ATL_TOMCAT_ENABLELOOKUPS</code> (default: false)</li> <li><code>ATL_TOMCAT_PROTOCOL</code> (default: org.apache.coyote.http11.Http11NioProtocol)</li> <li><code>ATL_TOMCAT_REDIRECTPORT</code> (default: 8443)</li> <li><code>ATL_TOMCAT_ACCEPTCOUNT</code> (default: 10)</li> <li><code>ATL_TOMCAT_DEBUG</code> (default: 0)</li> <li><code>ATL_TOMCAT_URIENCODING</code> (default: UTF-8)</li> <li><code>ATL_TOMCAT_MAXHTTPHEADERSIZE</code> (default: 8192)</li> <li><code>ATL_TOMCAT_STUCKTHREADDETECTIONVALVE_THRESHOLD</code> (default: 60)</li> </ul>"},{"location":"containers/CONFLUENCE/#access-log-settings","title":"Access Log Settings","text":"<p>You can set the maximum number of days for access logs to be retained before being deleted. The default value of -1 means never delete old files.</p> <ul> <li><code>ATL_TOMCAT_ACCESS_LOGS_MAXDAYS</code> (default: -1)</li> <li><code>ATL_TOMCAT_ACCESS_LOG_PATTERN</code> (default: <code>%h %{X-AUSERNAME}o %t &amp;quot;%r&amp;quot; %s %b %D %U %I &amp;quot;%{User-Agent}i&amp;quot;</code>)</li> </ul>"},{"location":"containers/CONFLUENCE/#jvm-configuration","title":"JVM configuration","text":"<p>If you need to pass additional JVM arguments to Confluence such as specifying a custom trust store, you can add them via the below environment variable</p> <ul> <li><code>JVM_SUPPORT_RECOMMENDED_ARGS</code></li> </ul> <p>Additional JVM arguments for Confluence.</p> Example <p><code>docker run -e JVM_SUPPORT_RECOMMENDED_ARGS=-Djavax.net.ssl.trustStore=/var/atlassian/application-data/confluence/cacerts -v confluenceVolume:/var/atlassian/application-data/confluence --name=\"confluence\" -d -p 8090:8090 -p 8091:8091 atlassian/confluence</code></p> <p>For additional settings that can be supplied, see: Recognized System Properties</p>"},{"location":"containers/CONFLUENCE/#confluence-specific-settings","title":"Confluence-specific settings","text":"<ul> <li><code>ATL_AUTOLOGIN_COOKIE_AGE</code> (default: 1209600; two weeks, in seconds)</li> </ul> <p>The maximum time a user can remain logged-in with 'Remember Me'.</p> <ul> <li><code>CONFLUENCE_HOME</code></li> </ul> <p>The confluence home directory. This may be on an mounted volume; if so it    should be writable by the user <code>confluence</code>. See note below about UID    mappings.</p> <ul> <li><code>ATL_LUCENE_INDEX_DIR</code></li> </ul> <p>The directory where Lucene search indexes should   be stored. Defaults to <code>index</code> under the Confluence home directory.</p> <ul> <li><code>ATL_LICENSE_KEY</code> (from Confluence 7.9 onwards)</li> </ul> <p>The Confluence license string. Providing this will remove the need to supply it through the web startup screen.</p> <ul> <li>use with caution <code>CONFLUENCE_LOG_STDOUT</code> <code>[true, false]</code>  (from Confluence 7.9 onwards)</li> </ul> <p>Prior to Confluence version 7.9.0, the log files are always stored in the <code>logs</code> folder in Confluence home. From version   7.9.0, the logs can be printed directly to the <code>stdout</code> and don't use the file at all. This makes it possible to fetch the log messages   via <code>docker logs &lt;CONTAINER_ID&gt;</code>. In this setup we recommend using some log aggregation tooling (e.g. AWS Cloudwatch or ELK stack).</p> <p>Beware, if enabled, the support ZIP produced by the Troubleshooting and Support plugin doesn't contain the application logs.</p> <ul> <li> <p><code>ATL_SEARCH_PLATFORM</code></p> <p>The search platform to use. Set to <code>opensearch</code> if you want to use OpenSearch as the search platform. See: https://confluence.atlassian.com/doc/configuring-opensearch-for-confluence-1387594125.html</p> </li> <li> <p><code>ATL_OPENSEARCH_HTTP_URL</code></p> <p>HTTP(S) URL of the OpenSearch cluster, or multiple URLs separated by commas.</p> </li> <li> <p><code>ATL_OPENSEARCH_USERNAME</code></p> <p>Username for the OpenSearch cluster.</p> </li> <li> <p><code>ATL_OPENSEARCH_PASSWORD</code></p> </li> </ul> <p>Password for the OpenSearch cluster.</p> <ul> <li><code>ATL_CONFLUENCE_SESSION_TIMEOUT</code></li> </ul> <p>The default Tomcat session timeout (in minutes) for all newly created sessions which is set in web.xml. Defaults to 30.</p> <ul> <li> <p><code>ATL_CONFLUENCE_CFG_ADDITIONAL_PROPERTIES</code></p> <p>Additional properties to be added to <code>confluence.cfg.xml</code>. The properties should be in the format <code>key=value</code> and separated by a comma. For example, <code>ATL_CONFLUENCE_CFG_ADDITIONAL_PROPERTIES=confluence.cluster.node.name=node1,confluence.cluster.node.server=server1</code>.</p> </li> </ul>"},{"location":"containers/CONFLUENCE/#database-configuration","title":"Database configuration","text":"<p>It is optionally possible to configure the database from the environment, avoiding the need to do so through the web startup screen.</p> <p>The following variables are all must all be supplied if using this feature:</p> <ul> <li><code>ATL_JDBC_URL</code></li> </ul> <p>The database URL; this is database-specific. It is allowed to use <code>&amp;</code> in the URL which will be automatically converted to <code>&amp;amp;</code>.</p> <ul> <li><code>ATL_JDBC_USER</code></li> </ul> <p>The database user to connect as.</p> <ul> <li><code>ATL_JDBC_PASSWORD</code></li> </ul> <p>The password for the database user.</p> <ul> <li><code>ATL_DB_TYPE</code></li> </ul> <p>The type of database; valid supported values are:</p> <ul> <li><code>mssql</code></li> <li><code>mysql</code></li> <li><code>oracle12c</code> (Confluence 7.3.0 or earlier only)</li> <li><code>oracle</code> (Confluence 7.3.1 or later only. Compatible with Oracle 12c and Oracle 19c)</li> <li><code>postgresql</code></li> </ul> MySQL or Oracle JDBC drivers <p>Due to licensing restrictions Confluence does not ship with a MySQL or Oracle JDBC drivers.  To use these databases you will need to copy a suitable driver into the container and restart it.  For example, to copy the MySQL driver into a container named \"confluence\", you would do the following:</p> <p><code>docker cp mysql-connector-java.x.y.z.jar confluence:/opt/atlassian/confluence/confluence/WEB-INF/lib</code></p> <p><code>docker restart confluence</code></p> <p>For more information see the Database JDBC Drivers page.</p>"},{"location":"containers/CONFLUENCE/#optional-database-settings","title":"Optional database settings","text":"<ul> <li><code>ATL_JDBC_SECRET_CLASS</code></li> </ul> <p>Encryption class for the database password. Depending on the secret class, the value of <code>ATL_JDBC_PASSWORD</code> will differ. Defaults to plaintext. </p> <p>JDBC encryption can only be used with Confluence instances that have already been set up.</p> <p>Starting from 8.6 AWS SecretsManager is supported.</p> <p>For non-clustered Confluence, manually edit <code>jdbc.password.decrypter.classname</code> and <code>hibernate.connection.password</code> properties as instructed by  step 5 of official documentation, then restart container. </p> <p>For clustered Confluence, set this property while making sure environment variables in cluster configuration are kept intact as well.  Example: <pre><code>docker run -v /data/your-confluence-home:/var/atlassian/application-data/confluence \\\n--name=\"confluence\" -d -p 8090:8090 -p 8091:8091 \\\n-e ATL_JDBC_SECRET_CLASS='com.atlassian.secrets.store.aws.AwsSecretsManagerStore' \\\n-e ATL_JDBC_PASSWORD='{\"region\": \"us-east-1\", \"secretId\": \"mysecret\", \"secretPointer\": \"/password\"}' \\\n-e ATL_CLUSTER_RELATED_VARIABLES='variable-value' \\\natlassian/confluence\n</code></pre></p> <p>The following variables are for the database connection pool, and are optional.</p> <ul> <li><code>ATL_DB_POOLMINSIZE</code> (default: 20)</li> <li><code>ATL_DB_POOLMAXSIZE</code> (default: 100)</li> <li><code>ATL_DB_TIMEOUT</code> (default: 30)</li> <li><code>ATL_DB_IDLETESTPERIOD</code> (default: 100)</li> <li><code>ATL_DB_MAXSTATEMENTS</code> (default: 0)</li> <li><code>ATL_DB_VALIDATE</code> (default: false)</li> <li><code>ATL_DB_ACQUIREINCREMENT</code> (default: 1)</li> <li><code>ATL_DB_VALIDATIONQUERY</code> (default: \"select 1\")</li> <li><code>ATL_DB_PROVIDER_CLASS</code> (default: <code>com.atlassian.confluence.impl.hibernate.DelegatingHikariConnectionProvider</code>)</li> <li><code>ATL_DB_CONNECTION_AUTOCOMMIT</code> (default false)</li> <li><code>ATL_DB_CONNECTION_ISOLATION</code> (default 2)</li> <li><code>ATL_DB_DATASOURCE_HIKARI_REGISTER_MBEANS</code> (default true)</li> </ul>"},{"location":"containers/CONFLUENCE/#data-center-configuration","title":"Data Center configuration","text":"<p>This docker image can be run as part of a Data Center cluster. You can specify the following properties to start Confluence as a Data Center node, instead of manually configuring a cluster. See Installing Confluence Data Center for more information.</p>"},{"location":"containers/CONFLUENCE/#cluster-configuration","title":"Cluster configuration","text":"<p>Confluence Data Center allows clustering via various methods. For more information on the setting for each type see this page.</p> <p>NOTE: The underlying network should be set-up to support the Confluence clustering type you are using. How to do this depends on the container management technology, and is beyond the scope of this documentation.</p>"},{"location":"containers/CONFLUENCE/#common-cluster-settings","title":"Common cluster settings","text":"<ul> <li><code>ATL_CLUSTER_TYPE</code></li> </ul> <p>The cluster type. Setting this effectively enables clustering. Valid values    are <code>aws</code>, <code>multicast</code>, and <code>tcp_ip</code>.</p> <ul> <li><code>ATL_CLUSTER_NAME</code></li> </ul> <p>The cluster name; this should be common across all nodes.</p> <ul> <li><code>ATL_PRODUCT_HOME_SHARED</code></li> </ul> <p>The location of the shared home directory for all Confluence nodes. Note:    This must be real shared filesystem that is mounted inside the    container. Additionally, see the note about UIDs.</p> <ul> <li><code>ATL_CLUSTER_TTL</code></li> </ul> <p>The time-to-live for cluster packets. Primarily of use in multicast clusters.</p> <ul> <li><code>ATL_CLUSTER_INTERFACE</code></li> </ul> <p>The network interface Confluence will use to communicate between nodes. Auto-detected by default.    Override interface if Confluence has selected the wrong one.</p>"},{"location":"containers/CONFLUENCE/#aws-cluster-settings","title":"AWS cluster settings","text":"<p>The following should be populated from the AWS environment.</p> <ul> <li><code>ATL_HAZELCAST_NETWORK_AWS_IAM_ROLE</code></li> <li><code>ATL_HAZELCAST_NETWORK_AWS_IAM_REGION</code></li> <li><code>ATL_HAZELCAST_NETWORK_AWS_HOST_HEADER</code></li> <li><code>ATL_HAZELCAST_NETWORK_AWS_SECURITY_GROUP</code></li> <li><code>ATL_HAZELCAST_NETWORK_AWS_TAG_KEY</code></li> <li><code>ATL_HAZELCAST_NETWORK_AWS_TAG_VALUE</code></li> </ul>"},{"location":"containers/CONFLUENCE/#tcp-cluster-settings","title":"TCP cluster settings","text":"<ul> <li><code>ATL_CLUSTER_PEERS</code></li> </ul> <p>A comma-separated list of peer IPs.</p>"},{"location":"containers/CONFLUENCE/#multicast-cluster-settings","title":"Multicast cluster settings","text":"<ul> <li><code>ATL_CLUSTER_ADDRESS</code></li> </ul> <p>The multicast address the cluster will communicate on.</p>"},{"location":"containers/CONFLUENCE/#container-configuration","title":"Container Configuration","text":"<ul> <li><code>ATL_FORCE_CFG_UPDATE</code> (default: false)</li> </ul> <p>The Docker entrypoint generates application configuration on    first start; not all of these files are regenerated on subsequent    starts. This is deliberate, to avoid race conditions or overwriting manual    changes during restarts and upgrades. However in deployments where    configuration is purely specified through the environment (e.g. Kubernetes)    this behaviour may be undesirable; this flag forces an update of all    generated files.</p> <p>In Confluence the affected files are: <code>confluence.cfg.xml</code></p> <p>See the entrypoint code for the details of how configuration    files are generated.</p> <ul> <li><code>SET_PERMISSIONS</code> (default: true)</li> </ul> <p>Define whether to set home directory permissions on startup. Set to <code>false</code> to disable    this behaviour.</p> <ul> <li><code>ATL_UNSET_SENSITIVE_ENV_VARS</code> (default: true)</li> </ul> <p>Define whether to unset environment variables containing keywords 'PASS', 'SECRET' or 'TOKEN'.    The unset function is executed in the entrypoint. Set to <code>false</code> if you want to allow passing    sensitive environment variables to Confluence container.</p> Value exposure on host OS <p>When using this property, the values to sensitive environment variables will be available in clear text on the  host OS. As such, this data may be exposed to users or processes running on the host OS.</p> <ul> <li><code>ATL_ALLOWLIST_SENSITIVE_ENV_VARS</code></li> </ul> <p>Define a comma separated list of environment variables containing keywords 'PASS', 'SECRET' or 'TOKEN' to be ignored by the unset function which is executed in the entrypoint. The function uses <code>^</code> regex. For example, if you set <code>ATL_ALLOWLIST_SENSITIVE_ENV_VARS=\"PATH_TO_SECRET_FILE\"</code>, all variables starting with <code>PATH_TO_SECRET_FILE</code> will not be unset.</p> Value exposure on host OS <p>When using this property, the values to sensitive environment variables will be available in clear text on the  host OS. As such, this data may be exposed to users or processes running on the host OS.</p>"},{"location":"containers/CONFLUENCE/#advanced-configuration","title":"Advanced Configuration","text":"<p>As mentioned at the top of this section, the settings from the environment are used to populate the application configuration on the container startup. However, in some cases you may wish to customise the settings in ways that are not supported by the environment variables above. In this case, it is possible to modify the base templates to add your own configuration. There are three main ways of doing this; modify our repository to your own image, build a new image from the existing one, or provide new templates at startup. We will briefly outline these methods here, but in practice how you do this will depend on your needs.</p>"},{"location":"containers/CONFLUENCE/#building-your-own-image","title":"Building your own image","text":"<ul> <li>Clone the Atlassian repository at https://bitbucket.org/atlassian-docker/docker-atlassian-confluence-server/</li> <li>Modify or replace the Jinja templates   under <code>config</code>; NOTE: The files must have the <code>.j2</code> extensions. However you   don't have to use template variables if you don't wish.</li> <li>Build the new image with e.g: <code>docker build --tag my-confluence-image --build-arg CONFLUENCE_VERSION=6.x.x .</code></li> <li>Optionally push to a registry, and deploy.</li> </ul>"},{"location":"containers/CONFLUENCE/#build-a-new-image-from-the-existing-one","title":"Build a new image from the existing one","text":"<ul> <li>Create a new <code>Dockerfile</code>, which starts with the line e.g: <code>FROM atlassian/confluence:latest</code>.</li> <li>Use a <code>COPY</code> line to overwrite the provided templates.</li> <li>Build, push and deploy the new image as above.</li> </ul>"},{"location":"containers/CONFLUENCE/#overwrite-the-templates-at-runtime","title":"Overwrite the templates at runtime","text":"<p>There are two main ways of doing this:</p> <ul> <li>If your container is going to be long-lived, you can create it, modify the   installed templates under <code>/opt/atlassian/etc/</code>, and then run it.</li> <li>Alternatively, you can create a volume containing your alternative templates,   and mount it over the provided templates at runtime   with <code>--volume my-config:/opt/atlassian/etc/</code>.</li> </ul>"},{"location":"containers/CONFLUENCE/#shared-directory-and-user-ids","title":"Shared directory and user IDs","text":"<p>By default the Confuence application runs as the user <code>confluence</code>, with a UID and GID of 2002. Consequently this UID must have write access to the shared filesystem. If for some reason a different UID must be used, there are a number of options available:</p> <ul> <li>The Docker image can be rebuilt with a different UID.</li> <li>Under Linux, the UID can be remapped using   user namespace remapping.</li> </ul> <p>To preserve strict permissions for certain configuration files, this container starts as <code>root</code> to perform bootstrapping before running Confluence under a non-privileged user account. If you wish to start the container as a non-root user, please note that Tomcat configuration, and the bootstrapping of seraph-config.xml (SSO) &amp; confluence-init.properties (overriding <code>$CONFLUENCE_HOME</code>) will be skipped and a warning will be logged. You may still apply custom configuration in this situation by mounting a custom file directly, e.g. by mounting your own server.xml file directly to <code>/opt/atlassian/confluence/conf/server.xml</code></p> <p>Database and Clustering bootstrapping will work as expected when starting this container as a non-root user.</p>"},{"location":"containers/CONFLUENCE/#upgrade","title":"Upgrade","text":"<p>To upgrade to a more recent version of Confluence Server you can simply stop the <code>Confluence</code> container and start a new one based on a more recent image: <pre><code>docker stop confluence\ndocker rm confluence\ndocker run ... (see above)\n</code></pre> As your data is stored in the data volume directory on the host, it will still be available after the upgrade.</p> <p>Please make sure that you don't accidentally remove the <code>confluence</code> container and its volumes using the <code>-v</code> option.</p>"},{"location":"containers/CONFLUENCE/#backup","title":"Backup","text":"<p>For evaluating Confluence you can use the built-in database that will store its files in the Confluence Server home directory. In that case it is sufficient to create a backup archive of the directory on the host that is used as a volume (<code>/data/your-confluence-home</code> in the example above).</p> <p>Confluence's automatic backup is currently supported in the Docker setup. You can also use the Production Backup Strategy approach if you're using an external database.</p> <p>Read more about data recovery and backups: Site Backup and Restore</p>"},{"location":"containers/CONFLUENCE/#shutdown","title":"Shutdown","text":"<p>Confluence allows a grace period of 20s for active operations to finish before termination. If sending a <code>docker stop</code> this should be taken into account with the <code>--time</code> flag.</p> <p>Alternatively, the script <code>/shutdown-wait.sh</code> is provided, which will initiate a clean shutdown and wait for the process to complete. This is the recommended method for shutdown in environments which provide for orderly shutdown, e.g. Kubernetes via the <code>preStop</code> hook.</p>"},{"location":"containers/CONFLUENCE/#versioning","title":"Versioning","text":"<p>The <code>latest</code> tag matches the most recent official release of Atlassian Confluence Server. So <code>atlassian/confluence:latest</code> will use the newest stable version of Confluence Server available.</p> <p>Alternatively, you can use a specific minor version of Confluence Server by using a version number tag: <code>atlassian/confluence:7.13</code>. This will install the latest <code>7.13.x</code> version that is available.</p> <p>We also publish docker images for our EAP releases (not supported for use in production). The tag for EAP releases is the EAP version. For example to get the <code>7.8.0-beta1</code> EAP release, use <code>atlassian/confluence:7.8.0-beta1</code>.</p> <p>For example, <code>atlassian/confluence:7.13-ubuntu-jdk11</code> will install the latest 7.13.x version with Eclipse Temurin OpenJDK 11.</p>"},{"location":"containers/CONFLUENCE/#supported-jdk-versions-and-base-images","title":"Supported JDK versions and base images","text":"<p>Atlassian Docker images are generated from the official Eclipse Temurin OpenJDK Docker images.</p> <p>Starting from 8.5.6 UBI based tags are published as well. Tags are available in 2 formats: <code>&lt;version&gt;-ubi9</code> and <code>&lt;version&gt;-ubi9-jdk17</code>.</p> <p>The build pipeline pushes both JDK11 and JDK17 tags for Confluence versions ranging from 7.19 to 8.9. Starting from 9.0 only JDK17 tags are published. UBI based tags are JDK17 only.</p> <p>The Docker images follow the Atlassian Support end-of-life policy; images for unsupported versions of the products remain available but will no longer receive updates or fixes.</p> <p>Historically, we have also generated other versions of the images, including JDK8, Alpine, and 'slim' versions of the JDK. These legacy images still exist in Docker Hub, however they should be considered deprecated, and do not receive updates or fixes.</p> <p>If for some reason you need a different version, see \"Building your own image\" above.</p>"},{"location":"containers/CONFLUENCE/#migration-to-ubi","title":"Migration to UBI","text":"<p>If you have been mounting any files to <code>${JAVA_HOME}</code> directory in <code>eclipse-temurin</code> based container, <code>JAVA_HOME</code> in UBI JDK17 container is set to <code>/usr/lib/jvm/java-17</code>.</p> <p>Also, if you have been mounting and running any custom scripts in the container, UBI-based images may lack some tools and utilities that are available out of the box in eclipse-temurin tags. If that's the case, see Building your own image.</p>"},{"location":"containers/CONFLUENCE/#supported-architectures","title":"Supported architectures","text":"<p>Currently, Confluence container images are built for the <code>linux/amd64</code> and <code>linux/arm64</code> target platforms. The Dockerfiles and support tooling have now had all architecture-specific components removed, so if necessary it is possible to build images for any platform supported by OCI-compliant container runtimes.</p>"},{"location":"containers/CONFLUENCE/#building-on-the-target-architecture","title":"Building on the target architecture","text":"<p>Note: This method is known to work on Mac M1 and AWS ARM64 machines, but has not been extensively tested.</p> <p>The simplest method of getting a platform image is to build it on a target machine. The following assumes you have git and Docker installed. You will also need to know which version of Confluence you want to build; substitute <code>CONFLUENCE_VERSION=x.x.x</code> with your required version:</p> <p><pre><code>git clone --recurse-submodule https://bitbucket.org/atlassian-docker/docker-atlassian-confluence-server.git\ncd docker-atlassian-confluence-server\ndocker build --tag my-image --build-arg CONFLUENCE_VERSION=x.x.x .\n</code></pre> This image can be pushed up to your own Docker Hub or private repository.</p>"},{"location":"containers/CONFLUENCE/#troubleshooting","title":"Troubleshooting","text":"<p>These images include built-in scripts to assist in performing common JVM diagnostic tasks.</p>"},{"location":"containers/CONFLUENCE/#thread-dumps","title":"Thread dumps","text":"<p><code>/opt/atlassian/support/thread-dumps.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of thread dumps from the containerized application. For example:</p> <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh\n</code></pre> <p>By default, this script will collect 10 thread dumps at 5 second intervals. This can be overridden by passing a custom value for the count and interval, by using <code>-c</code> / <code>--count</code> and <code>-i</code> / <code>--interval</code> respectively. For example, to collect 20 thread dumps at 3 second intervals:</p> <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh --count 20 --interval 3\n</code></pre> <p>Thread dumps will be written to <code>$APP_HOME/thread_dumps/&lt;date&gt;</code>.</p> Disable capturing output from top run <p>By default this script will also capture output from top run in 'Thread-mode'. This can be disabled by passing <code>-n</code> / <code>--no-top</code></p>"},{"location":"containers/CONFLUENCE/#heap-dump","title":"Heap dump","text":"<p><code>/opt/atlassian/support/heap-dump.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of a heap dump from the containerized application. For example:</p> <pre><code>docker exec my_container /opt/atlassian/support/heap-dump.sh\n</code></pre> <p>A heap dump will be written to <code>$APP_HOME/heap.bin</code>. If a file already exists at this location, use <code>-f</code> / <code>--force</code> to overwrite the existing heap dump file.</p>"},{"location":"containers/CONFLUENCE/#manual-diagnostics","title":"Manual diagnostics","text":"<p>The <code>jcmd</code> utility is also included in these images and can be used by starting a <code>bash</code> shell in the running container:</p> <pre><code>docker exec -it my_container /bin/bash\n</code></pre>"},{"location":"containers/CONFLUENCE/#support","title":"Support","text":"<p>For product support, go to support.atlassian.com.</p> <p>You can also visit the Atlassian Data Center forum for discussion on running Atlassian Data Center products in containers.</p>"},{"location":"containers/CONFLUENCE/#contribution","title":"Contribution","text":"<p>See the contributing guideline if you are contributing from outside Atlassian.</p>"},{"location":"containers/CONFLUENCE/#changelog","title":"Changelog","text":"<p>For a detailed list of changes to the Docker image configuration see the Git commit history.</p>"},{"location":"containers/CONFLUENCE/#license","title":"License","text":"<p>Copyright \u00a9 2020 Atlassian Corporation Pty Ltd. Licensed under the Apache License, Version 2.0.</p>"},{"location":"containers/CROWD/","title":"Crowd","text":""},{"location":"containers/CROWD/#overview","title":"Overview","text":"<p>Crowd provides single sign-on and user identity that's easy to use.</p> <p>Learn more about Crowd: https://www.atlassian.com/software/crowd</p> <p>This Docker container makes it easy to get an instance of Crowd up and running.</p> <p>Use docker version &gt;= 20.10.10</p>"},{"location":"containers/CROWD/#quick-start","title":"Quick Start","text":"<p>For the <code>CROWD_HOME</code> directory that is used to store application data (amongst other things) we recommend mounting a host directory as a data volume, or via a named volume.</p> <p>To get started you can use a data volume, or named volumes. In this example we'll use named volumes. <pre><code>docker volume create --name crowdVolume\ndocker run -v crowdVolume:/var/atlassian/application-data/crowd --name=\"crowd\" -d -p 8095:8095 atlassian/crowd\n</code></pre></p> <p>Crowd is now available on http://localhost:8095.</p> <p>Please ensure your container has the necessary resources allocated to it. See Supported Platforms for further information.</p> If you are using <code>docker-machine</code> on Mac OS X, please use <code>open http://$(docker-machine ip default):8095</code> instead."},{"location":"containers/CROWD/#common-settings","title":"Common settings","text":""},{"location":"containers/CROWD/#verbose-container-entrypoint-logging","title":"Verbose container entrypoint logging","text":"<p>During the startup process of the container, various operations and checks are performed to ensure that the application is configured correctly and ready to run. To help in troubleshooting and to provide transparency into this process, you can enable verbose logging. The <code>VERBOSE_LOGS</code> environment variable enables detailed debug messages to the container's log, offering insights into the actions performed by the entrypoint script.</p> <ul> <li><code>VERBOSE_LOGS</code> (default: false)</li> </ul> <p>Set to <code>true</code> to enable detailed debug messages during the container initialization.</p>"},{"location":"containers/CROWD/#memory-heap-size","title":"Memory / Heap Size","text":"<p>If you need to override Crowd's default memory allocation, you can control the minimum heap (Xms) and maximum heap (Xmx) via the below environment variables.</p> <ul> <li><code>JVM_MINIMUM_MEMORY</code> (default: 384m)</li> </ul> <p>The minimum heap size of the JVM</p> <ul> <li><code>JVM_MAXIMUM_MEMORY</code> (default: 768m)</li> </ul> <p>The maximum heap size of the JVM</p>"},{"location":"containers/CROWD/#reverse-proxy-settings","title":"Reverse Proxy Settings","text":"<p>If Crowd is run behind a reverse proxy server as described here, then you need to specify extra options to make Crowd aware of the setup. They can be controlled via the below environment variables.</p> <ul> <li><code>ATL_TOMCAT_CONTEXTPATH</code> (default: /crowd)</li> </ul> <p>The context path the application is served over.</p> <ul> <li><code>ATL_PROXY_NAME</code> (default: NONE)</li> </ul> <p>The reverse proxy's fully qualified hostname. <code>CATALINA_CONNECTOR_PROXYNAME</code>    is also supported for backwards compatability.</p> <ul> <li><code>ATL_PROXY_PORT</code> (default: NONE)</li> </ul> <p>The reverse proxy's port number via which Crowd is    accessed. <code>CATALINA_CONNECTOR_PROXYPORT</code> is also supported for backwards    compatability.</p> <ul> <li><code>ATL_TOMCAT_PORT</code> (default: 8095)</li> </ul> <p>The port for Tomcat/Crowd to listen on. Depending on your container    deployment method this port may need to be    [exposed and published][docker-expose].</p> <ul> <li><code>ATL_TOMCAT_SCHEME</code> (default: http)</li> </ul> <p>The protocol via which Crowd is accessed. <code>CATALINA_CONNECTOR_SCHEME</code> is also    supported for backwards compatability.</p> <ul> <li><code>ATL_TOMCAT_SECURE</code> (default: false)</li> </ul> <p>Set 'true' if <code>ATL_TOMCAT_SCHEME</code> is 'https'. <code>CATALINA_CONNECTOR_SECURE</code> is    also supported for backwards compatability.</p> <p>The following Tomcat/Catalina options are also supported. For more information, see https://tomcat.apache.org/tomcat-8.5-doc/config/index.html.</p> <ul> <li><code>ATL_TOMCAT_MGMT_PORT</code> (default: 8000)</li> <li><code>ATL_TOMCAT_MAXTHREADS</code> (default: 100)</li> <li><code>ATL_TOMCAT_MINSPARETHREADS</code> (default: 10)</li> <li><code>ATL_TOMCAT_CONNECTIONTIMEOUT</code> (default: 20000)</li> <li><code>ATL_TOMCAT_ENABLELOOKUPS</code> (default: false)</li> <li><code>ATL_TOMCAT_PROTOCOL</code> (default: HTTP/1.1)</li> <li><code>ATL_TOMCAT_ACCEPTCOUNT</code> (default: 10)</li> <li><code>ATL_TOMCAT_MAXHTTPHEADERSIZE</code> (default: 8192)</li> </ul>"},{"location":"containers/CROWD/#access-log-settings","title":"Access Log Settings","text":"<p>You can set the maximum number of days for access logs to be retained before being deleted. The default value of -1 means never delete old files.</p> <ul> <li><code>ATL_TOMCAT_ACCESS_LOGS_MAXDAYS</code> (default: -1)</li> </ul>"},{"location":"containers/CROWD/#jvm-configuration","title":"JVM Configuration","text":"<p>If you need to pass additional JVM arguments to Crowd, such as specifying a custom trust store, you can add them via the below environment variable</p> <ul> <li><code>JVM_SUPPORT_RECOMMENDED_ARGS</code></li> </ul> <p>Additional JVM arguments for Crowd</p> Example <p><code>docker run -e JVM_SUPPORT_RECOMMENDED_ARGS=-Djavax.net.ssl.trustStore=/var/atlassian/application-data/crowd/cacerts -v crowdVolume:/var/atlassian/application-data/crowd --name=\"crowd\" -d -p 8095:8095 atlassian/crowd</code> </p>"},{"location":"containers/CROWD/#data-center-configuration","title":"Data Center configuration","text":"<p>This docker image can be run as part of a Data Center cluster. You can specify the following properties to start Crowd as a Data Center node, instead of manually configuring a cluster. See Installing Crowd Data Center for more information.</p>"},{"location":"containers/CROWD/#container-configuration","title":"Container Configuration","text":"<ul> <li><code>SET_PERMISSIONS</code> (default: true)</li> </ul> <p>Define whether to set home directory permissions on startup. Set to <code>false</code> to disable    this behaviour.</p>"},{"location":"containers/CROWD/#advanced-configuration","title":"Advanced Configuration","text":"<p>As mentioned at the top of this section, the settings from the environment are used to populate the application configuration on the container startup. However, in some cases you may wish to customise the settings in ways that are not supported by the environment variables above. In this case, it is possible to modify the base templates to add your own configuration. There are three main ways of doing this; modify our repository to your own image, build a new image from the existing one, or provide new templates at startup. We will briefly outline these methods here, but in practice how you do this will depend on your needs.</p>"},{"location":"containers/CROWD/#building-your-own-image","title":"Building your own image","text":"<ul> <li>Clone the Atlassian repository at https://bitbucket.org/atlassian-docker/docker-atlassian-crowd/</li> <li>Modify or replace the Jinja templates   under <code>config</code>; NOTE: The files must have the <code>.j2</code> extensions. However you   don't have to use template variables if you don't wish.</li> <li>Build the new image with e.g: <code>docker build --tag my-crowd-image --build-arg CROWD_VERSION=3.x.x .</code></li> <li>Optionally push to a registry, and deploy.</li> </ul>"},{"location":"containers/CROWD/#build-a-new-image-from-the-existing-one","title":"Build a new image from the existing one","text":"<ul> <li>Create a new <code>Dockerfile</code>, which starts with the Atlassian Crowd base image e.g: <code>FROM atlassian/crowd:latest</code>.</li> <li>Use a <code>COPY</code> line to overwrite the provided templates.</li> <li>Build, push and deploy the new image as above.</li> </ul>"},{"location":"containers/CROWD/#overwrite-the-templates-at-runtime","title":"Overwrite the templates at runtime","text":"<p>There are two main ways of doing this:</p> <ul> <li>If your container is going to be long-lived, you can create it, modify the   installed templates under <code>/opt/atlassian/etc/</code>, and then run it.</li> <li>Alternatively, you can create a volume containing your alternative templates,   and mount it over the provided templates at runtime   with <code>--volume my-config:/opt/atlassian/etc/</code>.</li> </ul>"},{"location":"containers/CROWD/#shared-directory-and-user-ids","title":"Shared directory and user IDs","text":"<p>By default the Crowd application runs as the user <code>crowd</code>, with a UID and GID of 2004. Consequently this UID must have write access to the shared filesystem. If for some reason a different UID must be used, there are a number of options available:</p> <ul> <li>The Docker image can be rebuilt with a different UID.</li> <li>Under Linux, the UID can be remapped using   user namespace remapping.</li> </ul> <p>To preserve strict permissions for certain configuration files, this container starts as <code>root</code> to perform bootstrapping before running Crowd under a non-privileged user account. If you wish to start the container as a non-root user, please note that Tomcat configuration will be skipped and a warning will be logged. You may still apply custom configuration in this situation by mounting configuration files directly, e.g. by mounting your own server.xml file directly to <code>/opt/atlassian/crowd/apache-tomcat/conf/server.xml</code></p>"},{"location":"containers/CROWD/#upgrade","title":"Upgrade","text":"<p>To upgrade to a more recent version of Crowd you can simply stop the <code>crowd</code> container and start a new one based on a more recent image:</p> <pre><code>docker stop crowd\ndocker rm crowd\ndocker run ... (See above)\n</code></pre> <p>As your data is stored in the data volume directory on the host it will still  be available after the upgrade.</p> <p>Please make sure that you don't accidentally remove the <code>crowd</code> container and its volumes using the <code>-v</code> option.</p>"},{"location":"containers/CROWD/#backup","title":"Backup","text":"<p>For evaluations you can use the built-in database that will store its files in the Crowd home directory. In that case it is sufficient to create a backup archive of the docker volume.</p> <p>If you're using an external database, you can configure Crowd to make a backup automatically each night. This will back up the current state, including the database to the <code>crowdVolume</code> docker volume, which can then be archived. Alternatively you can backup the database separately, and continue to create a backup archive of the docker volume to back up the Crowd Home directory.</p> <p>Read more about data recovery and backups: Backing Up and Restoring Data</p>"},{"location":"containers/CROWD/#versioning","title":"Versioning","text":"<p>The <code>latest</code> tag matches the most recent release of Atlassian Crowd. Thus <code>atlassian/crowd:latest</code> will use the newest version of Crowd available.</p> <p>Alternatively you can use a specific major, major.minor, or major.minor.patch version of Crowd by using a version number tag:</p> <ul> <li><code>atlassian/crowd:3</code></li> <li><code>atlassian/crowd:3.2</code></li> <li><code>atlassian/crowd:3.2.3</code></li> </ul> <p>All versions from 3.0+ are available</p>"},{"location":"containers/CROWD/#supported-jdk-versions-and-base-images","title":"Supported JDK versions and base images","text":"<p>All Atlassian Crowd Docker images are now JDK11 only, and generated from the official Eclipse Temurin OpenJDK Docker images.</p> <p>Starting from 5.2.3 UBI based tags are published as well. UBI tags are available in 2 formats: <code>&lt;version&gt;-ubi9</code> and <code>&lt;version&gt;-ubi9-jdk11</code>.</p> <p>The Docker images follow the Atlassian Support end-of-life policy; images for unsupported versions of the products remain available but will no longer receive updates or fixes.</p> <p>Historically, we have also generated other versions of the images, including JDK8, Alpine, and 'slim' versions of the JDK. These legacy images still exist in Docker Hub, however they should be considered deprecated, and do not receive updates or fixes.</p> <p>If for some reason you need a different version, see \"Building your own image\" above.</p>"},{"location":"containers/CROWD/#migration-to-ubi","title":"Migration to UBI","text":"<p>If you have been mounting any files to <code>${JAVA_HOME}</code> directory in <code>eclipse-temurin</code> based container, <code>JAVA_HOME</code> in UBI JDK11 container is set to <code>/usr/lib/jvm/java-11</code>.</p> <p>Also, if you have been mounting and running any custom scripts in the container, UBI-based images may lack some tools and utilities that are available out of the box in <code>eclipse-temurin</code> tags. If that's the case, see Building your own image.</p>"},{"location":"containers/CROWD/#supported-architectures","title":"Supported architectures","text":"<p>Currently, Crowd container images are built for the <code>linux/amd64</code> and <code>linux/arm64</code> target platforms. The Dockerfiles and support tooling have now had all architecture-specific components removed, so if necessary it is possible to build images for any platform supported by OCI-compliant container runtimes.</p>"},{"location":"containers/CROWD/#building-on-the-target-architecture","title":"Building on the target architecture","text":"<p>Note: This method is known to work on Mac M1 and AWS ARM64 machines, but has not been extensively tested.</p> <p>The simplest method of getting a platform image is to build it on a target machine. The following assumes you have git and Docker installed. You will also need to know which version of Crowd you want to build; substitute <code>CROWD_VERSION=x.x.x</code> with your required version:</p> <p><pre><code>git clone --recurse-submodule https://bitbucket.org/atlassian-docker/docker-atlassian-crowd.git\ncd docker-atlassian-crowd\ndocker build --tag my-image --build-arg CROWD_VERSION=x.x.x .\n</code></pre> This image can be pushed up to your own Docker Hub or private repository.</p>"},{"location":"containers/CROWD/#troubleshooting","title":"Troubleshooting","text":"<p>These images include built-in scripts to assist in performing common JVM diagnostic tasks.</p>"},{"location":"containers/CROWD/#thread-dumps","title":"Thread dumps","text":"<p><code>/opt/atlassian/support/thread-dumps.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of thread dumps from the containerized application. For example:</p> <pre><code>docker exec my_crowd /opt/atlassian/support/thread-dumps.sh\n</code></pre> <p>By default this script will collect 10 thread dumps at 5 second intervals. This can be overridden by passing a custom value for the count and interval, by using <code>-c</code> / <code>--count</code> and <code>-i</code> / <code>--interval</code> respectively. For example, to collect 20 thread dumps at 3 second intervals:</p> <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh --count 20 --interval 3\n</code></pre> <p>Thread dumps will be written to <code>$APP_HOME/thread_dumps/&lt;date&gt;</code>.</p> Disable capturing output from top run <p>By default this script will also capture output from top run in 'Thread-mode'. This can be disabled by passing <code>-n</code> / <code>--no-top</code></p>"},{"location":"containers/CROWD/#heap-dump","title":"Heap dump","text":"<p><code>/opt/atlassian/support/heap-dump.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of a heap dump from the containerized application. For example:</p> <pre><code>docker exec my_container /opt/atlassian/support/heap-dump.sh\n</code></pre> <p>A heap dump will be written to <code>$APP_HOME/heap.bin</code>. If a file already exists at this location, use <code>-f</code> / <code>--force</code> to overwrite the existing heap dump file.</p>"},{"location":"containers/CROWD/#manual-diagnostics","title":"Manual diagnostics","text":"<p>The <code>jcmd</code> utility is also included in these images and can be used by starting a <code>bash</code> shell in the running container:</p> <pre><code>docker exec -it my_container /bin/bash\n</code></pre>"},{"location":"containers/CROWD/#support","title":"Support","text":"<p>For product support, go to:</p> <ul> <li>https://support.atlassian.com/crowd/</li> </ul> <p>You can also visit the Atlassian Data Center forum for discussion on running Atlassian Data Center products in containers.</p>"},{"location":"containers/CROWD/#changelog","title":"Changelog","text":"<p>For a detailed list of changes to the Docker image configuration see the Git commit history.</p>"},{"location":"containers/CROWD/#license","title":"License","text":"<p>Copyright \u00a9 2019 Atlassian Corporation Pty Ltd. Licensed under the Apache License, Version 2.0.</p>"},{"location":"containers/JIRA/","title":"Jira","text":""},{"location":"containers/JIRA/#overview","title":"Overview","text":"<p>Jira Software Data Center helps the world\u2019s best agile teams plan, track, and release great software at scale.</p> <ul> <li>Check out atlassian/jira-software on Docker Hub</li> <li>Learn more about Jira Software: https://www.atlassian.com/software/jira</li> </ul> <p>Jira Service Management Data Center is an enterprise ITSM solution that offers high availability, meeting your security and compliance needs so no request goes unresolved.</p> <ul> <li>Check out atlassian/jira-servicemanagement on Docker Hub</li> <li>Learn more about Jira Service Management: https://www.atlassian.com/software/jira/service-management</li> </ul> <p>Jira Core is a project and task management solution built for business teams.</p> <ul> <li>Check out atlassian/jira-core on Docker Hub</li> <li>Learn more about Jira Core: https://www.atlassian.com/software/jira/core</li> </ul> <p>This Docker container makes it easy to get an instance of Jira Software, Service Management or Core up and running.</p> <p>Note: Jira Software will be referenced in the examples provided.</p> <p>Use docker version &gt;= 20.10.10</p>"},{"location":"containers/JIRA/#quick-start","title":"Quick Start","text":"<p>For the <code>JIRA_HOME</code> directory that is used to store application data (amongst other things) we recommend mounting a host directory as a data volume, or via a named volume.</p> <p>Additionally, if running Jira in Data Center mode it is required that a shared filesystem is mounted. The mount point (inside the container) can be configured with <code>JIRA_SHARED_HOME</code>.</p> <p>To get started you can use a data volume, or named volumes. In this example we'll use named volumes.</p> <pre><code>docker volume create --name jiraVolume\ndocker run -v jiraVolume:/var/atlassian/application-data/jira --name=\"jira\" -d -p 8080:8080 atlassian/jira-software\n</code></pre> <p>Jira is now available on http://localhost:8080.</p> <p>Please ensure your container has the necessary resources allocated to it. We recommend 2GiB of memory allocated to accommodate the application server. See System Requirements for further information.</p> If you are using <code>docker-machine</code> on Mac OS X, please use <code>open http://$(docker-machine ip default):8080</code> instead."},{"location":"containers/JIRA/#configuring-jira","title":"Configuring Jira","text":"<p>This Docker image is intended to be configured from its environment; the provided information is used to generate the application configuration files from templates. This allows containers to be repeatably created and destroyed on-the-fly, as required in advanced cluster configurations. Most aspects of the deployment can be configured in this manner; the necessary environment variables are documented below. However, if your particular deployment scenario is not covered by these settings, it is possible to override the provided templates with your own; see the section Advanced Configuration below.</p>"},{"location":"containers/JIRA/#verbose-container-entrypoint-logging","title":"Verbose container entrypoint logging","text":"<p>During the startup process of the container, various operations and checks are performed to ensure that the application is configured correctly and ready to run. To help in troubleshooting and to provide transparency into this process, you can enable verbose logging. The <code>VERBOSE_LOGS</code> environment variable enables detailed debug messages to the container's log, offering insights into the actions performed by the entrypoint script.</p> <ul> <li><code>VERBOSE_LOGS</code> (default: false)</li> </ul> <p>Set to <code>true</code> to enable detailed debug messages during the container initialization.</p>"},{"location":"containers/JIRA/#memory-heap-size","title":"Memory / Heap Size","text":"<p>If you need to override Jira's default memory allocation, you can control the minimum heap (Xms) and maximum heap (Xmx) via the below environment variables.</p> <ul> <li><code>JVM_MINIMUM_MEMORY</code> (default: 384m)</li> </ul> <p>The minimum heap size of the JVM</p> <ul> <li><code>JVM_MAXIMUM_MEMORY</code> (default: 768m)</li> </ul> <p>The maximum heap size of the JVM</p> <ul> <li> <p><code>JVM_RESERVED_CODE_CACHE_SIZE</code> (default: 512m)</p> <p>The reserved code cache size of the JVM</p> </li> </ul>"},{"location":"containers/JIRA/#reverse-proxy-settings","title":"Reverse Proxy Settings","text":"<p>If Jira is run behind a reverse proxy server (e.g. a load-balancer or nginx server) as described here, then you need to specify extra options to make Jira aware of the setup. They can be controlled via the below environment variables.</p> <ul> <li><code>ATL_PROXY_NAME</code> (default: NONE)</li> </ul> <p>The reverse proxy's fully qualified hostname. <code>CATALINA_CONNECTOR_PROXYNAME</code>    is also supported for backwards compatability.</p> <ul> <li><code>ATL_PROXY_PORT</code> (default: NONE)</li> </ul> <p>The reverse proxy's port number via which Jira is    accessed. <code>CATALINA_CONNECTOR_PROXYPORT</code> is also supported for backwards    compatability.</p> <ul> <li><code>ATL_TOMCAT_PORT</code> (default: 8080)</li> </ul> <p>The port for Tomcat/Jira to listen on. Depending on your container    deployment method this port may need to be    exposed and published.</p> <ul> <li><code>ATL_TOMCAT_SCHEME</code> (default: http)</li> </ul> <p>The protocol via which Jira is accessed. <code>CATALINA_CONNECTOR_SCHEME</code> is also    supported for backwards compatability.</p> <ul> <li><code>ATL_TOMCAT_SECURE</code> (default: false)</li> </ul> <p>Set 'true' if <code>ATL_TOMCAT_SCHEME</code> is 'https'. <code>CATALINA_CONNECTOR_SECURE</code> is    also supported for backwards compatability.</p> <ul> <li><code>ATL_TOMCAT_CONTEXTPATH</code> (default: NONE)</li> </ul> <p>The context path the application is served over. <code>CATALINA_CONTEXT_PATH</code> is    also supported for backwards compatability.</p> <ul> <li><code>ATL_TOMCAT_REQUESTATTRIBUTESENABLED</code></li> </ul> <p>Checks for the existence of request attributes (typically set by the RemoteIpValve and similar)   that should be used to override the values returned by the request for remote address,   remote host, server port and protocol. This property is usually combined with <code>ATL_TOMCAT_TRUSTEDPROXIES</code>   and <code>ATL_TOMCAT_INTERNALPROXIES</code> to show IP address of the remote host instead of the load balancer's.   If not declared, the default value of <code>false</code> will be used.</p> <ul> <li><code>ATL_TOMCAT_TRUSTEDPROXIES</code></li> </ul> <p>A list of IP addresses separated by a pipe character e.g. <code>10.0.9.6|10.0.9.32</code>.   Trusted proxies that appear in the <code>remoteIpHeader</code> will be trusted and will appear   in the <code>proxiesHeader</code> value. By adding a list of Trusted Proxies, Confluence will remove the   load balancers' IP addresses from Confluence's view of the incoming connection. This could be desired   in a clustered load balancer architecture where the load balancer address changes depending on   which node proxies the connection.   If not specified, no trusted proxies will be trusted.</p> <ul> <li><code>ATL_TOMCAT_INTERNALPROXIES</code></li> </ul> <p>A list of IP addresses separated by a pipe character e.g. <code>10.0.9.6|10.0.9.32</code>.   Trusted proxies that appear in the <code>remoteIpHeader</code> will be trusted and will not appear   in the <code>proxiesHeader</code> value. By adding a list of Internal Proxies, Confluence will remove the   load balancers' IP addresses from Confluence's view of the incoming connection. This could be desired   in a clustered load balancer architecture where the load balancer address changes depending on   which node proxies the connection.   If not specified, no internal proxies will be trusted.</p> <ul> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_PORT</code> - If unset, no additional connector is defined in server.xml. No default.</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_CONNECTION_TIMEOUT</code> (default: 20000)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_MAX_THREADS</code> (default: 50)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_MIN_SPARE_THREADS</code> (default: 10)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_ENABLE_LOOKUPS</code> (default: false)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_ACCEPT_COUNT</code> (default: 10)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_URI_ENCODING</code> (default: UTF-8)</li> <li><code>ATL_TOMCAT_ADDITIONAL_CONNECTOR_SECURE</code> (no default, if undefined secure is not set in the connector)</li> </ul> <p>The following Tomcat/Catalina options are also supported. For more information, see https://tomcat.apache.org/tomcat-7.0-doc/config/index.html.</p> <ul> <li><code>ATL_TOMCAT_MGMT_PORT</code> (default: 8005)</li> <li><code>ATL_TOMCAT_MAXTHREADS</code> (default: 100)</li> <li><code>ATL_TOMCAT_MINSPARETHREADS</code> (default: 10)</li> <li><code>ATL_TOMCAT_CONNECTIONTIMEOUT</code> (default: 20000)</li> <li><code>ATL_TOMCAT_ENABLELOOKUPS</code> (default: false)</li> <li><code>ATL_TOMCAT_PROTOCOL</code> (default: HTTP/1.1)</li> <li><code>ATL_TOMCAT_ACCEPTCOUNT</code> (default: 10)</li> <li><code>ATL_TOMCAT_MAXHTTPHEADERSIZE</code> (default: 8192)</li> <li><code>ATL_TOMCAT_STUCKTHREADDETECTIONVALVE_THRESHOLD</code> (default: 120)</li> </ul>"},{"location":"containers/JIRA/#access-log-settings","title":"Access Log Settings","text":"<p>You can set the maximum number of days for access logs to be retained before being deleted. The default value of -1 means never delete old files.</p> <ul> <li><code>ATL_TOMCAT_ACCESS_LOGS_MAXDAYS</code> (default: -1)</li> <li><code>ATL_TOMCAT_ACCESS_LOG_PATTERN</code> (default: <code>%a %{jira.request.id}r %{jira.request.username}r %t &amp;quot;%m %U%q %H&amp;quot; %s %b %D &amp;quot;%{Referer}i&amp;quot; &amp;quot;%{User-Agent}i&amp;quot; &amp;quot;%{jira.request.assession.id}r&amp;quot;</code>)</li> </ul>"},{"location":"containers/JIRA/#jvm-configuration","title":"JVM configuration","text":"<p>If you need to pass additional JVM arguments to Jira, such as specifying a custom trust store, you can add them via the below environment variable</p> <ul> <li><code>JVM_SUPPORT_RECOMMENDED_ARGS</code></li> </ul> <p>Additional JVM arguments for Jira</p> Example <p><code>docker run -e JVM_SUPPORT_RECOMMENDED_ARGS=-Djavax.net.ssl.trustStore=/var/atlassian/application-data/jira/cacerts -v jiraVolume:/var/atlassian/application-data/jira --name=\"jira\" -d -p 8080:8080 atlassian/jira-software</code></p>"},{"location":"containers/JIRA/#jira-specific-settings","title":"Jira-specific settings","text":"<ul> <li><code>ATL_AUTOLOGIN_COOKIE_AGE</code> (default: 1209600; two weeks, in seconds)</li> </ul> <p>The maximum time a user can remain logged-in with 'Remember Me'.</p>"},{"location":"containers/JIRA/#s3-avatars-storage-configuration","title":"S3 Avatars storage configuration","text":"<p>Starting with Jira 9.9, you can configure Jira to store avatar files in Amazon S3. For requirements and additional  information, please refer to  Configuring Amazon S3 Object Storage.</p> <ul> <li><code>ATL_S3AVATARS_BUCKET_NAME</code></li> </ul> <p>Bucket name to store avatars.</p> <ul> <li><code>ATL_S3AVATARS_REGION</code></li> </ul> <p>AWS region where the S3 bucket is located.</p> <ul> <li><code>ATL_S3AVATARS_ENDPOINT_OVERRIDE</code></li> </ul> <p>Override the default AWS API endpoint with a custom one (optional).</p>"},{"location":"containers/JIRA/#s3-attachments-storage-configuration","title":"S3 Attachments storage configuration","text":"<p>Starting with Jira 9.9, you can configure Jira to store attachment files in Amazon S3. For requirements and additional information, please refer to Configuring Amazon S3 Object Storage.</p> <ul> <li><code>ATL_S3ATTACHMENTS_BUCKET_NAME</code></li> </ul> <p>Bucket name to store avatars.</p> <ul> <li><code>ATL_S3ATTACHMENTS_REGION</code></li> </ul> <p>AWS region where the S3 bucket is located.</p> <ul> <li><code>ATL_S3ATTACHMENTS_ENDPOINT_OVERRIDE</code></li> </ul> <p>Override the default AWS API endpoint with a custom one (optional).</p> <p>Avatars and attachments can be stored in the same bucket. If bucket names are identical, <code>ATL_S3AVATARS_*</code> env vars are used to generate a common filestore configuration.</p>"},{"location":"containers/JIRA/#s3-backups-storage-configuration","title":"S3 Backups storage configuration","text":"<p>Starting with 9.16, you can configure Jira to store backups in Amazon S3.</p> <ul> <li><code>ATL_S3BACKUPS_BUCKET_NAME</code></li> </ul> <p>Bucket name to store avatars.</p> <ul> <li><code>ATL_S3BACKUPS_REGION</code></li> </ul> <p>AWS region where the S3 bucket is located.</p> <ul> <li><code>ATL_S3BACKUPS_ENDPOINT_OVERRIDE</code></li> </ul> <p>Override the default AWS API endpoint with a custom one (optional).</p>"},{"location":"containers/JIRA/#database-configuration","title":"Database configuration","text":"<p>It is optionally possible to configure the database from the environment, avoiding the need to do so through the web startup screen.</p> <p>The following variables are all must all be supplied if using this feature:</p> <ul> <li><code>ATL_JDBC_URL</code></li> </ul> <p>The database URL; this is database-specific.</p> <ul> <li><code>ATL_JDBC_USER</code></li> </ul> <p>The database user to connect as.</p> <ul> <li><code>ATL_JDBC_PASSWORD</code></li> </ul> <p>The password for the database user.</p> <ul> <li><code>ATL_DB_DRIVER</code></li> </ul> <p>The JDBC driver class; supported drivers are:</p> <ul> <li><code>com.microsoft.sqlserver.jdbc.SQLServerDriver</code></li> <li><code>com.mysql.jdbc.Driver</code></li> <li><code>oracle.jdbc.OracleDriver</code></li> <li><code>org.postgresql.Driver</code></li> </ul> <p>The driver must match the DB type (see next entry).</p> <ul> <li><code>ATL_DB_TYPE</code></li> </ul> <p>The type of database; valid supported values are:</p> <ul> <li><code>mssql</code></li> <li><code>mysql</code></li> <li><code>mysql57</code></li> <li><code>mysql8</code></li> <li><code>oracle10g</code></li> <li><code>postgres72</code></li> </ul> MySQL supportability <p><code>mysql</code> is only supported for versions prior to 8.13, and <code>mysql57</code> and <code>mysql8</code> are only supported after.  See the 8.13.x upgrade instructions for details.</p> <p>The following variables may be optionally supplied when configuring the database from the environment:</p> <ul> <li><code>ATL_DB_SCHEMA_NAME</code></li> </ul> <p>The schema name of the database. Depending on the value of <code>ATL_DB_TYPE</code>,    the following default values are used if no schema name is specified:</p> <ul> <li><code>mssql</code>: <code>dbo</code></li> <li><code>mysql</code>: NONE</li> <li><code>mysql57</code>: NONE</li> <li><code>mysql8</code>: NONE</li> <li><code>oracle10g</code>: NONE</li> <li><code>postgres72</code>: <code>public</code></li> </ul> MySQL or Oracle JDBC drivers <p>Due to licensing restrictions Jira does not ship with MySQL or Oracle JDBC drivers.  To use these databases you will need to copy a suitable driver into the container and restart it.  For example, to copy the MySQL driver into a container named \"jira\", you would do the following:</p> <p><code>docker cp mysql-connector-java.x.y.z.jar jira:/opt/atlassian/jira/lib</code></p> <p><code>docker restart jira</code></p> <p>For more information see the page Startup check: JIRA database driver missing.</p>"},{"location":"containers/JIRA/#optional-database-settings","title":"Optional database settings","text":"<ul> <li><code>ATL_JDBC_SECRET_CLASS</code></li> </ul> <p>Encryption class for the database password. Depending on the secret class, the value of <code>ATL_JDBC_PASSWORD</code> will differ. Defaults to plaintext.</p> <p>Starting from 9.11 AWS SecretsManager is supported.</p> <p>IMPORTANT: to start using password encryption for Jira instances that have already been set up, make sure <code>ATL_FORCE_CFG_UPDATE</code> is set to true which will force the image entrypoint to regenerate <code>dbconfig.xml</code> with the new properties. Other database environment variables must be also set in the container:</p> <pre><code>docker run -v jiraVolume:/var/atlassian/application-data/jira --name='jira' -d -p 8080:8080 \\\n  -e ATL_JDBC_URL=jdbc:postgresql://172.17.0.1:5432/jira \\\n  -e ATL_JDBC_USER='jira' -e ATL_DB_DRIVER='org.postgresql.Driver' \\\n  -e ATL_DB_TYPE='postgres72' \\\n  -e ATL_JDBC_SECRET_CLASS='com.atlassian.secrets.store.aws.AwsSecretsManagerStore' \\\n  -e ATL_JDBC_PASSWORD='{\"region\": \"us-east-1\", \"secretId\": \"mysecret\", \"secretPointer\": \"/password\"}' \\\n  -e ATL_FORCE_CFG_UPDATE='true' atlassian/jira-software\n</code></pre> <p>The following variables are for the Tomcat JDBC connection pool, and are optional. For more information on these see: https://tomcat.apache.org/tomcat-7.0-doc/jdbc-pool.html</p> <ul> <li><code>ATL_DB_MAXIDLE</code> (default: 20)</li> <li><code>ATL_DB_MAXWAITMILLIS</code> (default: 30000)</li> <li><code>ATL_DB_MINEVICTABLEIDLETIMEMILLIS</code> (default: 5000)</li> <li><code>ATL_DB_MINIDLE</code> (default: 10)</li> <li><code>ATL_DB_POOLMAXSIZE</code> (default: 100)</li> <li><code>ATL_DB_POOLMINSIZE</code> (default: 20)</li> <li><code>ATL_DB_REMOVEABANDONED</code> (default: true)</li> <li><code>ATL_DB_REMOVEABANDONEDTIMEOUT</code> (default: 300)</li> <li><code>ATL_DB_TESTONBORROW</code> (default: false)</li> <li><code>ATL_DB_TESTWHILEIDLE</code> (default: true)</li> <li><code>ATL_DB_TIMEBETWEENEVICTIONRUNSMILLIS</code> (default: 30000)</li> <li><code>ATL_DB_VALIDATIONQUERY</code> (default: select 1)</li> </ul> <p>The following settings only apply when using the Postgres driver:</p> <ul> <li><code>ATL_DB_KEEPALIVE</code> (default: true)</li> <li><code>ATL_DB_SOCKETTIMEOUT</code> (default: 240)</li> </ul> <p>The following settings only apply when using the MySQL driver:</p> <ul> <li><code>ATL_DB_VALIDATIONQUERYTIMEOUT</code> (default: 3)</li> </ul>"},{"location":"containers/JIRA/#data-center-configuration","title":"Data Center configuration","text":"<p>This docker image can be run as part of a Data Center cluster. You can specify the following properties to start Jira as a Data Center node, instead of manually configuring a cluster.properties file, See Installing Jira Data Center for more information on each property and its possible configuration.</p>"},{"location":"containers/JIRA/#cluster-configuration","title":"Cluster configuration","text":"<p>Jira Software and Jira Service Management only</p> <ul> <li><code>CLUSTERED</code> (default: false)</li> </ul> <p>Set 'true' to enable clustering configuration to be used. This will create a    <code>cluster.properties</code> file inside the container's <code>$JIRA_HOME</code> directory.</p> <ul> <li><code>JIRA_NODE_ID</code> (default: jira_node_) <p>The unique ID for the node. By default, this includes a randomly generated ID    unique to each container, but can be overridden with a custom value.</p> <ul> <li><code>JIRA_SHARED_HOME</code> (default: $JIRA_HOME/shared)</li> </ul> <p>The location of the shared home directory for all Jira nodes. Note: This    must be real shared filesystem that is mounted inside the    container. Additionally, see the note about UIDs.</p> <ul> <li><code>EHCACHE_PEER_DISCOVERY</code> (default: default)</li> </ul> <p>Describes how nodes find each other.</p> <ul> <li><code>EHCACHE_LISTENER_HOSTNAME</code> (default: NONE)</li> </ul> <p>The hostname of the current node for cache communication. Jira Data Center    will resolve this this internally if the parameter isn't set.</p> <ul> <li><code>EHCACHE_LISTENER_PORT</code> (default: 40001)</li> </ul> <p>The port the node is going to be listening to. Depending on your container    deployment method this port may need to be exposed and published.</p> <ul> <li><code>EHCACHE_OBJECT_PORT</code> (default: dynamic)</li> </ul> <p>The port number on which the remote objects bound in the registry receive    calls. This defaults to a free port if not specified. This port may need to    be exposed and published.</p> <ul> <li><code>EHCACHE_LISTENER_SOCKETTIMEOUTMILLIS</code> (default: 2000)</li> </ul> <p>The default timeout for the Ehcache listener.</p> <ul> <li><code>EHCACHE_MULTICAST_ADDRESS</code> (default: NONE)</li> </ul> <p>A valid multicast group address. Required when EHCACHE_PEER_DISCOVERY is set    to 'automatic' instead of 'default'.</p> <ul> <li><code>EHCACHE_MULTICAST_PORT</code> (default: NONE)</li> </ul> <p>The dedicated port for the multicast heartbeat traffic. Required when    EHCACHE_PEER_DISCOVERY is set to 'automatic' instead of 'default'.  Depending    on your container deployment method this port may need to be    exposed and published.</p> <ul> <li><code>EHCACHE_MULTICAST_TIMETOLIVE</code> (default: NONE)</li> </ul> <p>A value between 0 and 255 which determines how far the packets will    propagate. Required when EHCACHE_PEER_DISCOVERY is set to 'automatic' instead    of 'default'.</p> <ul> <li><code>EHCACHE_MULTICAST_HOSTNAME</code> (default: NONE)</li> </ul> <p>The hostname or IP of the interface to be used for sending and receiving    multicast packets. Required when EHCACHE_PEER_DISCOVERY is set to 'automatic'    instead of 'default'.</p>"},{"location":"containers/JIRA/#shared-directory-and-user-ids","title":"Shared directory and user IDs","text":"<p>By default, the Jira application runs as the user <code>jira</code>, with a UID and GID of 2001. Consequently, this UID must have write access to the shared filesystem. If for some reason a different UID must be used, there are a number of options available:</p> <ul> <li>The Docker image can be rebuilt with a different UID.</li> <li>Under Linux, the UID can be remapped using   user namespace remapping.</li> </ul> <p>To preserve strict permissions for certain configuration files, this container starts as <code>root</code> to perform bootstrapping before running Jira under a non-privileged user account. If you wish to start the container as a non-root user, please note that Tomcat configuration will be skipped and a warning will be logged. You may still apply custom configuration in this situation by mounting a custom server.xml file directly to <code>/opt/atlassian/jira/conf/server.xml</code></p> <p>Database and Clustering bootstrapping will work as expected when starting this container as a non-root user.</p>"},{"location":"containers/JIRA/#container-configuration","title":"Container configuration","text":"<ul> <li><code>ATL_FORCE_CFG_UPDATE</code> (default: false)</li> </ul> <p>The Docker entrypoint generates application configuration on    first start; not all of these files are regenerated on subsequent    starts. This is deliberate, to avoid race conditions or overwriting manual    changes during restarts and upgrades. However in deployments where    configuration is purely specified through the environment (e.g. Kubernetes)    this behaviour may be undesirable; this flag forces an update of all    generated files.</p> <p>In Jira the affected files are: <code>dbconfig.xml</code></p> <p>See the entrypoint code for the details of how configuration    files are generated.</p> <ul> <li><code>ATL_ALLOWLIST_SENSITIVE_ENV_VARS</code></li> </ul> <p>Define a comma separated list of environment variables containing keywords 'PASS', 'SECRET' or 'TOKEN' to be ignored by the unset function which is executed in the entrypoint. The function uses <code>^</code> regex. For example, if you set <code>ATL_ALLOWLIST_SENSITIVE_ENV_VARS=\"PATH_TO_SECRET_FILE\"</code>, all variables starting with <code>PATH_TO_SECRET_FILE</code> will not be unset.</p> Value exposure on host OS <p>When using this property, the values to sensitive environment variables will be available in clear text on the  host OS. As such, this data may be exposed to users or processes running on the host OS.</p> <ul> <li><code>SET_PERMISSIONS</code> (default: true)</li> </ul> <p>Define whether to set home directory permissions on startup. Set to <code>false</code> to disable    this behaviour.</p> <ul> <li><code>ATL_UNSET_SENSITIVE_ENV_VARS</code> (default: true)</li> </ul> <p>Define whether to unset environment variables containing keywords 'PASS', 'SECRET' or 'TOKEN'.   The unset function is executed in the entrypoint. Set to <code>false</code> if you want to allow passing   sensitive environment variables to Jira container.</p> Value exposure on host OS <p>When using this property, the values to sensitive environment variables will be available in clear text on the  host OS. As such, this data may be exposed to users or processes running on the host OS.</p> <ul> <li><code>ATL_JIRA_CLEAR_PLUGIN_CACHE</code></li> </ul> <p>When set to true, plugin cache at <code>${JIRA_HOME}/plugins/.bundled-plugins</code> and <code>${JIRA_HOME}/plugins/.osgi-plugins</code>   will be deleted before Jira starts. Once set, make sure to unset this environment variable if you don't want Jira plugin cache to be flushed   every time the container starts.</p> <p>See: How to clear Jira's plugin cache.</p> <ul> <li><code>ATL_JIRA_SESSION_TIMEOUT</code></li> </ul> <p>The default Tomcat session timeout (in minutes) for all newly created sessions which is set in web.xml. Defaults to 30.</p>"},{"location":"containers/JIRA/#advanced-configuration","title":"Advanced Configuration","text":"<p>As mentioned at the top of this section, the settings from the environment are used to populate the application configuration on the container startup. However, in some cases you may wish to customise the settings in ways that are not supported by the environment variables above. In this case, it is possible to modify the base templates to add your own configuration. There are three main ways of doing this; modify our repository to your own image, build a new image from the existing one, or provide new templates at startup. We will briefly outline these methods here, but in practice how you do this will depend on your needs.</p>"},{"location":"containers/JIRA/#building-your-own-image","title":"Building your own image","text":"<ul> <li>Clone the Atlassian repository at https://bitbucket.org/atlassian-docker/docker-atlassian-jira/</li> <li>Modify or replace the Jinja templates   under <code>config</code>; NOTE: The files must have the <code>.j2</code> extensions. However you   don't have to use template variables if you don't wish.</li> <li>Build the new image with e.g: <code>docker build --tag my-jira-8-image --build-arg JIRA_VERSION=8.x.x .</code></li> <li>Optionally push to a registry, and deploy.</li> </ul>"},{"location":"containers/JIRA/#build-a-new-image-from-the-existing-one","title":"Build a new image from the existing one","text":"<ul> <li>Create a new <code>Dockerfile</code>, which starts with the line e.g: <code>FROM   atlassian/jira-software:latest</code>.</li> <li>Use a <code>COPY</code> line to overwrite the provided templates.</li> <li>Build, push and deploy the new image as above.</li> </ul>"},{"location":"containers/JIRA/#overwrite-the-templates-at-runtime","title":"Overwrite the templates at runtime","text":"<p>There are two main ways of doing this:</p> <ul> <li>If your container is going to be long-lived, you can create it, modify the   installed templates under <code>/opt/atlassian/etc/</code>, and then run it.</li> <li>Alternatively, you can create a volume containing your alternative templates,   and mount it over the provided templates at runtime   with <code>--volume my-config:/opt/atlassian/etc/</code>.</li> </ul>"},{"location":"containers/JIRA/#logging","title":"Logging","text":"<p>By default the Jira logs are written inside the container, under <code>${JIRA_HOME}/logs/</code>. If you wish to expose this outside the container (e.g. to be aggregated by logging system) this directory can be a data volume or bind mount. Additionally, Tomcat-specific logs are written to <code>/opt/atlassian/jira/logs/</code>.</p>"},{"location":"containers/JIRA/#upgrades","title":"Upgrades","text":"<p>To upgrade to a more recent version of Jira you can simply stop the <code>jira</code> container and start a new one based on a more recent image:</p> <pre><code>docker stop jira\ndocker rm jira\ndocker run ... (See above)\n</code></pre> <p>As your data is stored in the data volume directory on the host it will still  be available after the upgrade.</p> <p>Please make sure that you don't accidentally remove the <code>jira</code> container and its volumes using the <code>-v</code> option.</p>"},{"location":"containers/JIRA/#backup","title":"Backup","text":"<p>For evaluations you can use the built-in database that will store its files in the Jira home directory. In that case it is sufficient to create a backup archive of the docker volume.</p> <p>If you're using an external database, you can configure Jira to make a backup automatically each night. This will back up the current state, including the database to the <code>jiraVolume</code> docker volume, which can then be archived. Alternatively you can backup the database separately, and continue to create a backup archive of the docker volume to back up the Jira Home directory.</p> <p>Read more about data recovery and backups: https://confluence.atlassian.com/adminjiraserver071/backing-up-data-802592964.html</p>"},{"location":"containers/JIRA/#shutdown","title":"Shutdown","text":"<p>Depending on your configuration Jira may take a short period to shutdown any active operations to finish before termination. If sending a <code>docker stop</code> this should be taken into account with the <code>--time</code> flag.</p> <p>Alternatively, the script <code>/shutdown-wait.sh</code> is provided, which will initiate a clean shutdown and wait for the process to complete. This is the recommended method for shutdown in environments which provide for orderly shutdown, e.g. Kubernetes via the <code>preStop</code> hook.</p>"},{"location":"containers/JIRA/#versioning","title":"Versioning","text":"<p>The <code>latest</code> tag matches the most recent release of Atlassian Jira Software, Jira Core or Jira Service Management. Thus <code>atlassian/jira-software:latest</code> will use the newest version of Jira available.</p> <p>Alternatively you can use a specific major, major.minor, or major.minor.patch version of Jira by using a version number tag:</p> <ul> <li><code>atlassian/jira-software:8</code></li> <li><code>atlassian/jira-software:8.14</code></li> <li> <p><code>atlassian/jira-software:8.14.0</code></p> </li> <li> <p><code>atlassian/jira-servicemanagement:4</code></p> </li> <li><code>atlassian/jira-servicemanagement:4.14</code></li> <li> <p><code>atlassian/jira-servicemanagement:4.14.0</code></p> </li> <li> <p><code>atlassian/jira-core:8</code></p> </li> <li><code>atlassian/jira-core:8.14</code></li> <li><code>atlassian/jira-core:8.14.0</code></li> </ul> <p>All Jira versions from 7.13+ (Software/Core) / 3.16+ (Service Management) are available.</p> <code>atlassian/jira-servicedesk</code> deprecation <p>All Jira Service Management 4.x versions are also available as <code>atlassian/jira-servicedesk</code>.  This namespace has been deprecated and versions from 5+ onwards will only be available as <code>atlassian/jira-servicemanagement</code>.</p>"},{"location":"containers/JIRA/#supported-jdk-versions-and-base-images","title":"Supported JDK versions and base images","text":"<p>Atlassian Docker images are generated from either official Eclipse Temurin OpenJDK Docker images or Red Hat Universal Base Images. </p> <p>UBI based images are only published from Jira 9.5 onwards and JDK17 only. Tags are available in 2 formats: <code>&lt;version&gt;-ubi9</code> and <code>&lt;version&gt;-ubi9-jdk17</code>.</p> <p>The Docker images follow the Atlassian Support end-of-life policy; images for unsupported versions of the products remain available but will no longer receive updates or fixes.</p> <p>Historically, we have also generated other versions of the images, including JDK8, Alpine, and 'slim' versions of the JDK. These legacy images still exist in Docker Hub, however they should be considered deprecated, and do not receive updates or fixes.</p> <p>If for some reason you need a different version, see Building your own image above.</p>"},{"location":"containers/JIRA/#migration-to-ubi","title":"Migration to UBI","text":"<p>If you have been mounting any files to <code>${JAVA_HOME}</code> directory in <code>eclipse-temurin</code> based container, <code>JAVA_HOME</code> in UBI JDK17 container is set to <code>/usr/lib/jvm/java-17</code>.</p> <p>Also, if you have been mounting and running any custom scripts in the container, UBI-based images may lack some tools and utilities that are available out of the box in eclipse-temurin tags. If that's the case, see Building your own image.</p>"},{"location":"containers/JIRA/#supported-architectures","title":"Supported architectures","text":"<p>Currently, Jira and JSM container images are built for the <code>linux/amd64</code> and <code>linux/arm64</code> target platforms. The Dockerfiles and support tooling have now had all architecture-specific components removed, so if necessary it is possible to build images for any platform supported by OCI-compliant container runtimes.</p>"},{"location":"containers/JIRA/#building-on-the-target-architecture","title":"Building on the target architecture","text":"<p>Note: This method is known to work on Mac M1 and AWS ARM64 machines, but has not been extensively tested.</p> <p>The simplest method of getting a platform image is to build it on a target machine. The following assumes you have git and Docker installed. You will also need to know which version of Jira you want to build; substitute <code>JIRA_VERSION=x.x.x</code> with your required version:</p> <p><pre><code>git clone --recurse-submodule https://bitbucket.org/atlassian-docker/docker-atlassian-jira.git\ncd docker-atlassian-jira\ndocker build --tag my-image --build-arg JIRA_VERSION=x.x.x .\n</code></pre> This image can be pushed up to your own Docker Hub or private repository.</p>"},{"location":"containers/JIRA/#troubleshooting","title":"Troubleshooting","text":"<p>These images include built-in scripts to assist in performing common JVM diagnostic tasks.</p>"},{"location":"containers/JIRA/#thread-dumps","title":"Thread dumps","text":"<p><code>/opt/atlassian/support/thread-dumps.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of thread dumps from the containerized application. For example:</p> <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh\n</code></pre> <p>By default, this script will collect 10 thread dumps at 5 second intervals. This can be overridden by passing a custom value for the count and interval, by using <code>-c</code> / <code>--count</code> and <code>-i</code> / <code>--interval</code> respectively. For example, to collect 20 thread dumps at 3 second intervals:</p> <pre><code>docker exec my_container /opt/atlassian/support/thread-dumps.sh --count 20 --interval 3\n</code></pre> <p>Thread dumps will be written to <code>$APP_HOME/thread_dumps/&lt;date&gt;</code>.</p> Disable capturing output from top run <p>By default this script will also capture output from top run in 'Thread-mode'. This can be disabled by passing <code>-n</code> / <code>--no-top</code></p>"},{"location":"containers/JIRA/#heap-dump","title":"Heap dump","text":"<p><code>/opt/atlassian/support/heap-dump.sh</code> can be run via <code>docker exec</code> to easily trigger the collection of a heap dump from the containerized application. For example:</p> <pre><code>docker exec my_container /opt/atlassian/support/heap-dump.sh\n</code></pre> <p>A heap dump will be written to <code>$APP_HOME/heap.bin</code>. If a file already exists at this location, use <code>-f</code> / <code>--force</code> to overwrite the existing heap dump file.</p>"},{"location":"containers/JIRA/#manual-diagnostics","title":"Manual diagnostics","text":"<p>The <code>jcmd</code> utility is also included in these images and can be used by starting a <code>bash</code> shell in the running container:</p> <pre><code>docker exec -it my_container /bin/bash\n</code></pre>"},{"location":"containers/JIRA/#support","title":"Support","text":"<p>For product support, go to:</p> <ul> <li>https://support.atlassian.com/jira-software-server/</li> <li>https://support.atlassian.com/jira-service-management-server/</li> <li>https://support.atlassian.com/jira-core-server/</li> </ul> <p>You can also visit the Atlassian Data Center forum for discussion on running Atlassian Data Center products in containers.</p>"},{"location":"containers/JIRA/#development-and-testing","title":"Development and testing","text":"<p>See Development for details on setting up a development environment and running tests.</p>"},{"location":"containers/JIRA/#changelog","title":"Changelog","text":"<p>For a detailed list of changes to the Docker image configuration see the Git commit history.</p>"},{"location":"containers/JIRA/#license","title":"License","text":"<p>Copyright \u00a9 2020 Atlassian Corporation Pty Ltd. Licensed under the Apache License, Version 2.0.</p>"},{"location":"examples/EXAMPLES/","title":"Available examples","text":"<p>Support disclaimer</p> <p>Use the examples we provide as reference only, we don\u2019t offer official support for them. </p>"},{"location":"examples/EXAMPLES/#pre-requisites","title":"Pre-requisites","text":""},{"location":"examples/EXAMPLES/#kubernetes-clusters","title":"Kubernetes clusters","text":"<p>See examples of provisioning Kubernetes clusters on cloud-based providers:</p> <ul> <li>Amazon EKS </li> <li>Google GKE</li> <li>Azure AKS</li> </ul>"},{"location":"examples/EXAMPLES/#ingress","title":"Ingress","text":"<ul> <li>See an example of provisioning an NGINX Ingress controller</li> </ul>"},{"location":"examples/EXAMPLES/#database","title":"Database","text":"<ul> <li>See an example of creating an Amazon RDS database instance</li> </ul>"},{"location":"examples/EXAMPLES/#storage","title":"Storage","text":"AWS EBSAWS EFSNFS <ul> <li>See an example of local storage utilizing AWS EBS-backed volumes</li> </ul> <ul> <li>See an example of shared storage utilizing AWS EFS-backed filesystem</li> </ul> <ul> <li>See an example of standing up an NFS server for Bitbucket</li> </ul>"},{"location":"examples/EXAMPLES/#bamboo","title":"Bamboo","text":""},{"location":"examples/EXAMPLES/#remote-agents","title":"Remote agents","text":"<ul> <li>See an example of deploying a remote agent for Bamboo</li> </ul>"},{"location":"examples/EXAMPLES/#bitbucket","title":"Bitbucket","text":""},{"location":"examples/EXAMPLES/#elasticsearch","title":"Elasticsearch","text":"<ul> <li>See an example of standing up an Elasticsearch instance for Bitbucket</li> </ul>"},{"location":"examples/EXAMPLES/#smart-mirrors","title":"Smart Mirrors","text":"<ul> <li>See an example of Bitbucket Smart Mirrors</li> </ul>"},{"location":"examples/EXAMPLES/#ssh","title":"SSH","text":"<ul> <li>See an example of SSH service in Bitbucket on Kubernetes</li> </ul>"},{"location":"examples/EXAMPLES/#mesh","title":"Mesh","text":"<ul> <li>See an example of Bitbucket Mesh on Kubernetes</li> </ul>"},{"location":"examples/EXAMPLES/#other","title":"Other","text":""},{"location":"examples/EXAMPLES/#logging","title":"Logging","text":"<ul> <li>See an example of how to deploy an EFK stack to Kubernetes</li> </ul>"},{"location":"examples/EXAMPLES/#prometheus-monitoring","title":"Prometheus Monitoring","text":"<ul> <li>See an example of how to monitor DC products with Prometheus</li> </ul>"},{"location":"examples/EXAMPLES/#customization","title":"Customization","text":"<ul> <li>See an example of External libraries and plugins</li> </ul>"},{"location":"examples/bamboo/AGENT_CAPABILITIES/","title":"Agent capabilities","text":"<p>A capability is a feature of an agent. A capability can be defined on an agent for:</p> <ul> <li>an executable (e.g. Maven)</li> <li>a JDK</li> <li>a Version Control System client application (e.g. Git)</li> </ul> <p>You can learn more about remote agents capabilities on the official documentation page.</p>"},{"location":"examples/bamboo/AGENT_CAPABILITIES/#custom-capabilities","title":"Custom capabilities","text":"<p>Default capabilities</p> <p>By default the Bamboo agent Helm chart will deploy the bamboo-agent-base Docker image. This image provides the following capabilities out of the box:</p> <ul> <li>JDK 11</li> <li>Git &amp; Git LFS</li> <li>Maven 3</li> <li>Python 3</li> </ul> <p>If additional capabilities are required, the Bamboo agent base Docker image can be extended with those capabilities. </p> <p>This custom image can be used, by first updating the Bamboo agent <code>values.yaml</code> with the image <code>tag</code> of the custom Docker image i.e.</p> <pre><code>image:\n  repository: hoolicorp/bamboo-agent-base\n  pullPolicy: IfNotPresent\n  tag: \"ruby-agent\"\n</code></pre> <p>The custom agent can then be deployed via Helm:</p> <pre><code>helm install ruby-agent atlassian-data-center/bamboo-agent -f ruby-agent.yaml\n</code></pre>"},{"location":"examples/bamboo/REMOTE_AGENTS/","title":"Remote agents","text":"<p>Remote agents can be provisioned to a Kubernetes cluster to run jobs delegated to them via a Bamboo server. An agent can run a job if its capabilities match the requirements of a job. Each job inherits the requirements from individual tasks that it contains.</p> <p>You can learn more details about remote agents on the official documentation page.</p>"},{"location":"examples/bamboo/REMOTE_AGENTS/#requirements","title":"Requirements","text":"<p>Bamboo server prerequisites</p> <ul> <li>The Bamboo server instance must use a valid Bamboo Data Center instance license and be fully configured</li> <li>The Bamboo server instance must have <code>security token verification</code> enabled</li> <li>The Bamboo server instance must have <code>remote agent authentication</code> disabled</li> </ul>"},{"location":"examples/bamboo/REMOTE_AGENTS/#deployment","title":"Deployment","text":"<p>Steps required for deploying a remote agent</p> <ol> <li>Configure Bamboo server for remote agent support</li> <li>Deploy agent</li> </ol>"},{"location":"examples/bamboo/REMOTE_AGENTS/#1-configure-bamboo-server","title":"1. Configure Bamboo server","text":"<p>There are 2 approaches for doing this:</p> <ul> <li>Automatically when deploying Bamboo server</li> <li>Manually via Bamboo server agent settings</li> </ul>"},{"location":"examples/bamboo/REMOTE_AGENTS/#automatically","title":"Automatically","text":"<p>When initially deploying Bamboo server its <code>values.yaml</code> can be configured to:</p> <ul> <li>disable <code>remote agent authentication</code></li> <li>define a custom <code>security token</code> </li> </ul> <p>This will allow remote agents that are configured with the same security token to automatically join the cluster. </p> <p>First, create a secret to store a custom security token with which remote agent(s) should authenticate to the Bamboo server. </p> <p>Security token format</p> <p>The security token should be set to a 40-character hexadecimal string. The following command can be used to generate a string in this format: <pre><code>xxd -l 20 -p /dev/urandom\n</code></pre></p> <p>Add the generated string (security token) to a K8s secret</p> <pre><code>kubectl create secret generic security-token --from-literal=security-token=&lt;security token&gt;\n</code></pre> <p>Update the Bamboo <code>values.yaml</code> with this secret and disable agent authentication:</p> <pre><code>bamboo:\n  securityToken:\n    secretName: \"security-token\"\n    secretKey: security-token\n  disableAgentAuth: true\n</code></pre> <p>Disabling remote agent authentication</p> <p>when setting the property <code>disableAgentAuth</code> to <code>true</code> this will have the effect of automatically allowing agents with the correct security token to communicate with the Bamboo server. This property is useful for testing, and when deployments requiring many agents are needed. This property can also be left in its default state of <code>false</code> in which case each agent will need to be approved manually via the <code>Agents</code> settings tab of the Bamboo server instance. Additional details on agent authentication can be found here </p>"},{"location":"examples/bamboo/REMOTE_AGENTS/#manually","title":"Manually","text":"<ul> <li> <p>When logged into the Bamboo server instance, and from the <code>Agents</code> settings tab, enable <code>security token verification</code>, and disable <code>remote agent authentication</code> </p> </li> <li> <p>Navigate to the remote agent's installation page by selecting the <code>Install remote agent</code> button from the <code>Agents</code> settings tab    </p> </li> <li> <p>Create a K8s secret using the <code>security token</code> rendered on the <code>Installing a remote agent</code> page    </p> </li> </ul> <p>create secret using token...</p> <pre><code>kubectl create secret generic security-token --from-literal=security-token=&lt;security token&gt;\n</code></pre>"},{"location":"examples/bamboo/REMOTE_AGENTS/#2-deploy-agent","title":"2. Deploy agent","text":"<ul> <li>Update the bamboo agent <code>values.yaml</code> to utilize the security token secret and point to the bamboo server instance</li> </ul> <pre><code>replicaCount: 3\nagent:\n  securityToken:\n    secretName: \"security-token\"\n    secretKey: security-token\n  server: \"bamboo.bamboo.svc.cluster.local\"\n</code></pre> <p>Values</p> <ul> <li>As long as your cluster has the physical resources the <code>replicaCount</code> can be set to any value from <code>1</code> .. <code>1 + n</code> </li> <li><code>agent.server</code> should be configured with the K8s DNS record for the Bamboo server service. The value should be of the form: <code>&lt;service_name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> </ul> <ul> <li>Install the agent</li> </ul> <pre><code>helm install bamboo-agent atlassian-data-center/bamboo-agent -f values.yaml\n</code></pre> <p>Custom agents</p> <pre><code>By default the Bamboo agent Helm chart will deploy the [bamboo-agent-base](https://hub.docker.com/r/atlassian/bamboo-agent-base){.external} Docker image. This image provides the following capabilities out of the box:\n\n* JDK 11\n* Git &amp; Git LFS\n* Maven 3\n* Python 3\n\nFor details on defining and deploying agents with custom/additional capabilities view the [agent capabilities guide](AGENT_CAPABILITIES.md)\n</code></pre>"},{"location":"examples/bamboo/REMOTE_AGENTS/#scaling-the-agent-count","title":"Scaling the agent count","text":"<p>The number of active agents can be easily increased or decreased: </p> <pre><code>helm upgrade --set replicaCount=&lt;desired number of agents&gt; \\\n             --reuse-values \\\n             &lt;name of the release&gt;\n             atlassian-data-center/bamboo-agent\n</code></pre>"},{"location":"examples/bamboo/REMOTE_AGENTS/#troubleshooting","title":"Troubleshooting","text":"<p>You can find the most common errors relating to agent configuration in the official Bamboo agent documentation.</p>"},{"location":"examples/bitbucket/BITBUCKET_AWS_OPENSEARCH/","title":"Configure AWS OpenSearch with Bitbucket","text":""},{"location":"examples/bitbucket/BITBUCKET_AWS_OPENSEARCH/#configuration-example","title":"Configuration Example","text":"<pre><code>bitbucket:\n  # Disable default OpenSearch deployment\n  opensearch:\n    enabled: false\n\n  # Configure AWS OpenSearch connection\n  additionalEnvironmentVariables:\n    - name: SEARCH_ENABLED\n      value: \"true\"\n    - name: PLUGIN_SEARCH_CONFIG_BASEURL\n      value: \"https://your-opensearch-endpoint.region.es.amazonaws.com\"\n    - name: PLUGIN_SEARCH_CONFIG_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: aws-opensearch-credentials\n          key: username\n    - name: PLUGIN_SEARCH_CONFIG_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: aws-opensearch-credentials\n          key: password\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_AWS_OPENSEARCH/#create-aws-opensearch-credentials-secret","title":"Create AWS OpenSearch Credentials Secret","text":"<pre><code>kubectl create secret generic aws-opensearch-credentials \\\n  --from-literal=username=your-username \\\n  --from-literal=password=your-password \\\n  -n your-namespace\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/","title":"Elasticsearch (Deprecated)","text":"<p>Elasticsearch deprecation notice</p> <p>Elasticsearch has been deprecated as a search platform for Bitbucket. Use OpenSearch instead.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#bitbucket-elasticsearch-recommendations","title":"Bitbucket Elasticsearch recommendations","text":"<p>While Bitbucket has its own internal Elasticsearch instance, we highly recommend you use an external Elasticsearch installation, either within the Kubernetes cluster or, if available, an instance managed by your hosting provider.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#installing-and-configuring-elasticsearch-in-your-kubernetes-cluster","title":"Installing and configuring Elasticsearch in your Kubernetes cluster","text":""},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#installing-elasticsearch-into-your-kubernetes-cluster","title":"Installing Elasticsearch into your Kubernetes cluster","text":"<p>Choose a version of Elasticsearch that is supported by the version of Bitbucket you are installing. For Bitbucket 7.14 the latest supported Elasticsearch version is 7.9.3, so we will target that.</p> <p>There are official Helm charts for Elasticsearch 7.9.3. Following the documentation there add the Elasticsearch Helm charts repository:</p> <p><pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre> then install it: <pre><code>helm install elasticsearch --namespace &lt;namespace&gt; --set imageTag=\"7.9.3\" elastic/elasticsearch\n</code></pre></p> <p>Prerequisites of Elasticsearch Helm chart</p> <p>Running the above commands will install Elasticsearch with the default configuration, which is 3 worker nodes.  However, it may not always work out of the box if failed to fulfill prerequisites for the default installation.  Some example prerequisites include:</p> <ul> <li>CPU/memory requests: 1000m/2Gi (for each worker node)</li> <li>Preconfigured storage volumes (30Gi for each worker node)</li> </ul> <p>For more details refer to Elasticsearch values.yaml file.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment","title":"Configuring your Bitbucket deployment","text":"<p>To enable the installed Elasticsearch service you need to configure the service URL under <code>bitbucket:</code> stanza in the <code>values.yaml</code> file. Check the Kubernetes official documentation on how to get DNS record for a service. <pre><code>bitbucket:\n  elasticSearch:\n    baseUrl: http://elasticsearch-master.&lt;namespace&gt;.svc.cluster.local:9200\n</code></pre> This will also have the effect of disabling Bitbucket\u2019s internal Elasticsearch instance.</p> <p>Elasticsearch security</p> <p>If you have Elasticsearch cluster with security enabled, i.e. having credential details stored in a Kubernetes secret and passed into <code>extraEnvs</code> as this example does, you can then use the same secret and configure that in the bitbucket <code>values.yaml</code> file:      <pre><code>bitbucket:\n  elasticSearch:    \n     credentials:\n        secretName: &lt;my-elasticsearch-secret&gt;\n        usernameSecretKey: username\n        passwordSecretKey: password\n</code></pre> Read about Kubernetes secrets.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#configuring-amazon-elasticsearch-service-with-bitbucket-on-kubernetes","title":"Configuring Amazon Elasticsearch Service with Bitbucket on Kubernetes","text":""},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#creating-an-amazon-elasticsearch-service-domain-with-a-master-user","title":"Creating an Amazon Elasticsearch Service domain with a master user","text":"<p>The Elasticsearch instance (\u201cdomain\u201d) can be created via the AWS CLI or the web console; for this example we will use the web console and a master user:</p> <ol> <li>In the EKS console navigate to Your Cluster \u2192 Networking and note the VPC ID.</li> <li>In the Elasticsearch console create a new domain:</li> <li>Select a production deployment.</li> <li>Select Elasticsearch version 7.9.</li> <li>In the next screen configure the AZs and nodes as appropriate for your expected workload.</li> <li>On the Access and security page:</li> <li>Select the same VPC as the EKS cluster, as noted in step 1.</li> <li>Select appropriate subnets for each AZ; private subnets are fine.</li> <li>Select appropriate security groups that will grant node/pod access.</li> <li>Tick Fine\u2013grained access control:<ul> <li>Select Create master user and add a username and a strong password.</li> </ul> </li> <li>Configure tags, etc. as appropriate for your organisation.</li> </ol> <p>Once the Elasticsearch domain has finished creating, make a note of the VPC Endpoint, which will be an HTTPS URL.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#configuring-your-bitbucket-deployment_1","title":"Configuring your Bitbucket deployment","text":"<p>To use the managed Elasticsearch service, first create a Kubernetes secret using the username and password from step 4 above. Then configure the service URL under <code>bitbucket:</code> in the <code>values.yaml</code> file, substituting the values below from the above steps where appropriate: <pre><code>bitbucket:\n  elasticSearch:\n    baseUrl: &lt;VPC Endpoint&gt;\n    credentials:\n      secretName: &lt;my-elasticsearch-secret&gt;\n      usernameSecretKey: username\n      passwordSecretKey: password\n</code></pre></p> <p>Read about Kubernetes secrets.</p>"},{"location":"examples/bitbucket/BITBUCKET_ELASTICSEARCH/#testing-your-elasticsearch-connection","title":"Testing your Elasticsearch connection","text":"<p>To test if Elasticsearch is properly set up, go to Administration &gt; System - Server settings. The Elasticsearch URL should be pre-populated already in the search section. Click the Test button to see if it connects successfully. </p>"},{"location":"examples/bitbucket/BITBUCKET_MESH/","title":"Bitbucket Mesh","text":"<p>Bitbucket Mesh is a distributed, replicated, and horizontally scalable Git repository storage system, which increases performance and improves the resilience of Bitbucket.</p> <p></p> <p>You can learn more details about Bitbucket Mesh on the official documentation page.</p> <p>Recommendations for Mesh deployments</p> <p>Bitbucket version</p> <p>By default, the Helm charts target the latest Bitbucket LTS version. However, Mesh is only supported from version 8.0. You will need to select an 8.x version of Bitbucket to deploy Mesh. Learn more details below.</p> <p>Mesh agent version</p> <p>Bitbucket Mesh agents are versioned independently from Bitbucket. You should select the appropriate version for the deployed version of Bitbucket. See the Mesh download page for available versions.</p> <p>Number of Mesh nodes</p> <p>In order for high-availability to be possible, we recommend having a minimum of three Mesh nodes. There is no maximum on the number of nodes.</p> <p>Mesh node co-location</p> <p>We don't currently support deploying Mesh nodes into multiple availability zones. Just like the shared file system based deployments, the Mesh nodes (that is, the repository storage) and the application nodes must be co-located.</p> <p>Other Mesh deployment requirements</p> <p>For more details on the requirements and limitations of Mesh deployments, check the the Bitbucket Mesh FAQ.</p>"},{"location":"examples/bitbucket/BITBUCKET_MESH/#configuring-your-bitbucket-and-mesh-deployment","title":"Configuring your Bitbucket and Mesh deployment","text":"<p>For backwards compatibility, the Helm charts default to Mesh being disabled. To enable it, you will need to configure the service under <code>bitbucket:</code> stanza in the <code>values.yaml</code> file, substituting the values below from the above steps where appropriate:</p> <pre><code>image:\n  tag: &lt;an 8.x.x version of Bitbucket&gt;\n\nbitbucket:\n  mesh:\n    enabled: true\n    image:\n      version: &lt;Mesh agent version&gt;\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_MESH/#adding-the-mesh-nodes-to-bitbucket","title":"Adding the Mesh nodes to Bitbucket","text":"<p>To enable the deployed Mesh nodes you need to add them to the Bitbucket Data Center instance in the administration area. To do so, you'll need the service URL of each node; these are usually of the form <code>bitbucket-mesh-&lt;num&gt;</code>. Check the Kubernetes official documentation to learn how to get a DNS record for a service.</p> <p>To connect the Mesh node:</p> <ol> <li>In your Bitbucket Data Center instance, navigate to Administration &gt; Git &gt; Bitbucket Mesh.</li> <li>Enter the URL of the Mesh node in the Node URL field (e.g. <code>http://bitbucket-mesh-1:7777</code>).</li> <li>(Optional) Enter a name for the Mesh node in the Node name field.</li> <li>Select Add Mesh node.</li> </ol> <p>Learn more details about Mesh configuration.</p>"},{"location":"examples/bitbucket/BITBUCKET_MESH/#migrating-existing-repositories-to-mesh","title":"Migrating existing repositories to Mesh","text":"<p>Learn how to migrate repositories to Mesh.</p>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/","title":"Smart Mirroring","text":"<p>Smart Mirroring can greatly improve Git clone speeds for distributed teams working with large repositories. Large repositories that take hours to clone from a Bitbucket instance over the Internet from the other side of the world can take minutes when cloned from a local mirror on a fast network.</p> <p></p> <p>You can learn more details about smart mirroring on the official documentation page.</p> <p>Upstream/Primary instance</p> <p>Primary instance is sometimes called upstream instance.</p>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#requirements","title":"Requirements","text":"<p>Primary instance prerequisites</p> <p>Your primary Bitbucket instance must be a fully licensed Bitbucket Data Center instance</p> <p>You do not have to run your Bitbucket Data Center instance as a multi-node cluster to use smart mirroring, but you must have an up-to-date Data Center license.</p> <p>The primary instance and all mirror(s) must have HTTPS with a valid (i.e., signed by a Certificate Authority anchored to the root and not expired) SSL certificate</p> <p>This is a strict requirement of smart mirroring on both the primary instance and all mirror(s), and cannot be bypassed. The mirror setup wizard will not proceed if either the mirror or the primary instance does not have a valid SSL certificate.</p> <p>The primary Bitbucket instance must have SSH enabled</p> <p>Mirrors keep their repositories synchronized with the primary instance over SSH and cannot use HTTP or HTTPS for this. See Enable SSH access to Git repositories for instructions on enabling SSH access on your primary instance.</p>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#installation","title":"Installation","text":""},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#overview","title":"Overview","text":"<ol> <li>Install the primary as usual with Bitbucket Helm chart.<ul> <li>You need to make sure the instance complies with all the above listed requirements.</li> </ul> </li> <li>Install the mirror with second Bitbucket Helm chart.<ul> <li>There is a set of properties that need to be configured to make the mirror work.</li> </ul> </li> <li>Approve the mirror in the primary instance.</li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#steps","title":"Steps","text":""},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#mirror-farm-installation-in-kubernetes","title":"Mirror Farm installation in Kubernetes","text":"<p>Info</p> <p>Example is using nginx-ingress controller. If you are using a different ingress controller, you will need to modify the example.</p> <ol> <li>Install the primary as usual with Bitbucket Helm chart.<ul> <li>You need to make sure the instance complies with all the above listed requirements.</li> <li>Verify that you are able to clone from the primary via SSH protocol.</li> <li>Verify that primary instance is accessible over HTTPS with a valid SSL certificate.</li> </ul> </li> <li>Create a new file <code>values-mirror.yaml</code> with the following content: <pre><code>bitbucket:\n  mode: mirror\n  displayName: Bitbucket Mirror\n  clustering:\n    enabled: true\n  applicationMode: \"mirror\"\n  mirror:\n    upstreamUrl: &lt;https url of the primary&gt; # for example https://bitbucket-upstream.example.com\n\n# nginx specific configuration\ningress:\n  create: true\n  host: bitbucket-mirror.example.com\n  annotations:\n    cert-manager.io/issuer: \"letsencrypt-prod\" # Default issuer\n  tlsSecretName: &lt;secret with TLS private key&gt; # E.g. tls-certificate-mirror\n\n# enables persistence for mirror data\nvolumes:\n  localHome:\n    persistentVolumeClaim:\n      create: true\n</code></pre></li> <li>Edit the file to change the placeholder values.<ul> <li><code>bitbucket.mirror.upstreamUrl</code></li> <li><code>ingress.host</code></li> <li><code>ingress.tlsSecretName</code></li> </ul> </li> <li>Install the mirror     <pre><code>  helm install bitbucket-mirror atlassian-data-center/bitbucket -f values-mirror.yaml\n</code></pre></li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#mirror-farm-authorization","title":"Mirror farm authorization","text":"<p>These steps are described in detail in official documentation.</p> <ol> <li>Visit the mirror URL (it might take a couple of minutes to come up)</li> <li>Click on Go to the primary server in the mirror UI. The link will take you to the administration section of the primary instance. </li> <li>Click Authorize next to the mirror </li> <li>Select which projects should be synchronized </li> <li>Wait for the projects to be synchronized </li> <li>Verify that the synchronized projects can be cloned from the mirror </li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#scaling-the-mirror-farm","title":"Scaling the mirror farm","text":"<p>As the mirror is deployed with all the fulfilled requirements for the Bitbucket Mirror Farm, you are able to scale the mirrors easily. To increase or decrease the size of the mirror farm:</p> <pre><code>helm upgrade --set replicaCount=&lt;desired number of mirror nodes&gt; \\\n             --reuse-values \\\n             &lt;name of the release&gt;\n             atlassian-data-center/bitbucket\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#adding-or-removing-additional-mirror-farms","title":"Adding or removing additional mirror farms","text":"<p>It is possible to connect multiple mirror farms to a single primary instance. This can be useful to improve local performance for geographically distributed teams.</p> <p>To add a new mirror farm, follow the same steps that were necessary to connect the first mirror farm. This means installing another helm release and authenticating it in the administrator user interface.</p> <p>To remove a mirror farm:</p> <ol> <li>Navigate to Mirrors administration section on the primary</li> <li>Select the mirror from the list</li> <li>Click Delete button</li> <li>Uninstall the deleted mirror helm release from the cluster</li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_MIRRORS/#troubleshooting","title":"Troubleshooting","text":"<p>You can find the most common errors in mirror configuration described in the official Bitbucket documentation.</p>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/","title":"OpenSearch","text":"<p>Helm chart version</p> <p>OpenSearch sub-chart is supported in Bitbucket Helm chart version 1.20 onwards.</p>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#deploy-opensearch-helm-chart-with-bitbucket","title":"Deploy OpenSearch Helm chart with Bitbucket","text":"<p>Support disclaimer</p> <p>Atlassian does not officially support OpenSearch Helm chart that can be installed with the Bitbucket Helm release. Should you encounter any issues with the deployment, maintenance and upgrades, reach out to the vendor. Moreover, if you intend to deploy OpenSearch to a critical Kubernetes environment, make sure you follow all the best practices, i.e. deploy a multi node cluster, use taints and tolerations, affinity rules, sufficient resources requests, have DR and backup strategies etc.</p> <p>To deploy OpenSearch Helm chart and automatically configure Bitbucket to use it as a search platform, set the following in your Helm values file:</p> <p><pre><code>opensearch:\n  install: true\n</code></pre> This will:</p> <ul> <li>auto-generate the initial OpenSearch admin password and create a Kubernetes secret with <code>OPENSEARCH_INITIAL_ADMIN_PASSWORD</code> key</li> <li>deploy OpenSearch Helm chart to the target namespace with the default settings: single node, no SSL, 1Gi memory/1 vCPU resources requests, 10Gi storage request</li> <li>set <code>PLUGIN_SEARCH_CONFIG_BASEURL</code> to <code>http://opensearch-cluster-master:9200</code> unless overridden in <code>opensearch.baseUrl</code></li> <li>set <code>PLUGIN_SEARCH_CONFIG_USERNAME</code> to <code>admin</code> (this is the initial admin user created when the OpenSearch cluster starts for the very first time)</li> <li>set <code>PLUGIN_SEARCH_CONFIG_PASSWORD</code> to a randomly generated password saved to <code>opensearch-initial-password</code> secret</li> </ul>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#create-dedicated-opensearch-user","title":"Create dedicated OpenSearch user","text":"<p>When the Helm chart is installed with the default values, OpenSearch admin user is created with an auto-generated password, and Bitbucket is configured to use these credentials to connect to OpenSearch. If you want to have a more fine-grained control over internal users, you may pre-create a secret with a list of users, their credentials and roles. See internal_users.yml for more details.</p>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#hash-passwords","title":"Hash passwords","text":"<p>Passwords in <code>internal_users.yml</code> must be hashed. You can use <code>hash.sh</code> script bundled with Bitbucket to hash your passwords, for example:</p> <p><pre><code>docker run atlassian/bitbucket:latest /bin/bash -c \"chmod +x /opt/atlassian/bitbucket/opensearch/plugins/opensearch-security/tools/hash.sh &amp;&amp; /opt/atlassian/bitbucket/opensearch/plugins/opensearch-security/tools/hash.sh -p mySecureAdminPassword123\"\n\n$2y$12$bcUjaXcfutyYwkjp6r/RdePrywC3BmQLKvN77XuuR0PJs0qjBooSv\n</code></pre> If Bitbucket is already up and running in your Kubernetes cluster, you can run exec into the Bitbucket container to hash a password:</p> <pre><code>kubectl exec -ti bitbucket-0 -n atlassian -- /bin/bash -c  \"chmod +x /opt/atlassian/bitbucket/opensearch/plugins/opensearch-security/tools/hash.sh &amp;&amp; /opt/atlassian/bitbucket/opensearch/plugins/opensearch-security/tools/hash.sh -p mySecureBitbucketPassword123\"\n\n$2y$12$x910YF09tcfhNs009vOzWOMy9fswhpsuV5/AiiPwbY4rp5BXKv2tv\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#create-internal_usersyml-file","title":"Create internal_users.yml file","text":"<p>This is the minimal <code>internal_users.yml</code> file with an admin and a dedicated bitbucket user. See internal_users.yml to get more details about users, roles and role mappings in OpenSearch.</p> <pre><code>_meta:\n  type: \"internalusers\"\n  config_version: 2\n\nadmin:\n  hash: \"$2y$12$bcUjaXcfutyYwkjp6r/RdePrywC3BmQLKvN77XuuR0PJs0qjBooSi\"\n  reserved: true\n  backend_roles:\n  - \"admin\"\n  description: \"Demo admin user\"\n\nbitbucket:\n  hash: \"$2y$12$x910YF09tcfhNs009vOzWOMy9fswhpsuV5/AiiPwbY4rp5BXKv2tu\"\n  reserved: true\n  backend_roles:\n    - \"admin\"\n  description: \"Bitbucket admin user\"\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#create-kubernetes-secret","title":"Create Kubernetes secret","text":"<p>Create a Kubernetes secret with 3 keys in its data:</p> <p>internal_users.yml</p> <p>Make sure there are no typos in <code>internal_users.yml</code> filename, otherwise OpenSearch pods can't be created due to a missing key in the secret.</p> <pre><code>kubectl create secret generic opensearch-internal-users \\\n      --from-literal=username=bitbucket \\\n      --from-literal=password=\"mySecureBitbucketPassword123\" \\\n      --from-file=internal_users.yml=/path/to/internal_users.yml \\\n      -n atlassian\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#update-helm-values","title":"Update Helm values","text":"<p>Now that the secret has been created, update your Helm values to point OpenSearch and Bitbucket to <code>opensearch-internal-users</code> secret:</p> <pre><code>opensearch:\n  install: true\n  credentials:\n    secretName: opensearch-internal-users\n    usernameSecretKey: username\n    passwordSecretKey: password\n  securityConfig:\n    internalUsersSecret: opensearch-internal-users\n</code></pre> <p>If necessary, you can create 2 separate secrets - one for the <code>internal_users.yml</code> only, and one with the actual non-hashed OpenSearch credentials that Bitbucket will use.</p>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#override-opensearch-helm-chart-values","title":"Override OpenSearch Helm chart values","text":"<p>You can configure your OpenSearch cluster and the deployment options by overriding any values that the Helm chart exposes. OpenSearch values must be nested under <code>opensearch</code> stanza in your Helm values file, for example:</p> <pre><code>opensearch:\n  singleNode: false\n  replicas: 5\n  config:\n    opensearch.yml: |\n      cluster.name: custom-cluster\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#enable-ssl-with-custom-certificates","title":"Enable SSL with Custom Certificates","text":"<p>By default, OpenSearch starts with the SSL http plugin disabled, meaning the cluster is accessible via HTTP at http://opensearch-cluster-master:9200. The OpenSearch service is not exposed through a LoadBalancer or Ingress unless the default configurations are explicitly overridden. Bitbucket communicates with the OpenSearch cluster using the service name within the internal Kubernetes network. This setup uses the Kubernetes DNS to resolve the service name to the appropriate cluster IP address.</p> <p>To enable SSL in OpenSearch and start the service on a secure port, you need to: * Enable SSL HTTPS in Helm values. * Create Kubernetes secrets with <code>ca</code>, <code>certificate</code> and <code>key</code>, and pass them to OpenSearch. * Add <code>ca.crt</code> to Java trust store in Bitbucket containers if the custom certificate is not signed by a public authority.</p> <p>Below is an example (not the recommended way) of how to generate a CA certificate, a server certificate, and a corresponding private key for securing communications with OpenSearch:</p> <pre><code>openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 -subj \"/C=US/ST=CA/L=LA/O=MyCompany Name/CN=opensearch-cluster-master\" -keyout ca.key -out ca.crt\nopenssl req -new -newkey rsa:2048 -nodes -keyout opensearch-cluster-master.key -subj \"/C=US/ST=CA/L=LA/O=MyCompany Name/CN=opensearch-cluster-master\" -out opensearch-cluster-master.csr\nopenssl x509 -req -days 365 -in opensearch-cluster-master.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out opensearch-cluster-master.crt\n</code></pre> <p>It is important to generate the certificate for <code>opensearch-cluster-master</code> CN because this is the actual OpenSearch service/host name in the target namespace.</p> <p>Once done, create 3 Kubernetes secrets:</p> <pre><code>kubectl create secret generic opensearch-ssl-ca -n atlassian --from-file=root-ca.pem=ca.crt\nkubectl create secret generic opensearch-ssl-key -n atlassian --from-file=esnode-key.pem=opensearch-cluster-master.key\nkubectl create secret generic opensearch-ssl-cert -n atlassian --from-file=esnode.pem=opensearch-cluster-master.crt\n</code></pre> <p>Enable ssl http plugin, override the default ca, certificate and key, as well as provide additional volume mounts in Bitbucket Helm values file:</p> <p><pre><code>opensearch:\n  extraEnvs:\n    - name: plugins.security.ssl.http.enabled\n      value: \"true\"\n    - name: plugins.security.ssl.transport.pemkey_filepath\n      value: \"esnode-key.pem\"\n    - name: plugins.security.ssl.transport.pemcert_filepath\n      value: \"esnode.pem\"\n    - name: plugins.security.ssl.transport.pemtrustedcas_filepath\n      value: \"root-ca.pem\"\n  secretMounts:\n  - name: opensearch-ssl-cert\n    secretName: opensearch-ssl-cert\n    path: /usr/share/opensearch/config/esnode.pem\n    subPath: esnode.pem\n  - name: opensearch-ssl-key\n    secretName: opensearch-ssl-key\n    path: /usr/share/opensearch/config/esnode-key.pem\n    subPath: esnode-key.pem\n  - name: opensearch-ssl-ca\n    secretName: opensearch-ssl-ca\n    path: /usr/share/opensearch/config/root-ca.pem\n    subPath: root-ca.pem\n</code></pre> When using a self-signed certificate or a certificate not issued/signed by a recognized public certificate authority (CA), it is essential to add the CA certificate to the Java trust store in all Bitbucket containers:</p> <pre><code>bitbucket:\n  additionalCertificates:\n    secretName: opensearch-ssl-ca\n</code></pre> <p>Additionally, you need to update the protocol specified in the baseUrl to HTTPS:</p> <pre><code>bitbucket:\n  additionalCertificates:\n    secretName: opensearch-ssl-ca\nopensearch:\n  baseUrl: https://opensearch-cluster-master:9200\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#enable-ingress","title":"Enable Ingress","text":"<p>To communicate with the OpenSearch cluster through a fully qualified domain name rather than via Kubernetes internal DNS name, you can enable an Ingress in the OpenSearch Helm chart. Below is an example of how to configure Ingress and update base URL in the Helm values file:</p> <p><pre><code>opensearch:\n  baseUrl: https://myopensearch.com\n  ingress:\n    install: true\n</code></pre> Important considerations:</p> <ul> <li>Ensure that the <code>baseUrl</code> is set to use HTTPS protocol, which encrypts the data exchanged with the OpenSearch cluster</li> <li>Ensure that the Ingress hostname can be resolved from within the Kubernetes cluster network</li> <li>If the certificate is not signed by a public authority, you will need to add the certificate to Java trust store in Bitbucket containers by defining a secret name in <code>bitbucket.additionalCertificates.secretName</code></li> </ul>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#use-aws-opensearch-service","title":"Use AWS OpenSearch Service","text":"<p>If you're using AWS OpenSearch Service (managed service) instead of deploying your own OpenSearch cluster, configure Bitbucket to connect to your existing AWS OpenSearch domain using environment variables.</p>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create an AWS OpenSearch Service domain</li> <li>Configure domain access policy to allow your EKS cluster</li> <li>Have the domain endpoint URL and credentials ready</li> </ul>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#configuration","title":"Configuration","text":"<pre><code>bitbucket:\n  clustering:\n    enabled: true\n  # Do not enable the opensearch helm chart\n  opensearch:\n    enabled: false\n\n  additionalEnvironmentVariables:\n    - name: SEARCH_ENABLED\n      value: \"false\"\n    - name: PLUGIN_SEARCH_CONFIG_BASEURL\n      value: \"https://search-your-domain-abc123.us-east-1.es.amazonaws.com\"\n    - name: PLUGIN_SEARCH_CONFIG_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: aws-opensearch-credentials\n          key: username\n    - name: PLUGIN_SEARCH_CONFIG_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: aws-opensearch-credentials\n          key: password\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#create-aws-opensearch-credentials-secret","title":"Create AWS OpenSearch Credentials Secret","text":"<pre><code>kubectl create secret generic aws-opensearch-credentials \\\n  --from-literal=username=your-master-username \\\n  --from-literal=password=your-master-password \\\n  -n your-namespace\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#using-iam-authentication","title":"Using IAM Authentication","text":"<p>For domains using IAM authentication, configure IAM roles and omit username/password:</p> <pre><code>bitbucket:\n  clustering:\n    enabled: true\n  opensearch:\n    enabled: false\n\n  additionalEnvironmentVariables:\n    - name: SEARCH_ENABLED\n      value: \"false\"\n    - name: PLUGIN_SEARCH_CONFIG_BASEURL\n      value: \"https://search-your-domain-abc123.us-east-1.es.amazonaws.com\"\n    # No username/password needed - uses IAM role\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#authentication","title":"Authentication","text":"<p>If you run into auth issues (401 response from OpenSearch), check the following:</p> <ul> <li> <p>exec into the Bitbucket container and run:   <pre><code>curl -v -u ${PLUGIN_SEARCH_CONFIG_USERNAME}:${PLUGIN_SEARCH_CONFIG_PASSWORD} http://opensearch-cluster-master:9200/_cat/indices?v\n</code></pre>   If you get 401, then the user or password passed to the Bitbucket container does not match user or password defined in the <code>internal_users.yml</code></p> </li> <li> <p>you can find out where the actual values of <code>PLUGIN_SEARCH_CONFIG_USERNAME</code> and <code>PLUGIN_SEARCH_CONFIG_PASSWORD</code> environment variables come from by running:</p> <p><pre><code>kubectl get pod bitbucket-0 -n atlassian -o jsonpath=\"{.spec.containers[0].env[?(@.name=='PLUGIN_SEARCH_CONFIG_USERNAME')]}\"\nkubectl get pod bitbucket-0 -n atlassian -o jsonpath=\"{.spec.containers[0].env[?(@.name=='PLUGIN_SEARCH_CONFIG_PASSWORD')]}\"\n</code></pre> * Make sure that hashed password of the bitbucket user in the <code>internal_users.yml</code> file corresponds to the plain text password stored in the Kubernetes secret.</p> </li> </ul>"},{"location":"examples/bitbucket/BITBUCKET_OPENSEARCH/#networking","title":"Networking","text":"<p>The default <code>opensearch-cluster-master</code> hostname must be resolved to a Kubernetes service cluster IP, and the request is then forwarded directly to the pod endpoint. If you see connection refused/timeout errors, make sure all OpenSearch pods are in a Ready state and the corresponding endpoints have been created:</p> <pre><code>kubectl describe pod -n atlassian -l=app.kubernetes.io/component=opensearch-cluster-master\n</code></pre> <pre><code>kubectl get endpoints -n atlassian\n\nNAME                                 ENDPOINTS                                         AGE\nbitbucket                            10.0.2.31:7999,10.0.2.31:7990,10.0.2.31:5701      113m\nopensearch-cluster-master            10.0.1.170:9200,10.0.1.170:9300                   113m\nopensearch-cluster-master-headless   10.0.1.170:9600,10.0.1.170:9200,10.0.1.170:9300   113m\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_SESSION_AFFINITY/","title":"Configure Session Stickiness with NGINX Ingress","text":""},{"location":"examples/bitbucket/BITBUCKET_SESSION_AFFINITY/#default-configuration","title":"Default Configuration","text":"<pre><code>ingress:\n  create: true\n  nginx: true  # Enables session stickiness automatically\n  host: bitbucket.example.com\n\nbitbucket:\n  clustering:\n    enabled: true\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_SESSION_AFFINITY/#custom-configuration","title":"Custom Configuration","text":"<pre><code>ingress:\n  create: true\n  nginx: true\n  host: bitbucket.example.com\n  annotations:\n    \"nginx.ingress.kubernetes.io/session-cookie-name\": \"BITBUCKET-SESSION\"\n    \"nginx.ingress.kubernetes.io/session-cookie-max-age\": \"28800\"  # 8 hours\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_SESSION_AFFINITY/#verifying-session-stickiness","title":"Verifying Session Stickiness","text":"<p>Test that session stickiness is working:</p> <pre><code># Check ingress annotations\nkubectl describe ingress bitbucket -n atlassian\n\n# Test with curl (look for Set-Cookie header)\ncurl -I https://bitbucket.example.com/status\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_SESSION_AFFINITY/#troubleshooting","title":"Troubleshooting","text":"<p>If users are experiencing random logouts or Git operation failures:</p> <ol> <li> <p>Verify NGINX annotations are applied:    <pre><code>kubectl get ingress bitbucket -n atlassian -o yaml\n</code></pre></p> </li> <li> <p>Check if requests hit the same pod:    <pre><code>kubectl logs -f -l app.kubernetes.io/name=bitbucket -n atlassian\n</code></pre></p> </li> <li> <p>Ensure clustering is enabled:    <pre><code>bitbucket:\n  clustering:\n    enabled: true\n</code></pre></p> </li> </ol> <p>Production recommendation</p> <p>Set <code>session-cookie-max-age</code> to match your typical user session duration (e.g., 8 hours = 28800 seconds) to balance user experience with security. </p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/","title":"SSH service in Bitbucket on Kubernetes","text":"<p>In addition to providing a service on HTTP(S), Bitbucket also allows remote Git operations over SSH connections. By default, Kubernetes Ingress controllers only work for HTTP connections, but some ingress controllers also support TCP connections.</p> <p>Depending on the need of your deployment, SSH access can be provided through three mechanisms:</p> <ol> <li>Opening the TCP port through the ingress controller - This option should be used if the SSH service is required to be available on the same DNS name as the HTTP service.</li> <li>Creating a separate Kubernetes <code>LoadBalancer</code> service - This option is available if the ingress controller does not support TCP connections, or if you don\u2019t need your deployment to have the SSH service available on the same DNS name as the HTTP service.</li> <li>Exposing Bitbucket service as <code>LoadBalancer</code> and setting the desired ssh port (defaults to <code>7999</code>)</li> </ol>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#nginx-ingress-controller-config-for-ssh-connections","title":"NGINX Ingress controller config for SSH connections","text":"<p>We can follow the official documentation for the NGINX Ingress controller for this: Exposing TCP and UDP services - NGINX Ingress Controller.</p> <p>Namespace co-location</p> <p>These instructions should be performed in the same namespace in which the Ingress controller resides.</p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#1-create-configmap","title":"1. Create ConfigMap","text":"<p>Create a new <code>ConfigMap</code>: <pre><code>kubectl create configmap tcp-services\n</code></pre></p> <p>In our example we deployed Bitbucket using the Helm release name <code>bitbucket</code> in the namespace <code>ssh-test</code>, update the <code>ConfigMap</code> <code>tcp-services</code> accordingly:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tcp-services\n  namespace: ingress-nginx\ndata:\n  7999: \"&lt;bitbucket namespace&gt;/&lt;bitbucket helm release name&gt;:ssh\"\n</code></pre>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#2-update-ingress-deployment","title":"2. Update Ingress deployment","text":"<p>Next, we have to edit the <code>deployment</code> of the ingress controller and add the <code>--tcp-services-configmap</code> option: <pre><code>kubectl edit deployment &lt;name of ingress-nginx deployment&gt;\n</code></pre> Add this line in the <code>args</code> of the container <code>spec</code>: <pre><code>- --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n</code></pre> so it looks something like this: <pre><code> spec:\n   containers:\n   - args:\n     - /nginx-ingress-controller\n     - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller\n     - --election-id=ingress-controller-leader\n     - --ingress-class=nginx\n     - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller\n     - --validating-webhook=:8443\n     - --validating-webhook-certificate=/usr/local/certificates/cert\n     - --validating-webhook-key=/usr/local/certificates/key\n     - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services\n</code></pre></p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#3-update-the-ingress-service","title":"3. Update the Ingress service","text":"<p>Update the Ingress service to include an additional <code>port</code> definition for <code>ssh</code> <pre><code>kubectl edit service &lt;name of ingress-nginx service&gt;\n</code></pre> Add this section in the <code>ports</code> of the container <code>spec</code>: <pre><code>- name: ssh\n  port: 7999\n  protocol: TCP\n</code></pre> so it looks something like this: <pre><code>spec:\n  clusterIP: 10.100.19.60\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    nodePort: 31381\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    nodePort: 32612\n    port: 443\n    protocol: TCP\n    targetPort: https\n  - name: ssh\n    port: 7999\n    protocol: TCP\n</code></pre> After the deployment has been upgraded, the <code>SSH</code> service should be available on port <code>7999</code>.</p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#loadbalancer-service-for-ssh-connections-on-aws","title":"LoadBalancer service for SSH connections on AWS","text":"<p>In the values file for the helm chart, the extra SSH service can be enabled like this: <pre><code>bitbucket:\n  sshService:\n    enabled: true\n</code></pre> On a deployment using AWS, assuming you have external-dns configured, you can add these annotations to automatically set up the DNS name for the SSH service: <pre><code>bitbucket:\n  sshService:\n    enabled: true\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: bitbucket-ssh.example.com\n  additionalEnvironmentVariables:\n    - name: PLUGIN_SSH_BASEURL\n      value: ssh://bitbucket-ssh.example.com/\n</code></pre></p>"},{"location":"examples/bitbucket/BITBUCKET_SSH/#expose-bitbucket-service-as-a-loadbalancer","title":"Expose Bitbucket Service as a LoadBalancer","text":"<p>This method implies creation of one K8S service of a <code>LoadBalancer</code> type. A cloud provider (e.g. AWS) will create listeners for each port declared in the service. Here's an example of exposing Bitbucket service in EKS, using a Classic LoadBalancer and dynamically provisioning DNS entries with external-dns:</p> <pre><code>bitbucket:\n  service:\n    port: 443\n    sshPort: 22\n    type: LoadBalancer\n    annotations:\n      external-dns.alpha.kubernetes.io/hostname: bitbucket.example.com\n      service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\n      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:111111111111:certificate/8xy4ny81-0a4w-8caq-a524-1101cv3v4vwb\n  additionalEnvironmentVariables:\n    - name: PLUGIN_SSH_BASEURL\n      value: ssh://bitbucket.example.com\ningress:\n  host: bitbucket.example.com\n</code></pre> <p>The above service annotations are specific to the Classic LoadBalancer, however, you can provide NLB specific annotations as well.</p> <p>The default <code>bitbucket.service.sshPort</code> is set to <code>22</code> so that AWS can create a listener for this port, and as a result your ssh git URL will look like <code>ssh://bitbucket.example.com/project/repo</code>. </p> <p>Ingress host</p> <p>Even though <code>ingress</code> is disabled, <code>ingress.host</code> needs to be set because it is used in a few conditions in the StatefulSet template.</p>"},{"location":"examples/cluster/AKS_SETUP/","title":"Preparing an AKS cluster","text":"<p>This example provides instructions for creating a Kubernetes cluster using Azure AKS.</p>"},{"location":"examples/cluster/AKS_SETUP/#prerequisites","title":"Prerequisites","text":"<p>We recommend installing and configuring the Azure Cloud Shell, allowing for CLI interaction with the AKS cluster.</p>"},{"location":"examples/cluster/AKS_SETUP/#manual-creation","title":"Manual creation","text":"<p>Follow the Azure Kubernetes Service Quickstart for details on creating an AKS cluster.</p> <p>Next step - Ingress controller</p> <p>Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller.</p>"},{"location":"examples/cluster/CLOUD_PROVIDERS/","title":"Provisioning Kubernetes clusters on cloud-based providers","text":"<p>Here are installation and configuration instructions for cloud providers:</p> <ul> <li>Amazon EKS </li> <li>Google GKE</li> <li>Azure AKS</li> </ul>"},{"location":"examples/cluster/EKS_SETUP/","title":"Preparing an EKS cluster","text":"<p>This example provides instructions for creating a Kubernetes cluster using Amazon EKS.</p>"},{"location":"examples/cluster/EKS_SETUP/#prerequisites","title":"Prerequisites","text":"<p>We recommend installing and configuring eksctl, allowing for CLI interaction with the EKS cluster.</p>"},{"location":"examples/cluster/EKS_SETUP/#manual-creation","title":"Manual creation","text":"<p>Follow the Getting started with Amazon EKS for details on creating an EKS cluster. Or, using the <code>ClusterConfig</code> below as an example, deploy a K8s cluster with <code>eksctl</code> in ~20 minutes:</p> <pre><code>apiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: atlassian-cluster\n  region: ap-southeast-2\n\nmanagedNodeGroups:\n  - name: appNodes\n    instanceType: m5.large\n    desiredCapacity: 2\n    ssh: # enable SSH using SSM\n      enableSsm: true\n</code></pre> Cluster considerations <p>It's always a good idea to consider the following points before creating the cluster:</p> <ol> <li>Geographical region - where will the cluster reside.</li> <li>EC2 instance type - the instance type to be used for the nodes that make up the cluster.</li> <li>Number of nodes - guidance on the resource dimensions that should be used for these nodes can be found in Requests and limits.</li> </ol> <p>Adding the config above to a file named <code>config.yaml</code> provision the cluster: </p> <pre><code>eksctl create cluster -f config.yaml\n</code></pre> <p>Next step - Ingress controller</p> <p>Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller.</p>"},{"location":"examples/cluster/GKE_SETUP/","title":"Preparing an GKE cluster","text":"<p>This example provides instructions for creating a Kubernetes cluster using Google GKE.</p>"},{"location":"examples/cluster/GKE_SETUP/#prerequisites","title":"Prerequisites","text":"<p>We recommend installing and configuring Google Cloud SDK, allowing for CLI interaction with an GKE cluster.</p>"},{"location":"examples/cluster/GKE_SETUP/#manual-creation","title":"Manual creation","text":"<p>Follow the How-to guides for details on creating an GKE cluster. </p> <p>Next step - Ingress controller</p> <p>Having established a cluster, continue with provisioning the next piece of prerequisite infrastructure, the Ingress controller.</p>"},{"location":"examples/confluence/CONFLUENCE_OPENSEARCH/","title":"Configuring OpenSearch for Confluence","text":"<p>Confluence and Helm chart version</p> <p>OpenSearch is supported in Confluence 8.9.0 and Helm chart 1.19 onwards.</p> <p>As Confluence instances grow in size and scale, the default search engine, Lucene, may be slower to index and return search results. To address this, Confluence Data Center offers an alternative search engine as an opt-in feature \u2014 OpenSearch.</p> <p>Find more information on advantages of using OpenSearch in the official documentation</p>"},{"location":"examples/confluence/CONFLUENCE_OPENSEARCH/#deploy-opensearch-helm-chart-with-confluence","title":"Deploy OpenSearch Helm Chart with Confluence","text":"<p>Support disclaimer</p> <p>Atlassian does not officially support OpenSearch Helm chart that can be installed with the Confluence Helm release. Should you encounter any issues with the deployment, maintenance and upgrades, reach out to the vendor. Moreover, if you intend to deploy OpenSearch to a critical Kubernetes environment, make sure you follow all the best practices, i.e. deploy a multi node cluster, use taints and tolerations, affinity rules, sufficient resources requests, have DR and backup strategies etc. </p>"},{"location":"examples/confluence/CONFLUENCE_OPENSEARCH/#deploy-with-the-default-settings","title":"Deploy with the default settings","text":"<p>To deploy OpenSearch Helm chart and automatically configure Confluence to use it as a search platform, set the following in your Helm values file:</p> <p><pre><code>opensearch:\n  enabled: true\n</code></pre> This will:</p> <ul> <li>auto-generate the initial OpenSearch admin password and create a Kubernetes secret with <code>OPENSEARCH_INITIAL_ADMIN_PASSWORD</code> key</li> <li>deploy OpenSearch Helm chart to the target namespace with the default settings: single node, 1Gi memory/1 vCPU resources requests, 10Gi storage request</li> <li>configure Confluence to use the deployed OpenSearch cluster by adding <code>-Dsearch.platform=opensearch -Dopensearch.http.url=http://opensearch-cluster-master:9200 -Dopensearch.username=admin -Dopensearch.password=yourPassword</code> to the JVM ConfigMap</li> </ul>"},{"location":"examples/confluence/CONFLUENCE_OPENSEARCH/#override-opensearch-helm-chart-values","title":"Override OpenSearch Helm chart values","text":"<p>You can configure your OpenSearch cluster and the deployment options by overriding any values that the Helm chart exposes. OpenSearch values must be nested under <code>opensearch</code> stanza in your Helm values file, for example:</p> <pre><code>opensearch:\n  singleNode: false\n  replicas: 5\n  config:\n    opensearch.yml: |\n      cluster.name: opensearch-cluster\n</code></pre>"},{"location":"examples/database/AMAZON_RDS/","title":"Creating an RDS database instance","text":"<p>This example provides instructions for creating an Amazon RDS DB instance.</p>"},{"location":"examples/database/AMAZON_RDS/#prerequisites","title":"Prerequisites","text":"<ul> <li>An <code>AWS account</code>, <code>IAM user</code>, <code>VPC</code> (or default VPC) and <code>security group</code> are required before an RDS DB instance can be created. See Setting up for Amazon RDS for further instructions.</li> </ul>"},{"location":"examples/database/AMAZON_RDS/#database-creation","title":"Database creation","text":"<p>There are two steps for creating the database:</p> <ol> <li>Initialize database server</li> <li>Initialize database and user</li> </ol>"},{"location":"examples/database/AMAZON_RDS/#1-initialize-database-server","title":"1. Initialize database server","text":"<p>For details on standing up an RDS DB server follow the guide: Creating an Amazon RDS DB instance.</p>"},{"location":"examples/database/AMAZON_RDS/#2-initialize-database-and-user","title":"2. Initialize database and user","text":"<p>Don't forget to create the database and user!</p> <p>This is a required step. For details on creating the application database and database user follow the appropriate guide below:</p> JiraConfluenceBitbucketBambooCrowd <p>Create database for Jira</p> <p>Create database for Confluence</p> <p>Create database for Bitbucket</p> <p>Create database for Bamboo</p> <p>Create database for Crowd</p> <p>Next step - Shared storage</p> <p>Having created the database continue with provisioning the next piece of prerequisite infrastructure, shared storage.</p>"},{"location":"examples/database/CLOUD_PROVIDERS/","title":"Provisioning databases on cloud-based providers","text":"<p>Supported databases</p> <p>Your selected database engine type must be supported by the Data Center product you wish to install:</p> JiraConfluenceBitbucketBambooCrowd <p>Jira supported databases</p> <p>Confluence supported databases</p> <p>Bitbucket supported databases</p> <p>Supported databases</p> <p>Crowd supported databases</p> <p>Database deployment and configuration instructions for cloud providers can be found below:</p> <ul> <li>Amazon RDS</li> </ul>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/","title":"External libraries and plugins","text":"<p><code>.jar</code> files only</p> <p>Whether loading external libraries, drivers or plugins, the approaches outlined here can only be used with <code>.jar</code> files. Plugin <code>obr</code> files can be extracted (unzipped) to access the included <code>.jar</code></p> <p>In some situations, you may want to load 3rd party plugins, drivers or libraries so that they are available to the product  being installed.</p> <p>An example of when this may be needed are for those products that do not ship with the appropriate <code>MySQL</code> and <code>Oracle</code> <code>JDBC</code> drivers.</p> <p>There are 3 strategies for doing this, you can either:</p> <ul> <li>use the required prerequisite shared home volume </li> <li>create a custom volume specifically for this purpose </li> <li>provide a custom command for the <code>nfsPermissionFixer</code></li> </ul> <p>Each approach will be discussed below.</p> <p>Approach</p> <p>Which approach is used is totally up to you. For convenience you may want to just use shared-home, or if you'd like to  keep things clean you may decide to mount these 3rd party libraries in a volume of their own. This approach would be  particularly useful when these libraries need to be shared with other Pod's in your cluster.</p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#shared-home-volume","title":"Shared home volume","text":"<p>This approach consists of 3 high-level tasks:</p> <ol> <li>Create sub-dir in <code>shared-home</code> volume</li> <li>Copy libraries to sub-dir</li> <li>Update <code>additionalLibraries</code> stanza in <code>values.yaml</code></li> </ol>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#1-create-sub-dir","title":"1. Create sub-dir","text":"<p>Add the Pod definition below to a file called <code>shared-home-browser.yaml</code> </p> <p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: shared-home-browser\nspec:\n  containers:\n    - name: browser\n      image: debian:stable-slim\n      volumeMounts:\n        - mountPath: /shared-home\n          name: shared-home\n      command: [ \"bash\", \"-c\", \"--\" ]\n      args: [ \"while true; do sleep 30; done;\" ]\n  volumes:\n    - name: shared-home\n      persistentVolumeClaim:\n        claimName: &lt;shared-home-pvc-name&gt;\n</code></pre> Initialise the Pod in the same namespace in which the <code>shared-home</code> PVC was created <pre><code>kubectl apply -f shared-home-browser.yaml\n</code></pre> Once running execute the following command, it will create the sub-sir, <code>libraries</code>, under <code>/shared-home</code> <pre><code>kubectl exec -it shared-home-browser -- bash -c \"mkdir -p /shared-home/libraries\"\n</code></pre></p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#2-copy-libraries-to-sub-dir","title":"2. Copy libraries to sub-dir","text":"<p>Now copy the files you require to the sub-dir by using the <code>kubectl cp</code> command <pre><code>kubectl cp my_library.jar shared-home-browser:/shared-home/libraries\n</code></pre></p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#3-update-valuesyaml","title":"3. Update <code>values.yaml</code>","text":"<p>Update the stanza, <code>additionalLibraries</code>, in <code>values.yaml</code> accordingly: <pre><code>jira:\n  additionalLibraries:\n    - volumeName: shared-home\n      subDirectory: libraries\n      fileName: my_library.jar\n</code></pre> With this config these files (<code>my_library.jar</code>) will be injected into the container directory <code>&lt;product-installation-directory&gt;/lib</code>. For more info on how these files are injected into the appropriate product container location, see Jira's helper jira.additionalLibraries.  </p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#custom-volume","title":"Custom volume","text":"<p>This approach is very similar to the Shared home volume approach, only a custom volume is created and used as opposed <code>shared-home</code>. </p> <ol> <li>Create a new volume for storing 3rd party libraries</li> <li>Create sub-dir for the new volume</li> <li>Copy libraries to sub-dir</li> <li>Update <code>additionalLibraries</code> stanza in <code>values.yaml</code></li> <li>Update <code>additionalVolumeMounts</code> stanza in <code>values.yaml</code></li> <li>Update <code>additional</code> stanza in <code>values.yaml</code></li> </ol> <p>Steps</p> <p>Because many of the steps for this approach are similar to the steps used for Shared home volume only those that differ will be discussed.</p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#1-create-new-volume","title":"1. Create new volume","text":"<p>Using the same approach taken for provisioning the shared-home volume, create a new <code>EFS</code> with a corresponding <code>PV</code> and <code>PVC</code>.</p> <p>ReadOnlyMany</p> <p>Ensure that the PV and PVC are setup with <code>ReadOnlyMany</code> access</p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#2-update-valuesyaml","title":"2. Update <code>values.yaml</code>","text":"<p>Assuming that the <code>PVC</code> representing the <code>EFS</code> is called <code>third-party-libraries</code>, update the <code>values.yaml</code> so that the <code>PVC</code> is added as an <code>additional</code> mount: <pre><code>volumes:\n  additional:\n    - name: third-party-libraries\n      persistentVolumeClaim:\n        claimName: third-party-libraries\n</code></pre> Now add this as an <code>additionalVolumeMounts</code> <pre><code>additionalVolumeMounts:\n  - volumeName: third-party-libraries\n    mountPath: /libraries\n</code></pre> Finally inject the desired libraries by defining them under <code>additionalLibraries</code> <pre><code>additionalLibraries:\n  - volumeName: third-party-libraries\n    subDirectory: database_drivers\n    fileName: my_library.jar\n</code></pre></p>"},{"location":"examples/external_libraries/EXTERNAL_LIBS/#custom-command","title":"Custom command","text":"<p>This example is based on the GitHub issue discussed here. The <code>nfsPermissionFixer</code> in the <code>values.yaml</code> is used for appropriately setting the permissions on the <code>shared-home</code> volume. The command it uses for this is already defined by default, however it can also be supplied with a custom <code>command</code> for adding 3rd party libraries to <code>shared-home</code>. The example below shows how this approach can be used for adding the <code>JDBC</code> <code>MySQL</code> driver:</p> <pre><code>nfsPermissionFixer:\n  command: |\n    if [[ ! -f /shared-home/drivers/mysql-driver.jar ]]; then\n      mkdir -p /shared-home/drivers\n      apk add dpkg\n      wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java_8.0.26-1debian10_all.deb\n      dpkg-deb -R mysql-connector-java_8.0.26-1debian10_all.deb /tmp/\n      cp /tmp/usr/share/java/mysql-connector-java-8.0.26.jar  /shared-home/drivers/mysql-driver.jar\n    fi\n    chgrp 2003 /shared-home; chmod g+w /shared-home\n</code></pre> <p>shared-home permissions</p> <p>If taking this approach ensure the last thing your custom command does is apply the relevant permissions to the <code>shared-home</code> mount, see line <code>10</code> in <code>yaml</code>  snippet above. </p> <p>Each product chart has a <code>&lt;product&gt;.sharedHome.permissionFix.command</code> helper for doing this. look at Jira's helper jira.sharedHome.permissionFix.command  for more details on how these permissions are applied by default.</p> <p>Remember to also update the <code>additionalLibraries</code> stanza accordingly: <pre><code>additionalLibraries: \n  - volumeName: shared-home\n    subDirectory: drivers\n    fileName: mysql-driver.jar\n</code></pre></p>"},{"location":"examples/ingress/CONTROLLERS/","title":"Provisioning an Ingress controller","text":"<p>In order for the provided Ingress resource to work, your Kubernetes cluster must have an ingress controller running. The Atlassian Helm charts have been tested with the NGINX Ingress Controller, however alternatives can also be used. </p> <p>Here is an example of how these controllers can be installed and configured for use with the Atlassian Helm charts:</p> <ul> <li>NGINX Ingress Controller</li> </ul>"},{"location":"examples/ingress/DNS/","title":"Create DNS record via AWS CLI","text":"<p>DNS record creation using Route53</p> <p>The approach below shows how a DNS record can be created using AWS Route53 and the AWS CLI for record sets</p> <p>First, identify the name of the auto provisioned AWS Classic Load Balancer that was created above for Step 2. Install controller: <pre><code>kubectl get service -n ingress | grep ingress-nginx | awk '{print $4}' | head -1\n</code></pre> the output of this command should be the name of the load balancer, take note of the name i.e. <pre><code>b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com\n</code></pre> Next, using the first part of the load balancer name, get the <code>HostedZoneId</code> for the load balancer <pre><code>aws elb describe-load-balancers --load-balancer-name b834z142d8118406795a34df35e10b17 --region &lt;aws_region&gt; | jq '.LoadBalancerDescriptions[] | .CanonicalHostedZoneNameID'\n</code></pre> With the <code>HostedZoneId</code> and the full name of the load balancer create the <code>JSON</code> \"change batch\" file below:</p> <pre><code>{\n  \"Comment\": \"An alias resource record for Jira in K8s\",\n  \"Changes\": [\n    {\n      \"Action\": \"CREATE\",\n      \"ResourceRecordSet\": {\n        \"Name\": &lt;DNS record name&gt;,\n        \"Type\": \"A\",\n        \"AliasTarget\": {\n          \"HostedZoneId\": &lt;Load balancer hosted zone ID&gt;,\n          \"DNSName\": &lt;Load balancer name&gt;,\n          \"EvaluateTargetHealth\": true\n        }\n      }\n    }\n  ]\n}\n</code></pre> <p>DNS record name</p> <p>If for example, the DNS record name were set to <code>product.k8s.hoolicorp.com</code> then the host, <code>hoolicorp.com</code>, would be the pre-registerd AWS Route53 hosted zone.</p> <p>Next get the zone ID for the hosted zone: <pre><code>aws route53 list-hosted-zones-by-name | jq '.HostedZones[] | select(.Name == \"hoolicorp.com.\") | .Id'\n</code></pre> Finally, using the hosted zone ID and the <code>JSON</code> change batch file created above, initialize the record: <pre><code>aws route53 change-resource-record-sets --hosted-zone-id &lt;hosted zone ID&gt; --change-batch file://change-batch.json\n</code></pre> This will return a response similar to the one below: <pre><code>{\n    \"ChangeInfo\": {\n        \"Id\": \"/change/C03268442VMV922ROD1M4\",\n        \"Status\": \"PENDING\",\n        \"SubmittedAt\": \"2021-08-30T01:42:23.478Z\",\n        \"Comment\": \"An alias resource record for Jira in K8s\"\n    }\n}\n</code></pre> You can get the current status of the record's initialization: <pre><code>aws route53  get-change --id /change/C03268442VMV922ROD1M4\n</code></pre> Once the <code>Status</code> has transitioned to <code>INSYNC</code> the record is ready for use... <pre><code>{\n    \"ChangeInfo\": {\n        \"Id\": \"/change/C03268442VMV922ROD1M4\",\n        \"Status\": \"INSYNC\",\n        \"SubmittedAt\": \"2021-08-30T01:42:23.478Z\",\n        \"Comment\": \"Creating Alias resource record sets in Route 53\"\n    }\n}\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/","title":"NGINX Ingress Controller - with TLS termination","text":"<p>NGINX ingress controller with automatic TLS certificate management using cert-manager and certificates from Let's Encrypt.</p> <p>Using these instructions</p> <p>These instructions are for reference purposes, as such they should be used for development and testing purposes only! See the official instructions for Deploying and configuring the controller.</p> <p>These instructions are composed of 3 high-level parts:</p> <ol> <li>Controller installation and configuration</li> <li>Certificate manager installation and configuration</li> <li>Ingress resource configuration</li> </ol>"},{"location":"examples/ingress/INGRESS_NGINX/#controller-installation-and-configuration","title":"Controller installation and configuration","text":"<p>We recommend installing the controller using its official Helm Charts. You can also use the instructions below.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#1-add-controller-repository","title":"1. Add controller repository","text":"<p>Add the <code>ingress-nginx</code> Helm repository: <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n</code></pre> Update the repository: <pre><code>helm repo update\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/#2-install-controller","title":"2. Install controller","text":"<p>Create a new namespace for the Ingress controller: <pre><code>kubectl create namespace ingress\n</code></pre> Install the controller using Helm: <pre><code>helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress\n</code></pre></p> <p>This will take couple of minutes.</p> <p>Confirm your ingress controller is installed:</p> <pre><code>kubectl get pods --namespace ingress\n</code></pre>"},{"location":"examples/ingress/INGRESS_NGINX/#3-dns-setup","title":"3. DNS setup","text":"<p>Manually provision a new DNS record via your cloud provider, for instance AWS and Route53, or dynamically using external-dns. There are also instructions on how this can be done using the AWS CLI.</p> <p>Once created, associate the DNS record with the auto provisioned load balancer that was created in Step 2. above. To do this first identify the name of the auto provisioned LB, this can be done by examining the deployed ingress services i.e.: <pre><code>kubectl get service -n ingress | grep ingress-nginx\n</code></pre> the output of this command should look something like... <pre><code>ingress-nginx-controller             LoadBalancer   10.100.22.16    b834z142d8118406795a34df35e10b17-38927090.eu-west-1.elb.amazonaws.com   80:32615/TCP,443:31787/TCP   76m\ningress-nginx-controller-admission   ClusterIP      10.100.5.36     &lt;none&gt;                                                                  443/TCP                      76m\n</code></pre> Take note of the <code>LoadBalancer</code> and using it as a value update the DNS record so that traffic is routed to it.</p> <p>It can take a few minutes for the DNS to resolve these changes.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#certificate-manager-installation-and-configuration","title":"Certificate manager installation and configuration","text":"<p>Kubernetes certificate management is handled using cert-manager.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#1-install-cert-manager","title":"1. Install cert-manager","text":"<p>Add the cert-manager repository <pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre></p> <p>Update repositories <pre><code>helm repo update\n</code></pre></p> <p>Install the cert-manager using Helm <pre><code>helm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.3.1 \\\n  --set installCRDs=true\n</code></pre></p> <p>Confirm the cert-manager is appropriately installed: <pre><code>kubectl get pods --namespace cert-manager\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/#2-create-certificate-issuer","title":"2. Create certificate issuer","text":"<p>Using the <code>yaml</code> specification below create and apply the certificate <code>Issuer</code> resource:</p> <p>Namespace co-location</p> <p>Ensure that the certificate issuer is installed in the same namespace that the Atlassian product will be deployed to.</p> <p><pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: letsencrypt-prod\n  namespace: &lt;product_deployment_namespace&gt;\nspec:\n  acme:\n    # The ACME server URL\n    server: https://acme-v02.api.letsencrypt.org/directory\n    # Email address used for ACME registration\n    email: &lt;user_email&gt;\n    # Name of a secret used to store the ACME account private key\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    # Enable the HTTP-01 challenge provider\n    solvers:\n      - http01:\n          ingress:\n            class: nginx\n</code></pre> Install the <code>Issuer</code> resource: <pre><code>kubectl apply -f issuer.yaml\n</code></pre></p>"},{"location":"examples/ingress/INGRESS_NGINX/#ingress-resource-configuration","title":"Ingress resource configuration","text":"<p>Now that the Ingress controller and certificate manager are setup the Ingress resource can be configured accordingly by updating the <code>values.yaml</code>.</p>"},{"location":"examples/ingress/INGRESS_NGINX/#1-ingress-resource-config","title":"1. Ingress resource config","text":"<p>For TLS cert auto-provisioning and TLS termination update the <code>ingress</code> stanza within the products <code>values.yaml</code>: <pre><code>ingress:\n  create: true\n  nginx: true\n  maxBodySize: 250m\n  host: &lt;dns_record&gt;\n  path: \"/\"\n  annotations:\n    cert-manager.io/issuer: \"letsencrypt-prod\" # Using https://letsencrypt.org/\n  https: true\n  tlsSecretName: tls-certificate\n</code></pre></p> <p>Configuring the <code>host</code> value</p> <p>In this case the <code>&lt;dns_record&gt;</code> would correspond to the record name that was created in 3. DNS setup above</p>"},{"location":"examples/ingress/INGRESS_NGINX/#bitbucket-ssh-configuration","title":"Bitbucket SSH configuration","text":"<p>Additional configuration</p> <p>Bitbucket requires additional Ingress config to allow for <code>SSH</code> access. See NGINX Ingress controller config for SSH connections for details.</p> <p>Next step - Database</p> <p>Having created the Ingress controller continue with provisioning the next piece of prerequisite infrastructure, the database.</p>"},{"location":"examples/logging/efk/EFK/","title":"Logging in a Kubernetes environment","text":"<p>Warning</p> <p>This functionality is not officially supported. This document explains how to enable aggregated logging in your Kubernetes cluster. There are many ways to do this and this document showcases only a few of the options.</p>"},{"location":"examples/logging/efk/EFK/#efk-stack","title":"EFK stack","text":"<p>A common Kubernetes logging pattern is the combination of <code>Elasticsearch</code>, <code>Fluentd</code>, and <code>Kibana</code>, known as EFK Stack. </p> <p>Fluentd is an open-source and multi-platform log processor that collects data/logs from different sources, aggregates, and forwards them to multiple destinations. It is fully compatible with Docker and Kubernetes environments. </p> <p>Elasticsearch is a distributed open search and analytics engine for all types of data. </p> <p>Kibana is an open-source front-end application that sits on top of Elasticsearch, providing search and data visualization capabilities for data indexed in Elasticsearch.</p> <p>There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using a managed Elasticsearch instance outside the Kubernetes cluster. </p>"},{"location":"examples/logging/efk/EFK/#local-efk-stack","title":"Local EFK stack","text":"<p>This solution deploys the EFK stack inside the Kubernetes cluster. By setting <code>fluentd.enabled</code> value to <code>true</code>, Helm installs Fluentd on each of application pods. This means that after deployment all the product pods run Fluentd, which collects all the log files and sends them to the Fluentd aggregator container. </p> <p>To complete the EFK stack you need to install an Elasticsearch cluster and Kibana, and successfully forward the aggregated datalog to Elasticsearch using Fluentd, which is already installed. </p> <p>Follow these steps to install Elasticsearch</p>"},{"location":"examples/logging/efk/EFK/#1-install-elasticsearch","title":"1. Install Elasticsearch","text":"<p>Install Elasticsearch using the instructions documented here. Once installed make sure Elasticsearch cluster is working as expected by first port forwarding the service</p> <pre><code>kubectl port-forward svc/elasticsearch-master 9200\n</code></pre> <p>you can then <code>curl</code> the endpoint for the current state</p> <pre><code>$ curl localhost:9200\n{\n  \"name\" : \"elasticsearch-master-0\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"uNdYC-2nSdWVdzPCw9P7jQ\",\n  \"version\" : {\n       \"number\" : \"7.12.0\",\n       \"build_flavor\" : \"default\",\n       \"build_type\" : \"docker\",\n       \"build_hash\" : \"78722783c38caa25a70982b5b042074cde5d3b3a\",\n       \"build_date\" : \"2021-03-18T06:17:15.410153305Z\",\n       \"build_snapshot\" : false,\n       \"lucene_version\" : \"8.8.0\",\n       \"minimum_wire_compatibility_version\" : \"6.8.0\",\n       \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n</code></pre>"},{"location":"examples/logging/efk/EFK/#2-enable-fluentd","title":"2. Enable Fluentd","text":"<p>Now enable <code>Fluentd</code> and set the <code>hostname</code> for Elasticsearch in <code>values.yaml</code> as follows:</p> <p><pre><code>fluentd:\n   enabled: true\n   elasticsearch:\n     hostname: elasticsearch-master\n</code></pre> Fluentd tries to parse and send the data to Elasticsearch, but since it's not installed the data is lost. At this point you have logged data in the installed Elasticsearch, and you should install Kibana to complete the EFK stack deployment:</p>"},{"location":"examples/logging/efk/EFK/#3-install-kibana","title":"3. Install Kibana","text":"<p>With the same version that was used for installing Elasticsearch, use the <code>imageTag</code> property to install Kibana:</p> <pre><code>helm install kibana --namespace &lt;namespace&gt; --set imageTag=\"7.9.3\" elastic/kibana\n</code></pre> <p>Make sure kibana is running by checking the deployment</p> <p><pre><code>kubectl get deployment\n</code></pre> You should see something like... <pre><code>NAME                               READY   UP-TO-DATE   AVAILABLE   AGE\nhelm-operator                      1/1           1            1     23m\ningress-nginx-release-controller   1/1           1            1     22m\nkibana-kibana                      1/1           1            1     25m\n</code></pre> Through port-forwarding you can access Kibana via <code>http://localhost:5601</code> <pre><code>kubectl port-forward deployment/kibana-kibana 5601\n</code></pre>  To visualise the logs you need to create an index pattern and then look at the the data in the discovery part. To create the index pattern go to <code>Management</code> \u2192 <code>Stack Management</code> and then select <code>Kibana</code> \u2192 <code>Index Patterns</code>. </p>"},{"location":"examples/logging/efk/EFK/#managed-efk-stack","title":"Managed EFK stack","text":"<p>In this solution Elasticsearch is deployed as a managed AWS service and lives outside of the Kubernetes cluster. This approach uses Fluentbit instead of <code>Fluentd</code> for log processing.</p> Fluentbit <p><code>Fluentbit</code> is used to collect and aggregate log data inside the EKS cluster. It then sends this to an AWS Elasticsearch instance outside of the cluster.</p> <p>When a node inside an EKS cluster needs to call an AWS API, it needs to provide extended permissions. Amazon provides an image of <code>Fluentbit</code> that supports AWS service accounts,and using this you no longer need to follow the traditional way. All you need is to have an IAM role for the AWS service account on an EKS cluster. Using this service account, an AWS permission can be provided to the containers in any pod that use that service account. The result is that the pods on that node can call AWS APIs.</p> <p>Your first step is to configure IAM roles for Service Accounts (IRSA) for <code>Fluentbit</code>, to make sure you have an OIDC identity provider to use IAM roles for the service account in the cluster:</p> <p><pre><code>eksctl utils associate-iam-oidc-provider \\\n     --cluster &lt;cluster_name&gt; \\\n     --approve \n</code></pre> Then create an IAM policy to limit the permissions to connect to the Elasticsearch cluster. Before this, you need to set the following environment variables: </p> Environment variable Value KUBE_NAMESPACE The namespace for kubernetes cluster ES_DOMAIN_NAME Elasticsearch domain name ES_VERSION Elasticsearch version ES_USER Elasticsearch username ES_PASSWORD Elasticsearch password (eg. <code>export ES_PASSWORD=\"$(openssl rand -base64 8)_Ek1$\"</code>) ACCOUNT_ID AWS Account ID AWS_REGION AWS region code <p>Now create the file <code>fluent-bit-policy.json</code> to define the policy itself:</p> <p><pre><code>cat &lt;&lt;EoF &gt; ~/environment/logging/fluent-bit-policy.json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n         {\n             \"Action\": [\n                 \"es:ESHttp*\"\n             ],\n             \"Resource\": \"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}\",\n             \"Effect\": \"Allow\"\n         }\n    ]\n}\nEoF\n</code></pre> Next initialize the policy: <pre><code>aws iam create-policy  \\\n     --policy-name fluent-bit-policy \\\n     --policy-document file://~/environment/logging/fluent-bit-policy.json\n</code></pre> Create an IAM role for the service account: <pre><code>eksctl create iamserviceaccount \\\n     --name fluent-bit \\\n     --namespace dcd \\\n     --cluster dcd-ap-southeast-2 \\\n     --attach-policy-arn \"arn:aws:iam::${ACCOUNT_ID}:policy/fluent-bit-policy\" \\\n     --approve \\\n     --override-existing-serviceaccounts\n</code></pre> Confirm that the service account with an Amazon Resource Name (ARN) of the IAM role is annotated: <pre><code>kubectl describe serviceaccount fluent-bit\n</code></pre> Look for output similar to: <pre><code>Name: fluent-bit\nNamespace:  elastic\nLabels: &lt;none&gt;\nAnnotations: eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/eksctl-your-cluster-name-addon-iamserviceac-Role1-0A0A0A0A0A0A0\nImage pull secrets: &lt;none&gt;\nMountable secrets:  fluent-bit-token-pgpss\nTokens:  fluent-bit-token-pgpss\nEvents:  &lt;none&gt;\n</code></pre> Now define the Elasticsearch domain</p> <p>This configuration will provision a public Elasticsearch cluster with Fine-Grained Access Control enabled and a built-in user database:</p> <p><pre><code>cat &lt;&lt;EOF&gt; ~/environment/logging/elasticsearch_domain.json\n{\n    \"DomainName\": ${ES_DOMAIN_NAME},\n    \"ElasticsearchVersion\": ${ES_VERSION},\n    \"ElasticsearchClusterConfig\": {\n         \"InstanceType\": \"r5.large.elasticsearch\",\n         \"InstanceCount\": 1,\n             \"DedicatedMasterEnabled\": false,\n             \"ZoneAwarenessEnabled\": false,\n             \"WarmEnabled\": false\n         },\n    \"EBSOptions\": {\n         \"EBSEnabled\": true,\n         \"VolumeType\": \"gp2\",\n         \"VolumeSize\": 100\n    },\n    \"AccessPolicies\": \"{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{\\\"AWS\\\":\\\"*\\\"},\\\"Action\\\":\\\"es:ESHttp*\\\",\\\"Resource\\\":\\\"arn:aws:es:${AWS_REGION}:${ACCOUNT_ID}:domain/${ES_DOMAIN_NAME}/*\\\"}]}\",\n    \"SnapshotOptions\": {},\n    \"CognitoOptions\": {\n         \"Enabled\": false\n    },\n    \"EncryptionAtRestOptions\": {\n         \"Enabled\": true\n    },\n    \"NodeToNodeEncryptionOptions\": {\n         \"Enabled\": true\n    },\n    \"DomainEndpointOptions\": {\n         \"EnforceHTTPS\": true,\n         \"TLSSecurityPolicy\": \"Policy-Min-TLS-1-0-2019-07\"\n    },\n    \"AdvancedSecurityOptions\": {\n         \"Enabled\": true,\n         \"InternalUserDatabaseEnabled\": true,\n         \"MasterUserOptions\": {\n             \"MasterUserName\": ${ES_USER},\n             \"MasterUserPassword\": ${ES_PASSWORD}\n         }\n    }\n}\nEOF\n</code></pre> Initialize the Elasticsearch domain using the <code>elasticsearch_domain.json</code></p> <pre><code>aws es create-elasticsearch-domain \\\n   --cli-input-json   file://~/environment/logging/elasticsearch_domain.json\n</code></pre> <p>It takes a while for Elasticsearch clusters to change to an active state. Check the AWS Console to see the status of the cluster, and continue to the next step when the cluster is ready.</p> <p>At this point you need to map roles to users in order to set fine-grained access control, because without this mapping all the requests to the cluster will result in permission errors. You should add the <code>Fluentbit</code> ARN as a backend role to the <code>all-access</code> role, which uses the Elasticsearch APIs. To find the <code>fluentbit</code> ARN run the following command and export the value of <code>ARN Role</code> into the <code>FLUENTBIT_ROLE</code> environment variable: <pre><code>eksctl get iamserviceaccount --cluster dcd-ap-southeast-2\n</code></pre> The output of this command should look similar to this: <pre><code>NAMESPACE    NAME                ROLE ARN\nkube-system cluster-autoscaler   arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E\n</code></pre> Take note of the <code>ROLE ARN</code> and export it as the environment variable <code>FLUENTBIT_ROLE</code> <pre><code>export FLUENTBIT_ROLE=arn:aws:iam::887464544476:role/eksctl-dcd-ap-southeast-2-addon-iamserviceac-Role1-1RSRFV0BQVE3E\n</code></pre> Retrieve the Elasticsearch endpoint and update the internal database: <pre><code>export ES_ENDPOINT=$(aws es describe-elasticsearch-domain --domain-name ngh-search-domain --output text --query \"DomainStatus.Endpoint\")\n</code></pre></p> <p><pre><code>curl -sS -u \"${ES_DOMAIN_USER}:${ES_DOMAIN_PASSWORD}\" \\\n   -X PATCH \\\n   https://${ES_ENDPOINT}/_opendistro/_security/api/rolesmapping/all_access?pretty \\\n   -H 'Content-Type: application/json' \\\n   -d'\n[\n   {\n     \"op\": \"add\", \"path\": \"/backend_roles\", \"value\": [\"'${FLUENTBIT_ROLE}'\"]\n   }\n]\n'\n</code></pre> Finally, it is time to deploy the <code>Fluentbit</code> DaemonSet: <pre><code>kubectl apply -f docs/docs/examples/logging/efk/managed_es/fluentbit.yaml\n</code></pre> After a few minutes all pods should be up and in running status. you can open Kibana to visualise the logs. The endpoint for Kibana can be found in the Elasticsearch output tab in the AWS console, or you can run the following command: <pre><code>echo \"Kibana URL: https://${ES_ENDPOINT}/_plugin/kibana/\" \nKibana URL: https://search-domain-uehlb3kxledxykchwexee.ap-southeast-2.es.amazonaws.com/_plugin/kibana/\n</code></pre></p> <p>The user and password for Kibana are the same as the master user credential that is set in Elasticsearch in the provisioning stage. Open Kibana in a browser and after login, create an index pattern and see the report in the <code>Discover</code> page. </p>"},{"location":"examples/storage/STORAGE/","title":"Shared storage","text":"<p>Atlassian's Data Center products require a shared storage solution to effectively operate in multi-node environment. The specifics of how this shared storage is created is site-dependent, we do however provide examples on how shared storage can be created below.</p> <p>In case you are not using Bitbucket Mesh nodes, Bitbucket requires a dedicated NFS server providing persistence for a shared home due to requirements for high IO throughput. See NFS example for details</p>"},{"location":"examples/storage/STORAGE/#aws-efs","title":"AWS EFS","text":"<p>Jira, Confluence and Crowd can all be configured with an EFS-backed shared solution. For details on how this can be set up, see the AWS EFS example. </p>"},{"location":"examples/storage/STORAGE/#nfs","title":"NFS","text":"<p>For details on creating shared storage for Bitbucket, see the NFS example.</p>"},{"location":"examples/storage/additional%20storage/ADDTIONAL_STORAGE/","title":"Additional Storage","text":"<p>You can use volumeClaimTemplates to have additional storage. This is useful when your enviroment uses several types of storage. </p> <p>E.g. If you want to deploy Confluence on NFS, but you want to use BlockStorage (or everything else instead of NFS) for the lucene-index, you can create extra volumn for BlockStorage by defining <code>volumeClaimTemplates</code> in <code>values.yaml</code> then mount the volume in <code>additionalVolumeMounts</code>. <pre><code>confluence:\n  additionalVolumeClaimTemplates:\n    - name: myadditionalvolumeclaim\n      storageClassName: gp2\n      resources:\n        requests:\n          storage: 1Gi\n  additionalVolumeMounts:\n    - mountPath: /var/atlassian/application-data/confluence/index\n      name: myadditionalvolumeclaim\n</code></pre></p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/","title":"Local storage","text":"<p>This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes.</p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#dynamic-provisioning","title":"Dynamic provisioning","text":"<p>Due to the ephemeral nature of Kubernetes pods we advise dynamic provisioning be used for creating and consuming EBS volume(s).</p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#prerequisites","title":"Prerequisites","text":"<p>Ensure the EBS CSI driver is installed within the k8s cluster, you can confirm this by running:  </p> <p><pre><code>kubectl get csidriver\n</code></pre> the output of the above command should include the named driver <code>ebs.csi.aws.com</code> for example: <pre><code>NAME              ATTACHREQUIRED   PODINFOONMOUNT   MODES        AGE\nebs.csi.aws.com   true             false            Persistent   5d1h\n</code></pre> If not present the EBS driver can be installed using the following instructions here.</p>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#provisioning","title":"Provisioning","text":"<ol> <li>Create a Storage Class</li> <li>Update <code>values.yaml</code> to utilise Storage Class</li> </ol>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#1-create-storage-class","title":"1. Create Storage Class","text":"<pre><code>kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: ebs-sc\nprovisioner: ebs.csi.aws.com\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#2-update-valuesyaml","title":"2. Update values.yaml","text":"<p>Update the <code>localHome</code> <code>storageClassName</code> value within <code>values.yaml</code> to the name of the Storage Class created in step 1 above</p> <pre><code>volumes:\n  localHome:\n    persistentVolumeClaim:\n      create: true\n      storageClassName: \"ebs-sc\"\n</code></pre>"},{"location":"examples/storage/aws/LOCAL_STORAGE/#resources","title":"Resources","text":"<p>Some useful resources on provisioning local storage with the AWS CSI Driver</p> <ul> <li>EBS CSI driver - GitHub Repo</li> <li>Official Amazon EBS CSI driver documentation</li> </ul> <p>Product installation</p> <p>Creating the local home volume is the final step in provisioning the required infrastructure. You can now move onto the next step, Installation.</p>"},{"location":"examples/storage/aws/SHARED_STORAGE/","title":"Shared storage","text":"<p>This file provides examples on how a Kubernetes cluster and helm deployment can be configured to utilize an AWS EFS backed filesystem.</p>"},{"location":"examples/storage/aws/SHARED_STORAGE/#static-provisioning","title":"Static provisioning","text":"<p>An example detailing how an existing EFS filesystem can be created and consumed using static provisioning.</p>"},{"location":"examples/storage/aws/SHARED_STORAGE/#prerequisites","title":"Prerequisites","text":"<ol> <li>EFS CSI driver is installed within the k8s cluster.</li> <li>A physical EFS filesystem has been provisioned</li> </ol> <p>Additional details on static EFS provisioning can be found here</p> <p>You can confirm that the EFS CSI driver has been installed by running:</p> <p><pre><code>kubectl get csidriver\n</code></pre> the output of the above command should include the named driver <code>efs.csi.aws.com</code> for example: <pre><code>NAME              ATTACHREQUIRED   PODINFOONMOUNT   MODES        AGE\nefs.csi.aws.com   false            false            Persistent   5d3h\n</code></pre></p>"},{"location":"examples/storage/aws/SHARED_STORAGE/#provisioning","title":"Provisioning","text":"<ol> <li>Create a Persistent Volume</li> <li>Create a Persistent Volume Claim</li> <li>Update <code>values.yaml</code> to utilise Persistent Volume Claim</li> </ol>"},{"location":"examples/storage/aws/SHARED_STORAGE/#1-create-persistent-volume","title":"1. Create Persistent Volume","text":"<p>Create a persistent volume for the pre-provisioned EFS filesystem by providing the <code>&lt;efs-id&gt;</code>. The EFS id can be identified using the CLI command below with the appropriate region</p> <pre><code>aws efs describe-file-systems --query \"FileSystems[*].FileSystemId\" --region ap-southeast-2\n</code></pre> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-shared-vol-pv\nspec:\n  capacity:\n    storage: 1Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  storageClassName: efs-pv\n  persistentVolumeReclaimPolicy: Retain\n  mountOptions:\n    - rw\n    - lookupcache=pos\n    - noatime\n    - intr\n    - _netdev\n  csi:\n    driver: efs.csi.aws.com\n    volumeHandle: &lt;efs-id&gt;\n</code></pre>"},{"location":"examples/storage/aws/SHARED_STORAGE/#2-create-persistent-volume-claim","title":"2. Create Persistent Volume Claim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-shared-vol-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: efs-pv\n  volumeMode: Filesystem\n  volumeName: my-shared-vol-pv\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>"},{"location":"examples/storage/aws/SHARED_STORAGE/#3-update-valuesyaml","title":"3. Update values.yaml","text":"<p>Update the <code>sharedHome</code> <code>claimName</code> value within <code>values.yaml</code> to the name of the Persistent Volume Claim created in step 2 above</p> <pre><code>volumes:\n  sharedHome:\n    customVolume:\n      persistentVolumeClaim:\n        claimName: \"my-shared-vol-pvc\"\n</code></pre>"},{"location":"examples/storage/aws/SHARED_STORAGE/#resources","title":"Resources","text":"<p>Some useful resources on provisioning shared storage with the AWS CSI Driver:</p> <ul> <li>Amazon EFS CSI driver</li> <li>Introducing Amazon EFS CSI dynamic provisioning</li> </ul> <p>Next step - Local storage</p> <p>Having created the shared home volume continue with provisioning the next piece of prerequisite infrastructure, local storage.</p>"},{"location":"examples/storage/aws/s3/CONFLUENCE/","title":"AWS S3 Attachments Storage","text":"<p>Since 8.1.0 Confluence supports storing attachments in AWS S3. To enable this feature, update the image <code>tag</code> to <code>8.1.0</code> and define bucket name and AWS region in <code>confluence.s3AttachmentsStorage</code>, for example:</p> <pre><code>tag: 8.1.0\nconfluence:\n  s3AttachmentsStorage:\n    bucketName: confluence-attachments-bucket\n    bucketRegion: us-east-1\n</code></pre>"},{"location":"examples/storage/aws/s3/CONFLUENCE/#aws-authentication","title":"AWS Authentication","text":"<p>You will find details on available authentication methods in Credential Provider.</p> <p>Make sure <code>ATL_UNSET_SENSITIVE_ENV_VARS</code> is set to false if you choose to define <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> in <code>confluence.additionalEnvironmentVariables</code>:</p> <pre><code>additionalEnvironmentVariables:\n  - name: AWS_ACCESS_KEY_ID\n    valueFrom:\n      secretKeyRef:\n        name: aws-creds\n        key: AWS_ACCESS_KEY_ID\n  - name: AWS_SECRET_ACCESS_KEY\n    valueFrom:\n      secretKeyRef:\n        name: aws-creds\n        key: AWS_SECRET_ACCESS_KEY\n  - name: ATL_UNSET_SENSITIVE_ENV_VARS\n    value: \"false\"\n</code></pre>"},{"location":"examples/storage/aws/s3/CONFLUENCE/#eks-irsa","title":"EKS IRSA","text":"<p>If Confluence is deployed to AWS EKS, it is strongly recommended to use IAM roles for service accounts (IRSA).</p> <p>The Confluence service account will be automatically annotated with a role <code>ARN</code> if it is defined, for example:</p> <pre><code>serviceAccount:\n  eksIrsa:\n    roleArn: arn:aws:iam::37583956:role/confluence-s3-role\n</code></pre> <p>Below is an example policy, providing appropriate S3 access to Confluence, that needs to be attached to the role:</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::confluence-attachments-bucket/*\",\n                \"arn:aws:s3:::confluence-attachments-bucket\"\n            ],\n            \"Sid\": \"\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre>"},{"location":"examples/storage/aws/s3/JIRA/","title":"AWS S3 Avatars Storage","text":"<p>Since 9.9.0 Jira supports storing avatars in AWS S3. To enable this feature, update the image <code>tag</code> to <code>9.9.0</code> and define bucket name and AWS region in <code>jira.s3Storage.avatars</code>, for example:</p> <pre><code>tag: 9.9.0\njira:\n  s3Storage:\n    avatars:\n      bucketName: jira-avatars-bucket\n      bucketRegion: us-east-1\n</code></pre>"},{"location":"examples/storage/aws/s3/JIRA/#aws-authentication","title":"AWS Authentication","text":"<p>You will find details on available authentication methods in Credential Provider.</p> <p>Make sure <code>ATL_UNSET_SENSITIVE_ENV_VARS</code> is set to false if you choose to define <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> in <code>jira.additionalEnvironmentVariables</code>:</p> <pre><code>additionalEnvironmentVariables:\n  - name: AWS_ACCESS_KEY_ID\n    valueFrom:\n      secretKeyRef:\n        name: aws-creds\n        key: AWS_ACCESS_KEY_ID\n  - name: AWS_SECRET_ACCESS_KEY\n    valueFrom:\n      secretKeyRef:\n        name: aws-creds\n        key: AWS_SECRET_ACCESS_KEY\n  - name: ATL_UNSET_SENSITIVE_ENV_VARS\n    value: \"false\"\n</code></pre>"},{"location":"examples/storage/aws/s3/JIRA/#eks-irsa","title":"EKS IRSA","text":"<p>If Jira is deployed to AWS EKS, it is strongly recommended to use IAM roles for service accounts (IRSA).</p> <p>The Jira service account will be automatically annotated with a role <code>ARN</code> if it is defined, for example:</p> <pre><code>serviceAccount:\n  eksIrsa:\n    roleArn: arn:aws:iam::37583956:role/jira-s3-role\n</code></pre> <p>Below is an example policy, providing appropriate S3 access to Jira, that needs to be attached to the role:</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:ListBucket\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::jira-avatars-bucket/*\",\n                \"arn:aws:s3:::jira-avatars-bucket\"\n            ],\n            \"Sid\": \"\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre>"},{"location":"examples/storage/nfs/NFS/","title":"NFS server for Bitbucket","text":"<p>Disclaimer</p> <p>The NFS helm chart is not officially supported. It should not be used for production deployments!</p> <p>The included NFS example is provided as is and should be used as reference a only. Before you proceed we highly recommend that you understand your specific deployment needs and tailor your solution to them.</p>"},{"location":"examples/storage/nfs/NFS/#bitbucket-data-center-and-nfs","title":"Bitbucket Data Center and NFS","text":"<p>When deploying Bitbucket, you have two options how to store git data. You can deploy Bitbucket Mesh nodes or you can use shared home for your Bitbucket Data Center instance.</p> <p>Due to the high performance requirements on IO operations, it is critical that you adhere to the requirements in Bitbucket Supported platforms.</p>"},{"location":"examples/storage/nfs/NFS/#nfs-provisioning","title":"NFS provisioning","text":"<p>The NFS server can be provisioned manually or by using the supplied Helm chart. Details for both approaches can be found below.</p> <p>Pod affinity</p> <p>To reduce the IO latency between the NFS server and Bitbucket Pod(s) it is  highly recommend to keep them in close proximity. To achieve this, you can use standard Kubernetes affinity rules. The <code>affinity</code> stanza within <code>values.yaml</code> can be updated to take advantage of this behaviour i.e.</p>"},{"location":"examples/storage/nfs/NFS/#manual","title":"Manual","text":"<p>For information on setting up Bitbucket Data Center's shared file server, see Provision your shared file system.  This section contains the requirements and recommendations for setting up NFS for Bitbucket Data Center.</p> <p>NFS Server sizing</p> <p>Ensure the NFS server's size is appropriate for the needs of the Bitbucket instance. See capacity recommendations for details.</p>"},{"location":"examples/storage/nfs/NFS/#helm","title":"Helm","text":"<p>Disclaimer</p> <p>This Helm chart is not officially supported! It should not be used for production deployments!</p>"},{"location":"examples/storage/nfs/NFS/#installation","title":"Installation","text":"<p>Create a namespace for the NFS <pre><code>kubectl create namespace nfs\n</code></pre> Clone this repository and from the sub-directory, <code>data-center-helm-charts/docs/docs/examples/storage/nfs</code>, run the following command: <pre><code>helm install nfs-server nfs-server-example --namespace nfs\n</code></pre></p>"},{"location":"examples/storage/nfs/NFS/#uninstall","title":"Uninstall","text":"<pre><code>helm uninstall nfs-server --namespace nfs\n</code></pre>"},{"location":"examples/storage/nfs/NFS/#update-valuesyaml","title":"Update <code>values.yaml</code>","text":"<p>Get the IP address of the NFS service (<code>CLUSTER-IP</code>) by running the following command <pre><code>kubectl get service --namespace nfs -o jsonpath='{.items[0].spec.clusterIP}'\n</code></pre></p> <p>NFS directory share</p> <p>The NFS Helm chart creates and exposes the directory share <code>/srv/nfs</code>. This will be required when configuring <code>values.yaml</code> </p> <p>The approach below shows how a <code>persistentVolume</code> and corresponding <code>peristentVolumeClaim</code> can be dynamically created for the provisioned NFS. Using the NFS IP and directory share, (see above) update the <code>values.yaml</code> appropriately: <pre><code>volumes:\n  sharedHome:\n    persistentVolume:\n      create: true\n      nfs:\n        server: \"10.100.197.23\" # IP address of the NFS server \n        path: \"/srv/nfs\" # Directory share of NFS\n    persistentVolumeClaim:\n      create: true\n      storageClassName: \"\"\n</code></pre> You can of course manually provision your own <code>persistentVolume</code> and corresponding claim (as opposed to the dynamic approach described above) for the NFS server. In this case update the <code>values.yaml</code> to make use of them via the <code>customVolume</code> stanza, <code>sharedHome.persistentVolume.create</code> and <code>sharedHome.persistentVolumeClaim.create</code> should also both be set to <code>false</code>. <pre><code>sharedHome:\n  persistentVolume:\n    create: false\n  persistentVolumeClaim:\n    create: false\n  customVolume: \n    persistentVolumeClaim:\n      claimName: \"custom-nfs-server-claim\"\n</code></pre></p> <p>Next step - Local storage</p> <p>Having created the shared home NFS continue with provisioning the next piece of prerequisite infrastructure, local storage.</p>"},{"location":"platforms/PLATFORMS/","title":"Platform information","text":"<p>Support Disclaimer</p> <p>Helm is a Kubernetes package manager that orchestrates the provisioning of applications onto existing Kubernetes infrastructure. The requirements for this infrastructure are described in Prerequisites. The Kubernetes cluster remains your responsibility; we do not provide direct support for Kubernetes or the underlying hardware it runs on.</p> <p>If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation after running the <code>helm install</code> command. </p> <p>Read more about what we support and what we don\u2019t. </p> <p>Using our Helm charts on different platforms:</p> <ul> <li>OpenShift</li> </ul>"},{"location":"platforms/openshift/OPENSHIFT/","title":"Deploy To OpenShift","text":"<p>Support disclaimer</p> <p>Helm is a Kubernetes package manager that orchestrates the provisioning of applications onto existing Kubernetes infrastructure. The requirements for this infrastructure are described in Prerequisites. The Kubernetes cluster remains your responsibility; we do not provide direct support for Kubernetes or the underlying hardware it runs on.</p> <p>If you have followed our documentation on how to configure Helm charts, and you're correctly-created components, we will provide support if you encounter an error with installation after running the <code>helm install</code> command.</p> <p>Read more about what we support and what we don\u2019t.</p> <p>Official OpenShift support</p> <p>Starting from version 1.18, Atlassian Data Center Helm charts officially include support for OpenShift.</p>"},{"location":"platforms/openshift/OPENSHIFT/#deploy-with-a-restricted-and-nonroot-security-context-constraint","title":"Deploy with a restricted and nonroot security context constraint","text":"<p>Atlassian Data Center Helm charts are vendor and cloud agnostic, and create objects from standard APIs that OpenShift fully supports.</p> <p>However, the default OpenShift restricted Security Context Constrain (SCC) requires pods to be run with a UID and a SELinux context that are allocated to the namespace. It's therefore not possible to deploy DC Helm charts with the default settings (with pre-defined <code>securityContext</code> and containers that run as root). See FAQs for typical errors.</p> <p>ATL_TOMCAT_* environment variables</p> <p>When running as restricted or nonroot SCC, some of <code>ATL_TOMCAT_*</code> environment variables passed to the Helm chart in <code>additionalEnvironmentVariables</code> will be ignored, because <code>server.xml</code> and  <code>seraph-config.xml</code> are mounted as a ConfigMaps rather than generated in the container entrypoint from Jinja templates. Both <code>&lt;product&gt;.tomcatConfig</code> and <code>&lt;product&gt;.seraphConfig</code> have a number of properties which you can override if necessary. Look at <code>tomcatConfig</code> stanza in the chart's values.yaml for more details.</p> Restricted SCC ( Recommended)Nonroot SCC <p>Restricted SCC</p> <p>Restricted SCC denies access to all host features and requires pods to be run with a UID and a SELinux context that are allocated to the namespace.</p> <p>Running containers in OpenShift with a restricted SCC is the most secure approach. To be able to successfully deploy Atlassian DC Helm charts with a restricted SCC, set <code>openshift.runWithRestrictedSCC</code> to true in the Helm values file:</p> <pre><code>openshift:\n  runWithRestrictedSCC: true\n</code></pre> <p>This property will:</p> <ul> <li>unset <code>securityContext</code> on the pod and <code>initContainers</code> level, letting OpenShift set it</li> <li>trigger the creation of a ConfigMap with <code>server.xml</code> and <code>seraph-config.xml</code> that are mounted as read-only files into containers (rather than generated in the container entrypoint). Both <code>server.xml</code> and <code>seraph-config.xml</code> can be configured in <code>&lt;product&gt;.tomcatConfig</code> Helm stanza (except for Bitbucket which does not have these configuration files)</li> <li>disable the nfs-permission fixer init container that is run as root by default. If you need to change permissions in an existing shared-home volume, do it when provisioning/migrating the volume. See FAQs for more details.</li> </ul> <p>Nonroot SCC</p> <p>Nonroot SCC provides all features of the restricted SCC but allows users to run with any non-root UID.</p> <p>Supported versions</p> <p>OpenShift nonroot SCC friendly values are available in versions <code>1.14.0+</code> for Jira and Confluence, and <code>1.18+</code> for Crowd and Bamboo. If you can't upgrade, consider using 'additionalFiles' to mount <code>server.xml</code> and <code>seraph-config.xml</code> as ConfigMaps, which should be created outside the Helm chart.</p> <p>To be able to run DC containers as users created during an image build you need to let the ServiceAccount use the nonroot SCC. Typically, cluster admin privileges are required (replace namespace and Helm release name):</p> <pre><code>oc adm policy add-scc-to-user nonroot \\\n       system:serviceaccount:&lt;namespace&gt;:&lt;helm-release-name&gt;\n</code></pre> <p>Use the following Helm values (replace <code>jira</code> and <code>UIDs</code> depending on the deployed product):</p> <pre><code>jira:\n  securityContext:\n    runAsUser: 2001\n    runAsGroup: 2001\n    fsGroup: 2001\n  tomcatConfig:\n    generateByHelm: true\n  seraphConfig:\n    generateByHelm: true\nvolumes:\n  sharedHome:\n    nfsPermissionFixer:\n      enabled: false  \n</code></pre> <p>If you enabled <code>monitoring.exposeJmxMetrics</code>, you need to run the init container as non root:</p> <pre><code>monitoring:\n  jmxExporterInitContainer:\n    runAsRoot: false\n</code></pre>"},{"location":"platforms/openshift/OPENSHIFT/#openshift-routes","title":"OpenShift Routes","text":"<p>To create OpenShift Routes use the following Helm values:</p> <pre><code>ingress:\n  create: true\n  openShiftRoute: true\n  host: your.hostname.com\n  path: /yourpath # optional\n</code></pre> <p>Additional Route configuration include <code>annotations</code> and <code>routeHttpHeaders</code>:</p> <p><pre><code>ingress:\n  annotations: {}\n  routeHttpHeaders: {}\n</code></pre> Ingress values such as <code>maxBodySize</code>, <code>proxyConnectTimeout</code>, <code>proxyReadTimeout</code>, <code>proxySendTimeout</code>, <code>nginx</code>, and <code>className</code> are ignored when <code>ingress.openShiftRoute</code> is set to true. See FAQs for additional information on how to configure OpenShift routes.</p>"},{"location":"platforms/openshift/OPENSHIFT_FAQ/","title":"OpenShift FAQs","text":""},{"location":"platforms/openshift/OPENSHIFT_FAQ/#permission-errors-when-starting-containers","title":"Permission errors when starting containers","text":"<p>If you encounter the following log message upon starting the container, disregard it as it is not indicative of a critical error:</p> <pre><code>INFO:root:Generating /opt/atlassian/confluence/conf/server.xml from template server.xml.j2\nWARNING:root:Permission problem writing '/opt/atlassian/confluence/conf/server.xml'; skipping\nINFO:root:Generating /opt/atlassian/confluence/confluence/WEB-INF/classes/seraph-config.xml from template seraph-config.xml.j2\nWARNING:root:Permission problem writing '/opt/atlassian/confluence/confluence/WEB-INF/classes/seraph-config.xml'; skipping\nINFO:root:Generating /opt/atlassian/confluence/confluence/WEB-INF/classes/confluence-init.properties from template confluence-init.properties.j2\nWARNING:root:Permission problem writing '/opt/atlassian/confluence/confluence/WEB-INF/classes/confluence-init.properties'; skipping\n</code></pre> <p>The issue arises because the container's entrypoint script tries to generate and save <code>server.xml</code> and <code>seraph-config.xml</code> files but is unable to due to insufficient permissions. This is a normal behavior when Atlassian DC containers run as a non-root user. Note that both <code>server.xml</code> and <code>seraph-config.xml</code> are generated by Helm and mounted as read-only files within the container. If you need to change the default values for the <code>server.xml</code>, you can do it in the <code>&lt;product&gt;.tomcatConfig</code> stanza in the Helm values file.</p>"},{"location":"platforms/openshift/OPENSHIFT_FAQ/#no-pods-have-been-created","title":"No pods have been created","text":"<p>This can happen when deploying Atlassian DC Helm charts to OpenShift with the default settings, and the service account has not been granted any SCC other than restricted. You will typically find the following error in Kubernetes events:</p> <pre><code>Error creating: pods \"jira-0\" is forbidden:\nunable to validate against any security context constraint:\n[provider \"anyuid\": Forbidden: not usable by user or serviceaccount,\nprovider restricted-v2: .spec.securityContext.fsGroup: \nInvalid value: []int64{2001}: 2001 is not an allowed group, provider \"restricted\":\nForbidden: not usable by user or serviceaccount, provider \"nonroot-v2\":\nForbidden: not usable by user or serviceaccount, provider \"nonroot\":\nForbidden: not usable by user or serviceaccount, provider \"hostmount-anyuid\":\nForbidden: not usable by user or serviceaccount, provider \"machine-api-termination-handler\":\nForbidden: not usable by user or serviceaccount, provider \"hostnetwork-v2\":\nForbidden: not usable by user or serviceaccount, provider \"hostnetwork\":\nForbidden: not usable by user or serviceaccount, provider \"hostaccess\":\nForbidden: not usable by user or serviceaccount, provider \"node-exporter\":\nForbidden: not usable by user or serviceaccount, provider \"efs-csi-scc\":\nForbidden: not usable by user or serviceaccount, provider \"privileged\":\nForbidden: not usable by user or serviceaccount]\n</code></pre> <p>To fix this error, set <code>openshift.runWithRestrictedSCC</code> to true in Helm values.</p> <p>Another possible error in the events:</p> <pre><code>Error: container has runAsNonRoot and\nimage will run as root (pod: \"jira-0_atlassian\", container: container)\n</code></pre> <p>This can happen if the ServiceAccount has been granted <code>nonroot</code> SCC privileges, but <code>runAsUser</code> has not been set in <code>securityContext</code>. To fix the error, use the following Helm values:</p> <pre><code>jira:\n  securityContext:\n    fsGroup: 2001\n    runAsUser: 2001\n</code></pre> <p>See the complete list of Helm values for the Nonroot SCC.</p>"},{"location":"platforms/openshift/OPENSHIFT_FAQ/#whats-the-difference-between-restricted-and-nonroot-scc","title":"What's the difference between restricted and nonroot SCC?","text":"<p>From the security/compliance standpoint, Restricted SCC enforces a more secure set of rules, or rather limitations. However, running DC containers as nonroot users does not expose any security threats either. The only two disadvantages are that the ServiceAccount needs to be granted nonroot SCC privileges, and there are more Helm values to be overridden. See: Nonroot SCC.</p> <p> Running with a Restricted SCC is the recommended way to deploy Atlassian DC Helm charts. See: Restricted SCC</p>"},{"location":"platforms/openshift/OPENSHIFT_FAQ/#nfs-permission-fixer-is-disabled-how-can-i-make-shared-home-writable","title":"NFS permission fixer is Disabled, how can I make shared home writable?","text":"<p>When running Atlassian DC containers with a restricted or nonroot SCC, it is not possible to run the nfs-permission fixer container because it runs as root. You need to make sure the volume is writable either for an unprivileged restricted SCC user or a user defined in <code>securityContext</code> (with UID like 2001, 2002, 2003 depending on the product).</p> <p>If you run NFS server, you may either change permissions directly on the NFS server instance, or create a temporary ServiceAccount, grant it anyuid SCC privileges, create a pod that uses this ServiceAccount, mounts shared-home volume and recursively changes owner of the shared home directory. It can take longer for larger volumes. </p>"},{"location":"platforms/openshift/OPENSHIFT_FAQ/#can-i-use-a-custom-certificate-and-key-for-my-route","title":"Can I use a custom certificate and key for my route?","text":"<p>By default, all Routes that the Helm charts create will use the default OpenShift Router certificate and key. If you want to use a custom crt-key pair for an individual Route, create a TLS secret with <code>tls.crt</code>, <code>tls.key</code> and <code>ca.crt</code> (optional) in <code>data</code> and provide the secret name in Helm values:</p> <pre><code>ingress:\n  create: true\n  openShiftRoute: true\n  tlsSecretName: my-tls-secret\n</code></pre>"},{"location":"platforms/openshift/OPENSHIFT_FAQ/#how-can-i-configure-openshift-routes","title":"How can I configure OpenShift routes?","text":"<p>You can provide custom annotations for your Routes in the <code>ingress</code> Helm values stanza. For example, to increase connection timeout, annotate your route:</p> <pre><code>ingress:\n  create: true\n  openShiftRoute: true\n  annotations:\n    haproxy.router.openshift.io/timeout: 60s\n</code></pre> <p>You will find a list of available Route-specific annotations in the official documentation.</p> <p>Route HTTP Headers can be configured in the <code>ingress</code> Helm values stanza:</p> <pre><code>ingress:\n  create: true\n  openShiftRoute: true\n  routeHttpHeaders:\n    &lt;your-actions&gt;\n</code></pre>"},{"location":"platforms/openshift/OPENSHIFT_FAQ/#what-route-tls-termination-types-are-supported","title":"What route TLS termination types are supported?","text":"<p>Atlassian DC Helm charts only support Edge TLS termination, i.e. TLS termination is performed by the OpenShift router.</p> <p>Passthrough and Reencrypt TLS termination isn't currently supported. See: Secured Routes</p>"},{"location":"troubleshooting/LIMITATIONS/","title":"Limitations","text":""},{"location":"troubleshooting/LIMITATIONS/#product-limitations","title":"Product limitations","text":"<p>We haven't changed our Data Center applications' architecture to support Kubernetes. So, as is with all our Data Center products, the following limitations still exist:</p> <ul> <li>We don't support horizontal or vertical autoscaling in our products. Read about Product scaling.</li> <li>More pods doesn't mean that the application will be more performant.</li> <li>We still have session affinity, so you will need to have a network setup that supports that. </li> </ul>"},{"location":"troubleshooting/LIMITATIONS/#jira-and-horizontal-scaling","title":"Jira and horizontal scaling","text":"<p>At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time.</p> <ul> <li>Index replication service is paused indefinitely</li> <li>Automatic restore of indexes will fail </li> </ul> <p>Indexing improvements</p> <p>Please note that Jira is actively being worked on to address these issues in the coming releases.</p> <p>Although these issues are Jira specific, they are exasperated on account of the significantly reduced startup times for Jira when running in a Kubernetes cluster. As such these issues can have an impact on horizontal scaling if you don't take the correct approach.</p>"},{"location":"troubleshooting/LIMITATIONS/#bamboo","title":"Bamboo","text":"<p>There are a number of known limitations relating to Bamboo Data Center, these are documented below.</p>"},{"location":"troubleshooting/LIMITATIONS/#deployment","title":"Deployment","text":"<p>With Bamboo DC 8.1 deployments to K8s using the Helm charts are now possible. This release does however contain an issue where partial unattended deployments to K8s do not work. </p> <p>Unattended setup</p> <p>Until this issue has been resolved, the recommended approach for deploying Bamboo server is using an <code>unattended</code> approach. That is, providing values to all those properties labeled as <code>REQUIRED</code> and <code>UNATTENDED-SETUP</code> within the <code>values.yaml</code>. This has the added benefit of eliminating any manual intervention (via the setup wizard) required for configuring Bamboo post deployment.</p> <p>It should also be noted that the property, <code>bamboo.unattendedSetup</code> should be set to <code>true</code> (current default value) for this to work.</p>"},{"location":"troubleshooting/LIMITATIONS/#cluster-size","title":"Cluster size","text":"<p>At present Bamboo Data Center utilizes an active-passive clustering model. This architecture is not ideal where K8s deployments are concerned.</p> <p>1 pod clusters only</p> <p>At present, Bamboo server cluster sizes comprising only <code>1</code> pod is the only supported topology for now.</p>"},{"location":"troubleshooting/LIMITATIONS/#server-and-agent-affinity","title":"Server and agent affinity","text":"<p>It is preferable that the Bamboo server and agents are be deployed to the same cluster due to security and performance reasons. If agents are deployed outside the Kubernetes cluster, you need to expose Bamboo server JMS port as <code>LoadBalancer</code> or <code>NodeIP</code>:</p> <pre><code>bamboo:\n  jmsService:\n    enabled: true\n    type: (LoadBalancer|NodeIP)\n</code></pre> <p>Security and performance considerations</p> <p>If Bamboo server JMS port is exposed, it is highly recommended that you either allow access to it for Bamboo agents CIDR only (inbound security group rules if on AWS), and/or secure remote agent by configuring SSL for AMQ.</p> <p>Deploying Bamboo agents to the same cluster with the server or at least to the same region/availability zone/datacenter is highly recommended to avoid performance and latency issues.</p>"},{"location":"troubleshooting/LIMITATIONS/#bamboo-to-cloud-app-link","title":"Bamboo to Cloud App Link","text":"<p>When configuring application links between Bamboo server and any Atlassian Cloud server product, the Bamboo server base URL needs to be used. See public issue for more detail.</p>"},{"location":"troubleshooting/LIMITATIONS/#import-and-export-of-large-datasets","title":"Import and export of large datasets","text":"<p>At present there is an issue with Bamboo where the <code>/server</code> and <code>/status</code> endpoints become un-usable when performing an export or import of large datasets. </p> <p>Data migration</p> <p>For large Bamboo instances we recommend using native database and filesystem backup tools instead of the built in export / import functionality that Bamboo provides. See the migration guide for more details.</p> <p>The Bamboo Helm chart does however provide a facility that can be used to import data exports produced through Bamboo at deployment time. This can be used by configuring the Bamboo server <code>values.yaml</code> appropriately i.e. </p> <pre><code>import:\n  type: import\n  path: \"/var/atlassian/application-data/shared-home/bamboo-export.zip\"\n</code></pre> <p>Using this approach will restore the full dataset as part of the Helm install process.</p>"},{"location":"troubleshooting/LIMITATIONS/#crowd","title":"Crowd","text":""},{"location":"troubleshooting/LIMITATIONS/#loadbalancer-service-type","title":"LoadBalancer service type","text":"<p>If a Kubernetes cluster has multiple cluster nodes and Crowd <code>service.type</code> is <code>LoadBalancer</code>, Crowd pods may receive client requests from different cluster node IPs. This will cause session invalidation in the client browser, which is a security feature of Crowd. You can resolve it by either:</p> <ul> <li>Unticking \"Require consistent client IP address\" in Session configuration or</li> <li>Configuring AWS LoadBalancer with session affinity by annotating a service. You can find more details in AWS LoadBalancer documentation.</li> </ul>"},{"location":"troubleshooting/LIMITATIONS/#platform-limitations","title":"Platform limitations","text":"<p>These configurations are explicitly not supported, and the Helm charts don\u2019t work without modifications in these environments:</p> <ul> <li>Istio infrastructure<ul> <li>Due to several reasons, Istio is imposing networking rules on every workload in the Kubernetes cluster that doesn't work with our deployments.</li> <li>The current recommendation is to create an exemption for our workloads if Istio is enabled in the cluster by default.</li> </ul> </li> </ul>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/","title":"Support boundaries","text":"<p>This page describes what is within our scope of support for Kubernetes deployments, and what isn't. </p> Additional information <ul> <li> <p>Read our troubleshooting tips.</p> </li> <li> <p>Read about the product and platform limitations.</p> </li> </ul>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#supported-components","title":"Supported components","text":""},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#helm-charts","title":"Helm charts","text":"<p>Helm is a Kubernetes package manager. It allows us to provide generic <code>YAML</code> templates that you configure with the specific values for your environments.</p> <p>As described in the Prerequisites, you are responsible for creating the components that are required by the product for your type of deployment. You need to supply the appropriate values to your specific <code>values.yaml</code> file that is used for installation. We provide documentation for different configuration options in the Configuration guide and the Examples.</p> <p>If you have followed our documentation on how to configure the Helm charts, and you're using correctly created components, we will then provide support if you encounter an error with installation post <code>helm install</code>. </p> <p>If you find any issues, raise a ticket with our support team. If you have general feedback or questions regarding the charts, use Atlassian Community Kubernetes space.</p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#unsupported-components","title":"Unsupported components","text":"<p>The Prerequisites can be created in multiple ways. You are responsible for creating them correctly so that they can be used successfully with the Helm charts. Additional details on these prerequisites and their requirements below: </p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#kubernetes-cluster","title":"Kubernetes cluster","text":"<p>You need to make sure that you have enough privileges to run the application and create all the necessary entities that the Helm charts require. There are also different Kubernetes flavours that might require specific knowledge of how to install the products in them. For example, OpenShift and Rancher have more strict rules regarding container permissions.</p> <p>See examples of provisioning Kubernetes clusters on cloud-based providers.</p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#shared-storage","title":"Shared storage","text":"<p>Kubernetes setup requires you to have shared storage if you want to have a clustered instance. It's completely up to you how you set up the shared storage. The main requirement is that this storage needs to be accessible from Kubernetes and needs to be accessible from multiple pods. </p> <p>You can use a managed storage solution like EFS, Azure files, or some other dedicated solution that provides NFS-like access (e.g. dedicated NFS server, NetApp).</p> <p>There is a large number of combinations and potential setup scenarios and we can't support all of them. Our Helm charts expect you to provide a persistent volume claim, or a similar accessible shared storage in the <code>values.yaml</code> file.</p> <p>See examples of creating shared storage. For more information about volumes go to the Volumes section of the configuration guide. </p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#networking","title":"Networking","text":"<p>You're required to configure the network access to the cluster. In Kubernetes, this usually means providing an ingress controller. </p> <p>It is up to you to make sure that the network configuration doesn\u2019t prevent nodes from communicating with each other and other components.</p> <p>You also need to make sure that your instance is accessible to the users (DNS, firewalls, VPC config).</p> <p>See an example of provisioning an NGINX Ingress controller. </p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#database","title":"Database","text":"<p>Through the <code>values.yaml</code> the database is provided as a connection string with the option to provide credentials and a driver. The database needs to be configured following product-specific requirements and needs to be accessible from Kubernetes.</p> <p>See an example of provisioning databases on cloud-based providers.</p>"},{"location":"troubleshooting/SUPPORT_BOUNDARIES/#bamboo-custom-remote-agents","title":"Bamboo - custom remote agents","text":"<p>If creating and using custom remote docker agents, you're required to ensure that they are configured, built and working as expected. </p> <p>See an example of customizing a remote agent with bespoke capabilities.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/","title":"Troubleshooting tips","text":"<p>This guide contains general tips on how to investigate an application deployment that doesn't work correctly.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#general-tips","title":"General tips","text":"<p>First, it is important to gather the information that will help you better understand where to focus your investigation efforts. The next section assumes you've followed the installation and configuration guides, and you can't access the installed product service. </p> <p>For installation troubleshooting, you will need to access the Kubernetes cluster and have enough permissions to follow the commands below.</p> <p>We highly recommend that you read through the Kubernetes official documentation describing monitoring, logging and debugging. Additionally, for great starting tips read the Application Introspection and Debugging section.</p> Value placeholders <p>Some commands include <code>&lt;release_name&gt;</code> and <code>&lt;namespace&gt;</code>. Replace them with the Helm release name and namespace specified when running <code>helm install</code>.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#my-service-is-not-accessible","title":"My service is not accessible","text":"<p>After <code>helm install</code> finishes, it prints a product service URL. It usually takes a few minutes for the service to start. If you visit the URL too soon, it might return a <code>5XX</code> error HTTP code (the actual code might be dependent on your network implementation).</p> <p>If you have waited long enough (more than 10 minutes), and the service is still not accessible, it is time to investigate the reason why this is the case.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#helm-release-verification","title":"Helm release verification","text":"<ol> <li>Run <code>helm list --all-namespaces</code> to get the list of all installed chart releases.<ul> <li>You should be able to see your installation in the list</li> <li>The status for the release should be <code>deployed</code></li> </ul> </li> <li>Run <code>helm test &lt;release_name&gt; -n &lt;namespace&gt;</code><ul> <li>This should return application tests in <code>succeeded</code> phase</li> <li>In case there are any test failures you will need to further investigate the particular domain</li> </ul> </li> </ol>"},{"location":"troubleshooting/TROUBLESHOOTING/#dns-verification","title":"DNS verification","text":"<p>To verify the DNS record from your machine, run a basic <code>dig</code> test:</p> <pre><code>dig SERVICE_DOMAIN_NAME\n</code></pre> <p>Or use a web version of the tool.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#investigate-application-logs","title":"Investigate application logs","text":"<p>You can get application logs from the pods with a standard <code>kubectl</code> command:</p> <pre><code>kubectl logs APPLICATION_POD_NAME\n</code></pre> <p>You can read the output and make sure it doesn't contain an error or an exception.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#get-application-pod-details","title":"Get application pod details","text":"<p>For more details follow the official guide for debugging.</p> <p>Get the list of pods and their states:</p> <pre><code>kubectl get &lt;release_name&gt; -n &lt;namespace&gt; -o wide\n</code></pre> <p>Get details about a specific pod:</p> <pre><code>kubectl describe POD_NAME -n &lt;namespace&gt;\n</code></pre>"},{"location":"troubleshooting/TROUBLESHOOTING/#get-storage-details","title":"Get storage details","text":"<p>Each application pod needs to have successfully mounted local and shared home. You can find out the details for the persistent volume claims with this command: </p> Prerequisities <p>The example needs to have <code>jq</code> tool installed.</p> <p><pre><code>kubectl get pods --all-namespaces -o=json | jq -c \\\n'.items[] | {name: .metadata.name, namespace: .metadata.namespace, claimName:.spec.volumes[] | select( has (\"persistentVolumeClaim\") ).persistentVolumeClaim.claimName }'\n</code></pre> Find all the application pods in the output and verify they have the correct claims (shared home and local home). For more details follow the documentation for persistent volumes.</p>"},{"location":"troubleshooting/TROUBLESHOOTING/#self-signed-certificates","title":"Self signed certificates","text":"<p>Accessing applications or websites that are encrypted with SSL using certificates not signed by a public authority will result in a connection error. The stacktrace will contain the following:</p> <pre><code>caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException:\n       PKIX path building failed:sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\n</code></pre> <p>To fix the issue, follow the steps listed in the Self Signed Certificates configuration guide.</p>"},{"location":"userguide/CONFIGURATION/","title":"Configuration","text":""},{"location":"userguide/CONFIGURATION/#ingress","title":"Ingress","text":"<p>In order to make the Atlassian product available from outside of the Kubernetes cluster, a suitable HTTP/HTTPS ingress controller needs to be installed. The standard Kubernetes Ingress resource is not flexible enough for our needs, so a third-party ingress controller and resource definition must be provided. The exact details of the Ingress will be highly site-specific. These Helm charts were tested using the NGINX Ingress Controller. We also provide example instructions on how this controller can be installed and configured.</p> <p>The charts themselves provide a template for Ingress resource rules to be utilised by the provisioned controller. These include all required annotations and optional TLS configuration for the NGINX Ingress Controller.</p> <p>Some key considerations to note when configuring the controller are:</p> <p>Ingress</p> <ul> <li>At a minimum, the ingress needs the ability to support long request timeouts, as well as session affinity (aka \"sticky sessions\").</li> <li> <p>The Ingress Resource provided as part of the Helm charts is geared toward the NGINX Ingress Controller and can be configured via the <code>ingress</code> stanza in the appropriate <code>values.yaml</code>. Some key aspects that can be configured include:</p> <ul> <li>Usage of the NGINX Ingress Controller</li> <li>Ingress Controller annotations</li> <li>The request max body size</li> <li>The hostname of the ingress resource</li> </ul> </li> <li> <p>When installed, with the provided configuration, the NGINX Ingress Controller will provision an internet-facing (see diagram below) load balancer on your behalf. The load balancer should either support the Proxy Protocol or allow for the forwarding of <code>X-Forwarded-*</code> headers. This ensures any backend redirects are done so over the correct protocol.</p> </li> <li>If the <code>X-Forwarded-*</code> headers are being used, then enable the use-forwarded-headers option on the controllers <code>ConfigMap</code>. This ensures that these headers are appropriately passed on.</li> <li>The diagram below provides a high-level overview of how external requests are routed via an internet-facing load balancer to the correct service via Ingress.</li> </ul> <p></p> <p>Traffic flow (diagram)</p> <ol> <li>Inbound client request</li> <li>DNS routes request to appropriate LB</li> <li>LB forwards request to internal Ingress</li> <li>Ingress controller performs traffic routing lookup via Ingress object(s)</li> <li>Ingress forwards request to appropriate service based on Ingress object routing rule</li> <li>Service forwards request to appropriate pod</li> <li>Pod handles request</li> </ol> <p>Request body size</p> <p>By default the maximum allowed size for the request body is set to <code>250MB</code>. If the size in a request exceeds the maximum size of the client request body, an <code>413</code> error will be returned to the client. The maximum request body can be configured by changing the value of <code>maxBodySize</code> in <code>values.yaml</code>.</p>"},{"location":"userguide/CONFIGURATION/#session-stickiness-for-high-availability","title":"Session Stickiness for High Availability","text":""},{"location":"userguide/CONFIGURATION/#nginx-ingress-controller","title":"NGINX Ingress Controller","text":"<p>Session stickiness is automatically enabled when using NGINX Ingress Controller:</p> <pre><code>ingress:\n  create: true\n  nginx: true  # Automatically enables session stickiness\n  host: bitbucket.example.com\n</code></pre>"},{"location":"userguide/CONFIGURATION/#custom-configuration","title":"Custom Configuration","text":"<pre><code>ingress:\n  create: true\n  nginx: true\n  host: bitbucket.example.com\n  annotations:\n    \"nginx.ingress.kubernetes.io/session-cookie-name\": \"BITBUCKET-SESSION\"\n    \"nginx.ingress.kubernetes.io/session-cookie-max-age\": \"28800\"  # 8 hours\n    \"nginx.ingress.kubernetes.io/session-cookie-change-on-failure\": \"true\"\n</code></pre>"},{"location":"userguide/CONFIGURATION/#verify-configuration","title":"Verify Configuration","text":"<pre><code># Check ingress annotations\nkubectl describe ingress bitbucket -n atlassian\n\n# Test with curl (look for Set-Cookie header)\ncurl -I https://bitbucket.example.com/status\n</code></pre>"},{"location":"userguide/CONFIGURATION/#common-issues","title":"Common Issues","text":"Issue Solution Users getting logged out randomly Verify session stickiness annotations are applied Requests routing to different pods Check if clustering is enabled Session cookies not working Ensure ingress controller supports session affinity"},{"location":"userguide/CONFIGURATION/#other-ingress-controllers","title":"Other Ingress Controllers","text":"<p>For other ingress controllers (AWS ALB, Google Cloud Load Balancer, Azure Application Gateway), refer to your controller's documentation for session stickiness configuration.</p>"},{"location":"userguide/CONFIGURATION/#loadbalancernodeport-service-type","title":"LoadBalancer/NodePort Service Type","text":"<p>Feature Availability</p> <p>Service session affinity configuration is available starting from Helm chart version 1.13</p> <p>It is possible to make the Atlassian product available from outside of the Kubernetes cluster without using an ingress controller.</p>"},{"location":"userguide/CONFIGURATION/#nodeport-service-type","title":"NodePort Service Type","text":"<p>When installing Helm release, you can set:</p> <pre><code>jira:\n  service:\n      type: NodePort\n      sessionAffinity: ClientIP\n      sessionAffinityConfig:\n        clientIP:\n          timeoutSeconds: 10800\n</code></pre> <p>The service port will be exposed on a random port from the ephemeral port range (<code>30000</code>-<code>32767</code>) on all worker nodes. It is possible to explicitly set NodePort in <code>service.nodePort</code> (make sure it's not reserved for any existing service in the cluster). You can provision a LoadBalancer with <code>443</code> or <code>80</code> (or both) listeners that will forward traffic to the node port (you can get service node port by running <code>kubectl describe $service -n $namespace</code>). Both LoadBalancer and Kubernetes service should be configured to maintain session affinity. LoadBalancer session affinity should be configured as per instructions for your Kubernetes/cloud provider. Service session affinity is configured by overriding the default Helm chart values (see the above example). Make sure you configure networking rules to allow the LoadBalancer to communicate with the Kubernetes cluster worker node on the node port.</p> <p>Tip</p> <p>For more information about Kubernetes service session affinity, see Kubernetes documentation.</p>"},{"location":"userguide/CONFIGURATION/#loadbalancer-service-type","title":"LoadBalancer Service Type","text":"<p>LoadBalancer service type is the automated way to expose the service on the node port, create a LoadBalancer for it and configure networking rules allowing communication between the LoadBalancer and the Kubernetes cluster worker nodes:</p> <pre><code>jira:\n  service:\n      type: LoadBalancer\n      sessionAffinity: ClientIP\n      sessionAffinityConfig:\n        clientIP:\n          timeoutSeconds: 10800\n</code></pre> <p>AWS EKS users</p> <p>If you install the Helm chart for the first time, you will need to skip overriding <code>sessionAffinity</code> in your <code>values.yaml</code>, otherwise the LoadBalancer will not be created and you will see the following error: <pre><code>Error syncing load balancer: failed to ensure load balancer: unsupported load balancer affinity: ClientIP\n</code></pre> Once the Helm chart is installed with the default <code>sessionAffinity</code>, run <code>helm upgrade</code> with <code>$productName.service.sessionAffinity</code> set to <code>ClientIP</code>.</p>"},{"location":"userguide/CONFIGURATION/#volumes","title":"Volumes","text":"<p>The Data Center products make use of filesystem storage. Each DC node has its own <code>local-home</code> volume, and all nodes in the DC cluster share a single <code>shared-home</code> volume.</p> <p>By default, the Helm charts will configure all of these volumes as ephemeral emptyDir volumes. This makes it possible to install the charts without configuring any volume management, but comes with two big caveats:</p> <ol> <li>Any data stored in the <code>local-home</code> or <code>shared-home</code> will be lost every time a pod starts.</li> <li>Whilst the data that is stored in <code>local-home</code> can generally be regenerated (e.g. from the database), this can be a very expensive process that sometimes requires manual intervention.</li> </ol> <p>For these reasons, the default volume configuration of the Helm charts is suitable only for running a single DC pod for evaluation purposes. Proper volume management needs to be configured in order for the data to survive restarts, and for multi-pod DC clusters to operate correctly.</p> <p>While you are free to configure your Kubernetes volume management in any way you wish, within the constraints imposed by the products, the recommended setup is to use Kubernetes PersistentVolumes and <code>PersistentVolumeClaims</code>.</p> <p>The <code>local-home</code> volume requires a <code>PersistentVolume</code> with ReadWriteOnce (RWO) capability, and <code>shared-home</code> requires a <code>PersistentVolume</code> with ReadWriteMany (RWX) capability. Typically, this will be an NFS volume provided as part of your infrastructure, but some public-cloud Kubernetes engines provide their own <code>RWX</code> volumes (e.g. AWS EFS, Google Filestore, Azure Files). While this entails a higher upfront setup effort, it gives the best flexibility.</p>"},{"location":"userguide/CONFIGURATION/#volumes-configuration","title":"Volumes configuration","text":"<p>By default, the charts will configure the <code>local-home</code> and <code>shared-home</code> values as follows:</p> <pre><code>volumes:\n  - name: local-home\n    emptyDir: {}\n  - name: shared-home\n    emptyDir: {}\n</code></pre> <p>As explained above, this default configuration is suitable only for evaluation or testing purposes. Proper volume management needs to be configured.</p> <p>Bitbucket default <code>shared-home</code> location</p> <p>For a single node Bitbucket deployment, if no <code>shared-home</code> volume is defined, then a subpath of <code>local-home</code> will automatically be used for this purpose, namely: <code>&lt;LOCAL_HOME_DIRECTORY&gt;/shared</code>. This behaviour is specific to Bitbucket itself and is not orchestrated via the Helm chart.</p> <p>In order to enable the persistence of data stored in these volumes, it is necessary to replace these volumes with something else.</p> <p>The recommended way is to enable the use of <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> for both volumes, using your install-specific <code>values.yaml</code> file, for example:</p> <pre><code>volumes:\n  localHome:\n    persistentVolumeClaim:\n      create: true\n  sharedHome:\n    persistentVolumeClaim:\n      create: true\n</code></pre> <p>This will result in each pod in the <code>StatefulSet</code> creating a <code>local-home</code> <code>PersistentVolumeClaim</code> of type <code>ReadWriteOnce</code>, and a single <code>PersistentVolumeClaim</code> of type <code>ReadWriteMany</code> being created for the <code>shared-home</code>.</p> <p>For each <code>PersistentVolumeClaim</code> created by the chart, a suitable <code>PersistentVolume</code> needs to be made available prior to installation. These can be provisioned either statically or dynamically, using an auto-provisioner.</p> <p>An alternative to <code>PersistentVolumeClaims</code> is to use inline volume definitions, either for <code>local-home</code> or <code>shared-home</code> (or both), for example:</p> <pre><code>volumes:\n  localHome:\n    customVolume:\n      hostPath:\n        path: /path/to/my/data\n  sharedHome:\n    customVolume:\n      nfs:\n        server: mynfsserver\n        path: /export/path\n</code></pre> <p>Generally, any valid Kubernetes volume resource definition can be substituted here. However, as mentioned previously, externalising the volume definitions using <code>PersistentVolumes</code> is the strongly recommended approach.</p>"},{"location":"userguide/CONFIGURATION/#volumes-examples","title":"Volumes examples","text":"<ol> <li>Bitbucket needs a dedicated NFS server providing persistence for a shared home if you are not using Bitbucket Mesh. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket. Please also review   Bitbucket Supported platforms.</li> <li>We have an example detailing how an existing EFS filesystem can be created and consumed using static provisioning: Shared storage - utilizing AWS EFS-backed filesystem.</li> <li>You can also refer to an example on how a Kubernetes cluster and helm deployment can be configured to utilize AWS EBS backed volumes: Local storage - utilizing AWS EBS-backed volumes.</li> </ol>"},{"location":"userguide/CONFIGURATION/#additional-volumes","title":"Additional volumes","text":"<p>In addition to the <code>local-home</code> and <code>shared-home</code> volumes that are always attached to the product pods, you can attach your own volumes for your own purposes, and mount them into the product container.  Use the <code>additional</code> (under <code>volumes</code>) and <code>additionalVolumeMounts</code> values to both attach the volumes and mount them in to the product container.</p> <p>This might be useful if, for example, you have a custom plugin that requires its own filesystem storage.</p> <p>Example:</p> <pre><code>jira:\n  additionalVolumeMounts:\n    - volumeName: my-volume\n      mountPath: /path/to/mount\nvolumes:\n  additional:\n    - name: my-volume\n      persistentVolumeClaim:\n        claimName: my-volume-claim\n</code></pre>"},{"location":"userguide/CONFIGURATION/#database-connectivity","title":"Database connectivity","text":"<p>The products need to be supplied with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences.</p>"},{"location":"userguide/CONFIGURATION/#databaseurl","title":"<code>database.url</code>","text":"<p>All products require the JDBC URL of the database. The format if this URL depends on the JDBC driver being used, but some examples are:</p> Vendor JDBC driver class Example JDBC URL PostgreSQL <code>org.postgresql.Driver</code> <code>jdbc:postgresql://&lt;dbhost&gt;:5432/&lt;dbname&gt;</code> MySQL <code>com.mysql.jdbc.Driver</code> <code>jdbc:mysql://&lt;dbhost&gt;/&lt;dbname&gt;</code> SQL Server <code>com.microsoft.sqlserver.jdbc.SQLServerDriver</code> <code>jdbc:sqlserver://&lt;dbhost&gt;:1433;databaseName=&lt;dbname&gt;</code> Oracle <code>oracle.jdbc.OracleDriver</code> <code>jdbc:oracle:thin:@&lt;dbhost&gt;:1521:&lt;SID&gt;</code> <p>Database creation</p> <p>The Atlassian product doesn't automatically create the database,<code>&lt;dbname&gt;</code>, in the <code>JDBC URL</code>, so you need to manually create a user and database for the used database instance. Details on how to create product-specific databases can be found below:</p> JiraConfluenceBitbucketBambooCrowd <p>Connect Jira to an external database</p> <p>Connect Confluence to an external database</p> <p>Connect Bitbucket to an external database</p> <p>Connect Bamboo to an external database</p> <p>Connect Crowd to an external database</p>"},{"location":"userguide/CONFIGURATION/#databasedriver","title":"<code>database.driver</code>","text":"<p>Jira and Bitbucket require the JDBC driver class to be specified (Confluence and Bamboo will autoselect this based on the <code>database.type</code> value, see below). The JDBC driver must correspond to the JDBC URL used; see the table above for example driver classes.</p> <p>Note that the products only ship with certain JDBC drivers installed, depending on the license conditions of those drivers.</p> <p>Non-bundled DB drivers</p> <p>MySQL and Oracle database drivers are not shipped with the products due to licensing restrictions. You will need to provide <code>additionalLibraries</code> configuration.</p>"},{"location":"userguide/CONFIGURATION/#databasetype","title":"<code>database.type</code>","text":"<p>Jira, Confluence and Bamboo all require this value to be specified, this declares the database engine to be used. The acceptable values for this include:</p> Vendor Jira Confluence Bamboo PostgreSQL <code>postgres72</code> <code>postgresql</code> <code>postgresql</code> MySQL <code>mysql57</code> / <code>mysql8</code> <code>mysql</code> <code>mysql</code> SQL Server <code>mssql</code> <code>mssql</code> <code>mssql</code> Oracle <code>oracle10g</code> <code>oracle</code> <code>oracle12c</code> Aurora PostgreSQL <code>postgresaurora96</code>"},{"location":"userguide/CONFIGURATION/#databasecredentials","title":"<code>database.credentials</code>","text":"<p>All products can have their database connectivity and credentials specified either interactively during first-time setup, or automatically by specifying certain configuration via Kubernetes.</p> <p>Depending on the product, the <code>database.type</code>, <code>database.url</code> and <code>database.driver</code> chart values can be provided. In addition, the database username and password can be provided via a Kubernetes secret, with the secret name specified with the <code>database.credentials.secretName</code> chart value. When all the required information is provided in this way, the database connectivity configuration screen will be bypassed during product setup.</p>"},{"location":"userguide/CONFIGURATION/#namespace","title":"Namespace","text":"<p>The Helm charts are not opinionated whether they have a Kubernetes namespace to themselves. If you wish, you can run multiple Helm releases of the same product in the same namespace.</p>"},{"location":"userguide/CONFIGURATION/#clustering","title":"Clustering","text":"<p>By default, the Helm charts will not configure the products for Data Center clustering. In order to enable clustering, the <code>enabled</code> property for clustering must be set to <code>true</code>.</p> <p>Clustering by default for Crowd</p> <p>Crowd does not offer clustering configuration via Helm Chart. Set <code>crowd.clustering.enabled</code> to <code>true/false</code> in <code>${CROWD_HOME}/shared/crowd.cfg.xml</code> and rollout restart Crowd StatefulSet after the initial product setup is complete.</p> JiraConfluenceBitbucketBambooCrowd <pre><code>jira:\n  clustering:\n    enabled: true\n</code></pre> <pre><code>confluence:\n  clustering:\n    enabled: true\n</code></pre> <pre><code>bitbucket:\n  clustering:\n    enabled: true\n</code></pre> <p>Because of the limitations outlined under Bamboo and clustering the <code>clustering</code> stanza is not available as a configurable property in the Bamboo <code>values.yaml</code>.</p> <p>Clustering is enabled by default. To disable clustering, set <code>crowd.clustering.enabled</code> to <code>false</code> in <code>${CROWD_HOME}/shared/crowd.cfg.xml</code> and rollout restart Crowd StatefulSet after the initial product setup is complete.</p> <p>In addition, the <code>shared-home</code> volume must be correctly configured as a ReadWriteMany (RWX) filesystem (e.g. NFS, AWS EFS, Azure Files, GCP Filestore, or any NFS-compatible storage)</p>"},{"location":"userguide/CONFIGURATION/#generating-configuration-files","title":"Generating configuration files","text":"<p>The Docker entrypoint scripts generate application configuration on first start; not all of these files are regenerated on subsequent starts. This is deliberate, to avoid race conditions or overwriting manual changes during restarts and upgrades. However, in deployments where configuration is purely specified through the environment (e.g. Kubernetes) this behaviour may be undesirable; this flag forces an update of all generated files.</p> <p>The affected files are: - Jira: <code>dbconfig.xml</code> - Confluence: <code>confluence.cfg.xml</code> - Bamboo: <code>bamboo.cfg.xml</code></p> <p>To force update of the configuration files when pods restart, set <code>&lt;product_name.forceConfigUpdate&gt;</code> to true. You can do it by passing an argument to helm install/update command: <pre><code>--set jira.forceConfigUpdate=true\n</code></pre> or set it in <code>values.yaml</code>:</p> <pre><code>jira:\n  forceConfigUpdate: true\n</code></pre> <p>Bitbucket and Bitbucket Mesh configuration file</p> <p>It's not possible to generate the Bitbucket and Bitbucket Mesh configuration files. Bitbucket uses <code>${BITBUCKET_HOME}/shared/bitbucket.properties</code>. The properties are from  bitbucket.properties file  and can be provided as environment variables. For example: <pre><code>bitbucket:\n  additionalEnvironmentVariables:\n  - name: SEACRH_ENABLED\n    value: false\n  - name: PLUGIN_SEARCH_CONFIG_BASEURL\n    value: http://my.opensearch.host\n</code></pre> Bitbucket Mesh uses <code>${BITBUCKET_HOME}/mesh.properties</code>. The properties are from  mesh.properties file  and can be provided as environment variables. For example: <pre><code>bitbucket:\n  mesh:\n    additionalEnvironmentVariables:\n    - name: GIT_PATH_EXECUTABLE\n      value: /usr/bin/git\n</code></pre></p> <p>To translate property into an environment variable:</p> <ul> <li>dot <code>.</code> becomes underscore <code>_</code></li> <li>dash <code>-</code> becomes underscore <code>_</code></li> <li>Example: <code>this.new-property</code> becomes <code>THIS_NEW_PROPERTY</code></li> </ul>"},{"location":"userguide/CONFIGURATION/#additional-libraries-plugins","title":"Additional libraries &amp; plugins","text":"<p>The products' Docker images contain the default set of bundled libraries and plugins. Additional libraries and plugins can be mounted into the product containers during the Helm install. One such use case for this is mounting <code>JDBC</code> drivers that are not shipped with the products' by default.</p> <p>To make use of this mechanism, the additional files need to be available as part of a Kubernetes volume. Options here include putting them into the <code>shared-home</code> volume that's required as part of the prerequisites. Alternatively, you can create a custom <code>PersistenVolume</code> for them, as long as it has <code>ReadOnlyMany</code> capability.</p> <p>Custom volumes for loading libraries</p> <p>If you're not using the <code>shared-home</code> volume, then you can declare your own custom volume, by following the Additional volumes section above.</p> <p>You could even store the files as a <code>ConfigMap</code> that gets mounted as a volume, but you're likely to run into file size limitations there.</p> <p>Assuming that the existing <code>shared-home</code> volume is used for this, then the only configuration required is to specify the <code>additionalLibraries</code> in your <code>values.yaml</code> file, e.g.</p> <pre><code>jira:\n  additionalLibraries:\n    - volumeName: shared-home\n      subDirectory: mylibs\n      fileName: lib1.jar\n    - volumeName: shared-home\n      subDirectory: mylibs\n      fileName: lib2.jar\n</code></pre> <p>This will mount the <code>lib1.jar</code> and <code>lib2.jar</code> from the <code>mylibs</code> sub-directory from <code>shared-home</code> into the appropriate place in the container.</p> <p>Similarly, you can use <code>additionalBundledPlugins</code> to load product plugins into the container.</p> <p>System plugin</p> <p>Plugins installed via this method will appear as system plugins rather than user plugins. An alternative to this method is to install the plugins via \"Manage Apps\" in the product system administration UI.</p> <p>For more details on the above, and how 3rd party libraries can be supplied to a Pod see the example External libraries and plugins</p>"},{"location":"userguide/CONFIGURATION/#p1-plugins","title":"P1 Plugins","text":"<p>While <code>additionalLibraries</code> and <code>additionalBundledPlugins</code> will mount files to <code>/opt/atlassian/&lt;product-name&gt;/lib</code> and <code>/opt/atlassian/&lt;product-name&gt;/&lt;product-name&gt;/WEB-INF/atlassian-bundled-plugins</code> respectively, plugins built on Atlassian Plugin Framework 1 need to be stored in <code>/opt/atlassian/&lt;product-name&gt;/&lt;product-name&gt;/WEB-INF/lib</code>. While there's no dedicated Helm values stanza for P1 plugins, it is fairly easy to persist them (the below example is for Jira deployed in namespace <code>atlassian</code>):</p> <ul> <li> <p>In your shared-home, create a directory called <code>p1-plugins</code>:   <pre><code>kubectl exec -ti jira-0 -n atlassian \\\n        -- mkdir -p /var/atlassian/application-data/shared-home/p1-plugins\n</code></pre></p> </li> <li> <p>Copy a P1 plugin to the newly created directory in shared-home:   <pre><code>kubectl cp hello.jar \\\n  atlassian/jira-0:/var/atlassian/application-data/shared-home/p1-plugins/hello.jar\n</code></pre></p> </li> <li> <p>Add the following to your custom values:   <pre><code>jira:\n  additionalVolumeMounts:\n    - name: shared-home\n      mountPath: /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/hello.jar\n      subPath: p1-plugins/hello.jar\n</code></pre></p> </li> </ul> <p>Run <code>helm upgrade</code> to find <code>/var/atlassian/application-data/shared-home/p1-plugins/hello.jar</code> mounted into <code>/opt/atlassian/jira/atlassian-jira/WEB-INF/lib</code> in all pods of Jira StatefulSet.</p> <p>You can use the same approach to mount any files from a <code>shared-home</code> volume into any location in the container to persist such files across container restarts.</p>"},{"location":"userguide/CONFIGURATION/#cpu-and-memory-requests","title":"CPU and memory requests","text":"<p>The Helm charts allow you to specify container-level CPU and memory resource requests and limits e.g.</p> <pre><code>jira:\n  resources:\n    container:\n      requests:\n        cpu: \"4\"\n        memory: \"8G\"\n</code></pre> <p>By default, the Helm Charts have no container-level resource limits, however there are default requests that are set.</p> <p>Specifying these values is fine for CPU limits/requests, but for memory resources it is also necessary to configure the JVM's memory limits. By default, the JVM maximum heap size is set to 1 GB, so if you increase (or decrease) the container memory resources as above, you also need to change the JVM's max heap size, otherwise the JVM won't take advantage of the extra available memory (or it'll crash if there isn't enough).</p> <p>You specify the JVM memory limits like this:</p> <pre><code>jira:\n  resources:\n    jvm:\n      maxHeap: \"8g\"\n</code></pre> <p>Another difficulty for specifying memory resources is that the JVM requires additional overheads over and above the max heap size, and the container resources need to take account of that.  A safe rule-of-thumb would be for the container to request 2x the value of the max heap for the JVM.</p> <p>This requirement to configure both the container memory and JVM heap will hopefully be removed.</p> <p>You can read more about resource scaling and resource requests and limits.</p>"},{"location":"userguide/CONFIGURATION/#additional-containers","title":"Additional containers","text":"<p>The Helm charts allow you to add your own <code>container</code> and initContainer entries to the product pods. Use the <code>additionalContainers</code> and <code>additionalInitContainers</code> stanzas within the <code>values.yaml</code> for this. One use-case for an additional container would be to attach a sidecar container to the product pods.</p>"},{"location":"userguide/CONFIGURATION/#additional-options","title":"Additional options","text":"<p>The Helm charts also allow you to specify:</p> <ul> <li><code>additionalLabels</code></li> <li><code>tolerations</code>,</li> <li><code>nodeSelectors</code> </li> <li><code>affinities</code>.</li> <li><code>ordinals</code>.</li> </ul> <p>These are standard Kubernetes structures that will be included in the pods.</p>"},{"location":"userguide/CONFIGURATION/#startup-readiness-and-liveness-probes","title":"Startup, Readiness and Liveness Probes","text":""},{"location":"userguide/CONFIGURATION/#readiness-probe","title":"Readiness Probe","text":"<p>By default, a <code>readinessProbe</code> is defined for the main (server) container for all Atlassian DC Helm charts. A <code>readinessProbe</code> makes <code>HTTP</code> calls to the application status endpoint (<code>/status</code>). A pod is not marked as ready until the <code>readinessProbe</code> receives an <code>HTTP</code> response code greater than or equal to <code>200</code>, and less than <code>400</code>, indicating success.</p> <p>If a pod is not in a ready state, there is no endpoint associated with a service, as a result such a pod receives no traffic (if this is the only pod in the <code>StatefulSet</code>, you may see <code>503</code> response from the Ingress).</p> <p>It is possible to disable the <code>readinessProbe</code> (set <code>&lt;product&gt;.readinessProbe.enabled=false</code>) which may make sense if this is the first (cold) start of a DC product in Kubernetes. With a disabled <code>readinessProbe</code>, the pod almost immediately becomes ready after it has been started, and the Ingress URL will take you to a page showing node start process. We strongly recommend enabling the <code>readinessProbe</code> after the application has been fully migrated and setup in Kubernetes.</p> <p>Depending on the dataset size, resources allocation and application configuration, you may want to adjust the <code>readinessProbe</code> to work best for your particular DC workload:</p> <pre><code>readinessProbe:\n  # -- Whether to apply the readinessProbe check to pod.\n  #\n  enabled: true\n\n  # -- The initial delay (in seconds) for the container readiness probe,\n  # after which the probe will start running.\n  #\n  initialDelaySeconds: 10\n\n  # -- How often (in seconds) the container readiness probe will run\n  #\n  periodSeconds: 5\n\n  # -- Number of seconds after which the probe times out\n  #\n  timeoutSeconds: 1\n\n  # -- The number of consecutive failures of the container readiness probe\n  # before the pod fails readiness checks.\n  #\n  failureThreshold: 60\n</code></pre>"},{"location":"userguide/CONFIGURATION/#startup-and-liveness-probes","title":"Startup and Liveness Probes","text":"<p><code>startupProbe</code> and <code>livenessProbe</code></p> <p>Both <code>startupProbe</code> and <code>livenessProbe</code> are disabled by default. Make sure you go through the Kubernetes documentation before enabling such probes. Misconfiguration can result in unwanted container restarts and failed \"cold\" starts.</p>"},{"location":"userguide/CONFIGURATION/#self-signed-certificates","title":"Self Signed Certificates","text":"<p>There are 2 ways to add self-signed certificates to Java truststore: from a single secret or multiple secrets.</p> From a single secretFrom multiple secrets <ul> <li>Create a Kubernetes secret containing base64-encoded certificate(s). Here's an example kubectl command to create a secret from 2 local files:</li> </ul> <pre><code>kubectl create secret generic dev-certificates \\\n    --from-file=stg.crt=./stg.crt \\\n    --from-file=dev.crt=./dev.crt -n $namespace\n</code></pre> <p>The resulting secret will have the following data:</p> <pre><code>data:\n  stg.crt: base64encodedstgcrt\n  dev.crt: base64encodeddevcrt\n</code></pre> <p>You can have as many keys (certificates) in the secret as required. All keys will be mounted as files to <code>/tmp/crt</code> in the container and imported into Java truststore. In the example above, certificates will be mounted as <code>/tmp/crt/stg.crt</code> and <code>/tmp/crt/dev.crt</code>. File extension in the secret keys does not matter as long as the file is a valid certificate.</p> <ul> <li>Provide the secret name in Helm values (unlike the case with multiple secrets you don't need to provide secret keys):</li> </ul> <pre><code>jira:\n  additionalCertificates:\n     secretName: dev-certificates\n</code></pre> <ul> <li>Create 2 Kubernetes secrets containing base64-encoded certificate(s). Here's an example kubectl command to create 2 secrets from local files (the first one with 2 certificates/keys and the second one with just one):</li> </ul> <pre><code>kubectl create secret generic dev-certificates \\\n    --from-file=stg.crt=./stg.crt \\\n    --from-file=dev.crt=./dev.crt -n $namespace\n\nkubectl create secret generic root-ca \\\n    --from-file=ca.crt=./ca.crt -n $namespace\n</code></pre> <p>You can have as many keys (certificates) in the secrets, however, you will need to list the keys you'd like to get mounted. All keys will be mounted as files to <code>/tmp/crt</code> in the container and imported into Java truststore.</p> <ul> <li>Provide the list of secrets and their keys in Helm values:</li> </ul> <p><pre><code>jira:\n  additionalCertificates:\n    secretList:\n      - name: dev-certificates\n        keys:\n          - stg.crt\n          - dev.crt\n      - name: root-ca\n        keys:\n          - ca.crt\n</code></pre> To allow having identical keys in different secrets, filenames will have the following format: <code>&lt;secret-name&gt;-&lt;key&gt;</code>, so files will get mounted as <code>/tmp/crt/dev-certificates-stg.crt</code>, <code>/tmp/crt/dev-certificates-dev.crt</code> and <code>/tmp/crt/root-ca-ca.crt</code> and imported to Java truststore with the same aliases.</p> <p>The product Helm chart will add additional <code>volumeMounts</code> and <code>volumes</code> to the pod(s), as well as an extra init container that will:</p> <ul> <li>copy the default Java cacerts to a runtime volume shared between the init container and the main container at <code>/var/ssl</code></li> <li>run keytool -import to import all certificates in <code>/tmp/crt</code> mounted from secret(s) to <code>/var/ssl/cacerts</code></li> <li><code>-Djavax.net.ssl.trustStore=/var/ssl/cacerts</code> system property will be automatically added to <code>JVM_SUPPORT_RECOMMENDED_ARGS</code> environment variable.</li> </ul> <p>If necessary, it is possible to override the default <code>keytool -import</code> command:</p> <pre><code>jira:\n  additionalCertificates:\n     secretName: dev-certificates\n     customCmd: keytool -import ...\n</code></pre>"},{"location":"userguide/CONFIGURATION/#atlassian-support-and-analytics","title":"Atlassian Support and Analytics","text":"<p>Starting from <code>1.17.0</code> Helm chart version, by default, an additional <code>ConfigMap</code> is created and mounted into <code>/opt/atlassian/helm</code> in the containers. This <code>ConfigMap</code> has 2 keys: <code>values.yaml</code> and <code>analytics.json</code> that are picked up by the Atlassian Troubleshooting and Support Tools (ATST) plugin. To disable either of the keys or the entire <code>ConfigMap</code>, set enabled to <code>false</code> in the following Helm values stanza:</p> <pre><code>atlassianAnalyticsAndSupport:\n  analytics:\n    enabled: true\n  helmValues:\n    enabled: true\n</code></pre> <p>Helm values are mounted to be included to the support.zip. The values file is sanitized both on the Helm chart side (any <code>additionalEnvironmentVariables</code> and <code>additionalJvmArgs</code> that can potentially contain sensitive information are redacted) and ATST plugin that will redact hostnames, URLs, AWS ARNs and sensitive environment variables and JVM flags, if any. If the file is found at <code>/opt/atlassian/helm/values.yaml</code> you will see an option to include it to the support.zip when generating one in admin UI.</p> <p>Analytics json is a subset of <code>values.yaml</code> and contains selected Helm values that are sent as an analytics event and written to analytics logs, if analytics is enabled in the product. Analytics values are purely informational and contain information on how Helm charts are used.</p> <p>You can find the complete list of analytics values in <code>_helpers.tpl</code>, <code>&lt;product&gt;.analyticsJson</code>.</p>"},{"location":"userguide/CONFIGURATION/#tunnels","title":"Tunnels","text":"<p>Jira and Confluence Helm charts support configuring tunnelling. To enable tunneling, set the following in your Helm values file: <pre><code>jira:\n  tunnel:\n    additionalConnector:\n      port: 8093\n</code></pre></p> <p>An additional connector will be added to server.xml along with <code>Dsecure.tunnel.upstream.port</code> system property. If necessary, connector configuration can be overridden by setting <code>additionalConnector</code> properties to custom values:</p> <pre><code>jira:\n  tunnel:\n    additionalConnector:\n      port: 8093\n      connectionTimeout: \"200000\"\n      maxThreads: \"100\"\n      minSpareThreads: \"20\"\n      enableLookups: \"true\"\n      acceptCount: \"100\"\n      URIEncoding: \"UTF-8\"\n      secure: false\n</code></pre>"},{"location":"userguide/INSTALLATION/","title":"Installation","text":"<p>Follow these instructions to install your Atlassian product using the Helm charts. Before you proceed with the installation, make sure you have followed the Prerequisites guide.</p>"},{"location":"userguide/INSTALLATION/#1-add-the-helm-chart-repository","title":"1. Add the Helm chart repository","text":"<p>Add the Helm chart repository to your local Helm installation:</p> <pre><code>helm repo add atlassian-data-center \\\n https://atlassian.github.io/data-center-helm-charts\n</code></pre> <p>Update the repository:</p> <pre><code>helm repo update\n</code></pre>"},{"location":"userguide/INSTALLATION/#2-obtain-valuesyaml","title":"2. Obtain <code>values.yaml</code>","text":"<p>Obtain the default product <code>values.yaml</code> file from the chart:</p> <pre><code>helm show values atlassian-data-center/&lt;product&gt; &gt; values.yaml\n</code></pre> <p>Bamboo deployments</p> <p>If deploying Bamboo, be sure to read about the current limitations relating to Bamboo deployments and values.yaml</p>"},{"location":"userguide/INSTALLATION/#3-configure-database","title":"3. Configure database","text":"<p>Crowd deployments</p> <p>Crowd Data Center Helm chart does not support unattended installation. Connection to the database must be manually configured during the product setup.</p> <p>Using the <code>values.yaml</code> file obtained in step 2, configure the usage of the database provisioned as part of the prerequisites. </p> <p>Automated setup steps</p> <p>By providing all the required database values, you will bypass the database connectivity configuration during the product setup.</p> <p>Migration</p> <p>If you are migrating an existing Data Center product to Kubernetes, use the values of your product's database. See Migration guide.</p> <p>Create a Kubernetes secret to store the connectivity details of the database:</p> <pre><code>kubectl create secret generic &lt;secret_name&gt; --from-literal=username='&lt;db_username&gt;' --from-literal=password='&lt;db_password&gt;'\n</code></pre> <p>Using the Kubernetes secret, update the <code>database</code> stanza within <code>values.yaml</code> appropriately. Refer to the commentary within the <code>values.yaml</code> file for additional details on how to configure the remaining database values:</p> <pre><code>database:\n  type: &lt;db_type&gt;\n  url: &lt;jdbc_url&gt;\n  driver: &lt;engine_driver&gt;\n  credentials:\n    secretName: &lt;secret_name&gt;\n    usernameSecretKey: username\n    passwordSecretKey: password\n</code></pre> <p>Database connectivity</p> <p>For additional information on how the above values should be configured, see the Database connectivity section of the configuration guide.</p> <p>Read about Kubernetes secrets.</p>"},{"location":"userguide/INSTALLATION/#4-configure-ingress","title":"4. Configure Ingress","text":"<p>Using the <code>values.yaml</code> file obtained in step 2, configure the Ingress controller provisioned as part of the Prerequisites. The values you provide here will be used to provision an Ingress resource for the controller. Refer to the associated comments within the <code>values.yaml</code> file for additional details on how to configure the Ingress resource:</p> <pre><code>ingress:\n  create: true #1. Setting true here will create an Ingress resource\n  nginx: true #2. If using the ingress-nginx controller set this property to true\n  maxBodySize: 250m\n  host: &lt;dns_host_name&gt; #2. Hosts can be precise matches (for example \u201cfoo.bar.com\u201d) or a wildcard (for example \u201c*.foo.com\u201d).\n  path: \"/\"\n  annotations:\n    cert-manager.io/issuer: &lt;certificate_issuer&gt;\n  https: true\n  tlsSecretName: &lt;tls_certificate_name&gt;\n</code></pre> <p>Ingress configuration</p> <p>For additional details on Ingress controllers see the Ingress section of the configuration guide. </p> <p>See an example of how to set up a controller.</p>"},{"location":"userguide/INSTALLATION/#5-configure-persistent-storage","title":"5. Configure persistent storage","text":"<p>Using the <code>values.yaml</code> file obtained in step 2, configure the <code>shared-home</code> that was provisioned as part of the Prerequisites. See shared home example.</p> <p>If you are migrating an existing Data Center product to Kubernetes, use the values of your product's shared home. </p> <pre><code>volumes:\n  sharedHome:\n    customVolume:\n      persistentVolumeClaim:\n        claimName: &lt;pvc_name&gt;\n</code></pre> <p>Each pod will also require its own <code>local-home</code> storage. This can be configured with a <code>StorageClass</code>, as can be seen in the local home example. Having created the <code>StorageClass</code>, update <code>values.yaml</code> to make use of it: </p> <pre><code>volumes:\n  localHome:\n    persistentVolumeClaim:\n      create: true\n      storageClassName: &lt;storage-class-name&gt;\n</code></pre> <p>Volume configuration</p> <p>For more details, refer to the Volumes section of the configuration guide.</p> <p>Bitbucket shared storage</p> <p>Bitbucket needs a dedicated NFS server providing persistence for a shared home. Prior to installing the Helm chart, a suitable NFS shared storage solution must be provisioned. The exact details of this resource will be highly site-specific, but you can use this example as a guide: Implementation of an NFS Server for Bitbucket.</p>"},{"location":"userguide/INSTALLATION/#6-configure-clustering","title":"6. Configure clustering","text":"<p>By default, the Helm charts will not configure the products for Data Center clustering. You can enable clustering in the <code>values.yaml</code> file:</p> <pre><code>  clustering:\n    enabled: true\n</code></pre> <p>Bamboo clustering</p> <p>Because of the limitations outlined under Bamboo and clustering the <code>clustering</code> stanza is not available as a configurable property in the Bamboo <code>values.yaml</code>.</p> <p>Crowd clustering</p> <p>Crowd does not offer clustering configuration via Helm Chart. Set <code>crowd.clustering.enabled</code> to <code>true/false</code> in <code>${CROWD_HOME}/shared/crowd.cfg.xml</code> and rollout restart Crowd StatefulSet after the initial product setup is complete.</p>"},{"location":"userguide/INSTALLATION/#7-configure-license","title":"7. Configure license","text":"<p>Pre-configuring license</p> <p>Pre-provisioning a license in this way is only applicable to <code>Confluence</code>, <code>Bitbucket</code> and <code>Bamboo</code> deployments. For <code>Jira</code> and <code>Crowd</code> deployments a license can be supplied via the setup wizard post deployment.</p> <p>You can configure the product license if you provide a <code>license</code> stanzas within the <code>values.yaml</code> obtained in step 2. To do that, create a Kubernetes secret to hold the product license:</p> <pre><code>kubectl create secret generic &lt;license_secret_name&gt; --from-literal=license-key='&lt;product_license_key&gt;'\n</code></pre> <p>Update the <code>values.yaml</code> file with the secrets:</p> <pre><code>license:\n  secretName: &lt;secret_name&gt;\n  secretKey: license-key\n</code></pre> Sysadmin credentials for Bitbucket and Bamboo  <p><code>Bitbucket</code> and <code>Bamboo</code> are slightly different from the other products in that they can be completely configured during deployment, meaning no manual setup is required. To do this, you need to update the <code>sysadminCredentials</code> and also provide the <code>license</code> stanza from the previous step.</p> <p>Create a Kubernetes secret to hold the Bitbucket/Bamboo system administrator credentials:</p> <pre><code>kubectl create secret generic &lt;sysadmin_creds_secret_name&gt; --from-literal=username='&lt;sysadmin_username&gt;' --from-literal=password='&lt;sysadmin_password&gt;' --from-literal=displayName='&lt;sysadmin_display_name&gt;' --from-literal=emailAddress='&lt;sysadmin_email&gt;'\n</code></pre> <p>Update the <code>values.yaml</code> file with the secrets:</p> <pre><code>sysadminCredentials:\n  secretName: &lt;sysadmin_creds_secret_name&gt;\n  usernameSecretKey: username\n  passwordSecretKey: password\n  displayNameSecretKey: displayName\n  emailAddressSecretKey: emailAddress\n</code></pre>"},{"location":"userguide/INSTALLATION/#8-configure-container-images","title":"8. Configure container images","text":"<p>By default, container images are pulled from official Atlassian DockerHub repositories. Deployments may also use 2 other official non-Altassian images - alpine and fluentd.</p> <p>In air-gapped environments that cannot directly access DockerHub you will need to pass custom repositories/tags to the Helm installation/upgrade command.</p>"},{"location":"userguide/INSTALLATION/#product-images","title":"Product images","text":"<p>For product images (such as Jira, Bitbucket, Confluence, Bamboo), update <code>values.yaml</code>:</p> <pre><code>image:\n  registry: artifactory.mycompany.com\n  repository: jira\n  tag: 7.8.20\n</code></pre>"},{"location":"userguide/INSTALLATION/#helper-containers","title":"Helper containers","text":"<p>If <code>volumes.sharedHome.persistentVolumeClaim.sharedHome.nfsPermissionFixer.enabled</code> is set to <code>true</code>, update <code>values.yaml</code>:</p> <pre><code>volumes:\n  sharedHome:\n    nfsPermissionFixer:\n      imageRepo: artifactory.mycompany.com/alpine\n      imageTag: latest\n</code></pre> <p>If <code>fluentd.enabled</code> is set to <code>true</code> (false by default), update <code>values.yaml</code>:</p> <pre><code>fluentd:\n  imageRepo: artifactory.mycompany.com/fluentd-kubernetes-daemonset\n  imageTag: v1.11.5-debian-elasticsearch7-1.2\n</code></pre>"},{"location":"userguide/INSTALLATION/#9-install-your-chosen-product","title":"9. Install your chosen product","text":"<pre><code>helm install &lt;release-name&gt; \\\n             atlassian-data-center/&lt;product&gt; \\\n             --namespace &lt;namespace&gt; \\\n             --version &lt;chart-version&gt; \\\n             --values values.yaml\n</code></pre> <p>Values &amp; flags</p> <ul> <li><code>&lt;release-name&gt;</code> the name of your deployment. You can also use <code>--generate-name</code>.</li> <li> <p><code>&lt;product&gt;</code> the product to install. Options include: </p> <ul> <li><code>jira</code> </li> <li><code>confluence</code></li> <li><code>bitbucket</code></li> <li><code>bamboo</code></li> <li><code>bamboo-agent</code></li> <li><code>crowd</code></li> </ul> </li> <li> <p><code>&lt;namespace&gt;</code> optional flag for categorizing installed resources.</p> </li> <li><code>&lt;chart-version&gt;</code> optional flag for defining the chart version to be used. If omitted, the latest version of the chart will be used.</li> <li><code>values.yaml</code> optional flag for defining your site-specific configuration information. If omitted, the chart config default will be used.</li> <li>Add <code>--wait</code> if you wish the installation command to block until all of the deployed Kubernetes resources are ready, but be aware that this may wait for several minutes if anything is mis-configured.</li> </ul> <p>Elasticsearch for Bitbucket</p> <p>We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations.    </p>"},{"location":"userguide/INSTALLATION/#9-test-your-deployed-product","title":"9. Test your deployed product","text":"<p>Make sure the service pod/s are running, then test your deployed product:</p> <pre><code>helm test &lt;release-name&gt; --logs --namespace &lt;namespace&gt;\n</code></pre> <ul> <li>This will run some basic smoke tests against the deployed release.</li> <li>If any of these tests fail, it is likely that the deployment was not successful. Check the status of the deployed resources for any obvious errors that may have caused the failure.</li> </ul>"},{"location":"userguide/INSTALLATION/#10-complete-product-setup","title":"10. Complete product setup","text":"<p>Using the service URL provided by Helm post install, open your product in a web browser and complete the setup via the setup wizard. </p>"},{"location":"userguide/INSTALLATION/#11-additional-deployments","title":"11. Additional deployments","text":"<p>Bamboo agents and Bitbucket mirrors can also be deployed via their dedicated charts:</p> Bamboo agentBitbucket mirror <p>Bamboo agent installation</p> <p>Instructions for deploying Bamboo agents</p> <p>Bitbucket mirror installation</p> <p>Instructions for deploying Bitbucket mirror's</p>"},{"location":"userguide/INSTALLATION/#uninstall","title":"Uninstall","text":"<p>The deployment and all of its associated resources can be un-installed with the following command: <pre><code>helm uninstall &lt;release-name&gt; atlassian-data-center/&lt;product&gt;\n</code></pre></p>"},{"location":"userguide/MIGRATION/","title":"Migration","text":"<p>If you already have an existing Data Center product deployment, you can migrate it to a Kubernetes cluster using the Data Center Helm charts. </p> <p>You will need to migrate your database and your shared home (including local home for Bamboo), then all you need to do is to follow the Installation guide, using your migrated resources instead of provisioning new ones.</p>"},{"location":"userguide/MIGRATION/#migrating-your-database","title":"Migrating your database","text":"<p>To migrate your database, you should point the Helm charts to the existing database or to a migrated version of the database. Do this by updating the <code>database</code> stanza in the <code>values.yaml</code> file as explained in the Configure database step in the installation guide.</p>"},{"location":"userguide/MIGRATION/#migrating-your-shared-home","title":"Migrating your shared home","text":"<p>Application nodes should have access to a shared directory in the same path. Examples of what the shared file system stores include plugins, shared caches, repositories, attachments, and avatars. Configure your shared home by updating the <code>sharedHome</code> stanza in the <code>values.yaml</code> file as explained in the Configure persistent storage step in the installation guide.</p>"},{"location":"userguide/MIGRATION/#migrating-bamboo-server-local-home","title":"Migrating Bamboo server local home","text":"<p>Bamboo DC stores pertinent config data in local home, namely <code>bamboo.cfg.xml</code>. Care should be taken to include this data when migrating Bamboo DC deployments.</p>"},{"location":"userguide/MIGRATION/#helpful-links","title":"Helpful links","text":"<ul> <li>Atlassian Data Center migration plan\u00a0- gives\u00a0some guidance on overall process, organizational preparedness, estimated time frames, and app\u00a0compatibility.\u00a0</li> <li>Atlassian Data Center migration checklist\u00a0- also provides useful tests and checks to perform throughout the moving process.</li> <li>Migrating to another database - describes how to migrate your data from your existing database to another database:<ul> <li>Migrating Confluence to another database</li> <li>Migrating Jira to another database </li> <li>Migrating Bamboo to another database </li> </ul> </li> </ul> <p>Availability Zone proximity</p> <p>For better performance consider co-locating your migrated database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads.</p>"},{"location":"userguide/OPERATION/","title":"Operation","text":"<p>Once you have installed your product, use this document if you want to scale your product, update your product, or see what examples we have.</p>"},{"location":"userguide/OPERATION/#expose-jmx-metrics","title":"Expose JMX Metrics","text":"<p>When <code>monitoring.exposeJmxMetrics</code> is enabled, JMX exporter runs as javaagent to expose http server and serve metrics of a local JVM. In other words, JMX MBeans (if enabled in the product) are exposed and available to be scraped by Prometheus.</p> <p>JMX exporter jar isn't available in products dependencies or container images, that is why when <code>monitoring.exposeJmxMetrics</code> is enabled there are 2 ways to get it:</p> <ul> <li>copy from an init container (bitnamilegacy/jmx-exporter DockerHub image) - the default option in values.yaml which works out of the box</li> <li>manually download from JMX exporter GitHub releases page and copy to shared-home or mount as a secret</li> </ul> Init ContainerManually Copy the Jar <p>Run <code>helm upgrade</code> to apply changes. Once done, you can verify metrics is available by running:</p> <pre><code>kubectl port-forward confluence-0 9999:9999 -n atlassian \n</code></pre> <p>Go to <code>http://localhost:9999/metrics</code> in your local browser to verify metrics availability.</p> <p>JMX service security</p> <p>By default, JMX services are created as ClusterIP types, i.e. they are not available outside the Kubernetes cluster. Because the metrics endpoint isn't password protected, make sure you protect it using your cloud provider's network security features (e.g., AWS SecurityGroups, GCP Cloud Firewall, Azure Network Security Groups) when exposing it as a LoadBalancer if required: <pre><code>monitoring:\n  jmxExporterPortType: LoadBalancer\n</code></pre></p>"},{"location":"userguide/OPERATION/#enable-jmx-monitoring-the-default-method","title":"Enable JMX Monitoring: The Default Method","text":"<p>You can expose JMX metrics by either passing <code>--set monitoring.exposeJmxMetrics=true</code> to your <code>helm install/upgrade command</code> or change the default value in your <code>values.yaml</code> file:</p> <pre><code>monitoring:\n  exposeJmxMetrics: true\n</code></pre>"},{"location":"userguide/OPERATION/#enable-jmx-monitoring-manually-copy-the-jar","title":"Enable JMX Monitoring: Manually Copy the Jar","text":"<p>If you don't want to have an additional container which will slightly increase pod startup time, you can manually download the exporter jar, override its location and disable an init container:</p> <p>First, download the jar:</p> <pre><code># check the latest version from https://github.com/prometheus/jmx_exporter/releases\nwget -P . https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.18.0/jmx_prometheus_javaagent-0.18.0.jar\n</code></pre> <p>Then, copy it to shared-home. The example below assumes that Confluence is deployed to <code>atlassian</code> namespace, and <code>volumes.sharedHome.mountPath</code> uses the default value:</p> <pre><code>kubectl cp \\\n  jmx_prometheus_javaagent-0.18.0.jar \\\n  atlassian/confluence-0:/var/atlassian/application-data/shared-home/jmx_prometheus_javaagent-0.18.0.jar\n</code></pre> <p>Disable an init container and set a custom jar location in the <code>values.yaml</code> file:</p> <pre><code>monitoring:\n  exposeJmxMetrics: true\n  fetchJmxExporterJar: false\n  jmxExporterCustomJarLocation: /var/atlassian/application-data/shared-home/jmx_prometheus_javaagent-0.18.0.jar\n</code></pre>"},{"location":"userguide/OPERATION/#scrape-metrics-with-prometheus","title":"Scrape Metrics With Prometheus","text":"<p>See: Prometheus Monitoring</p>"},{"location":"userguide/OPERATION/#managing-resources","title":"Managing resources","text":"<p>You can scale your application by adding additonal pods or by managing available resources with requests and limits.</p>"},{"location":"userguide/OPERATION/#upgrading-application","title":"Upgrading application","text":""},{"location":"userguide/OPERATION/#kubernetes-update-strategies","title":"Kubernetes update strategies","text":"<p>Kubernetes provides two strategies to update applications managed by <code>statefulset</code> controllers:</p>"},{"location":"userguide/OPERATION/#rolling-update","title":"Rolling update","text":"<p>The pods will be upgraded one by one until all pods run containers with the updated template. The upgrade is managed by  Kubernetes and the user has limited control during the upgrade process, after having modified the template. This is the default  upgrade strategy in Kubernetes. </p> <p>To perform a canary or multi-phase upgrade, a partition can be defined on the cluster and Kubernetes will upgrade just  the nodes in that partition. </p> <p>The default implementation is based on RollingUpdate strategy with no partition defined. </p>"},{"location":"userguide/OPERATION/#ondelete-strategy","title":"OnDelete strategy","text":"<p>In this strategy users select the pod to upgrade by deleting it, and Kubernetes will replace it by creating a new pod  based on the updated template. To select this strategy the following should be replaced with the current   implementation of <code>updateStrategy</code> in the <code>statefulset</code> spec:</p> <pre><code>  updateStrategy:\n    type: OnDelete\n</code></pre>"},{"location":"userguide/OPERATION/#upgrade","title":"Upgrade","text":"<ul> <li>To learn about upgrading the Helm charts see Helm chart upgrade.  </li> <li>To learn about upgrading the products without upgrading the Helm charts see Products upgrade.</li> </ul>"},{"location":"userguide/OPERATION/#examples","title":"Examples","text":""},{"location":"userguide/OPERATION/#logging","title":"Logging","text":""},{"location":"userguide/OPERATION/#how-to-deploy-an-efk-stack-to-kubernetes","title":"How to deploy an EFK stack to Kubernetes","text":"<p>There are different methods to deploy an EFK stack. We provide two deployment methods, the first is deploying EFK locally on Kubernetes, and the second is using managed Elasticsearch outside the Kubernetes cluster. Please refer to Logging in Kubernetes.</p>"},{"location":"userguide/PREREQUISITES/","title":"Prerequisites","text":""},{"location":"userguide/PREREQUISITES/#requirements","title":"Requirements","text":"<p>In order to deploy Atlassian\u2019s Data Center products, the following is required:</p> <ol> <li>An understanding of Kubernetes and Helm concepts.</li> <li><code>kubectl</code> <code>v1.21</code> or later, must be compatible with your cluster.</li> <li><code>helm</code> <code>v3.3</code> or later.</li> </ol>"},{"location":"userguide/PREREQUISITES/#environment-setup","title":"Environment setup","text":"<p>Before installing the Data Center Helm charts you need to set up your environment:</p> <ol> <li>Create and connect to the Kubernetes cluster</li> <li>Provision an Ingress Controller</li> <li>Provision a database</li> <li>Configure a shared-home volume</li> <li>Configure a local-home volume</li> </ol> <p>Elasticsearch for Bitbucket</p> <p>We highly recommend you use an external Elasticsearch installation for Bitbucket. When you run more than one node you need to have a separate Elasticsearch cluster to enable code search. See Bitbucket Elasticsearch recommendations.</p>"},{"location":"userguide/PREREQUISITES/#create-and-connect-to-the-kubernetes-cluster","title":"Create and connect to the Kubernetes cluster","text":"<ul> <li>In order to install the charts to your Kubernetes cluster (version 1.21+), your Kubernetes client config must be configured appropriately, and you must have the necessary permissions.</li> <li>It is up to you to set up security policies.</li> </ul> <p>See examples of provisioning Kubernetes clusters on cloud-based providers.</p>"},{"location":"userguide/PREREQUISITES/#provision-an-ingress-controller","title":"Provision an Ingress Controller","text":"<ul> <li>This step is necessary in order to make your Atlassian product available from outside of the Kubernetes cluster after deployment.</li> <li>The Kubernetes project supports and maintains ingress controllers for the major cloud providers including; AWS, GCE and nginx. There are also a number of open-source third-party projects available.</li> <li>Because different Kubernetes clusters use different ingress configurations/controllers, the Helm charts provide\u00a0Ingress Object\u00a0templates only.</li> <li>The Ingress resource provided as part of the Helm charts is geared toward the\u00a0NGINX Ingress Controller\u00a0and can be configured via the\u00a0<code>ingress</code>\u00a0stanza in the appropriate\u00a0<code>values.yaml</code> (an alternative controller can be used).</li> <li>For more information about the Ingress controller go to the Ingress section of the configuration guide.</li> </ul> <p>See an example of provisioning an NGINX Ingress Controller.</p>"},{"location":"userguide/PREREQUISITES/#provision-a-database","title":"Provision a database","text":"<ul> <li>Must be of a type and version supported by the Data Center product you wish to install:</li> </ul> JiraConfluenceBitbucketBambooCrowd <p>Supported databases</p> <p>Supported databases</p> <p>Supported databases</p> <p>Supported databases</p> <p>Supported databases</p> <ul> <li>Must be reachable from the product deployed within your Kubernetes cluster.</li> <li>The database service may be deployed within the same Kubernetes cluster as the Data Center product or elsewhere.</li> <li>The products need to be provided with the information they need to connect to the database service. Configuration for each product is mostly the same, with some small differences. For more information go to the Database connectivity section of the configuration guide.</li> </ul> <p>Reducing pod to database latency</p> <p>For better performance consider co-locating your database in the same Availability Zone (AZ) as your product nodes. Database-heavy operations, such as full re-index, become significantly faster when the database is collocated with the Data Center node in the same AZ. However we don't recommend this if you're running critical workloads.</p> <p>See an example of provisioning databases on cloud-based providers.</p>"},{"location":"userguide/PREREQUISITES/#configure-a-shared-home-volume","title":"Configure a shared-home volume","text":"<ul> <li>All of the Data Center products require a shared network filesystem if they are to be operated in multi-node clusters. If no shared filesystem is available, the products can only be operated in single-node configuration.</li> <li>Some cloud based options for a shared filesystem include Google Filestore, AWS EFS, Azure Files. You can also stand up your own NFS.</li> </ul> <p>Bitbucket shared storage</p> <p>Due to the high performance requirements on IO operations, it is critical that you adhere to the requirements in Bitbucket Supported platforms.</p> <ul> <li>The logical representation of the chosen storage type within Kubernetes can be defined as <code>PersistentVolumes</code> with an associated <code>PersistentVolumeClaims</code> in a <code>ReadWriteMany (RWX)</code> access mode.</li> <li>For more information about volumes see the Volumes section of the configuration guide.</li> </ul> <p>See examples of creating shared storage.</p>"},{"location":"userguide/PREREQUISITES/#configure-local-home-volume","title":"Configure local-home volume","text":"<ul> <li>As with the shared-home, each pod requires its own volume for <code>local-home</code>. Each product needs this for defining operational data.</li> <li>If not defined, an emptyDir will be utilised.</li> <li>Although an <code>emptyDir</code> may be acceptable for evaluation purposes, we recommend that each pod is allocated its own volume.</li> <li>A <code>local-home</code> volume could be logically represented within the cluster using a <code>StorageClass</code>. This will dynamically provision a persistent block volume (e.g., AWS EBS, GCP Persistent Disks, Azure Managed Disks) to each pod depending on your cloud provider.</li> </ul> <p>An example of this strategy can be found the local storage example.</p>"},{"location":"userguide/VERIFICATION/","title":"Verification","text":"<p>From release 1.11.0, all the Helm charts are signed with a GPG key, following the instructions on the official Helm documentation. </p> <p>To verify the integrity of the charts,  1. Download chart <code>.tgz</code> file, <code>.prov</code> file and <code>helm_key.pub</code> from release assets, </p> <ol> <li> <p>Import the public key into your local GPG keyring. (Install GnuPG tool if you haven't done so already.) </p> <pre><code>gpg --import helm_key.pub \n</code></pre> </li> <li> <p>At present, Helm only supports the legacy gpg format so export the keyring into the legacy format:     <pre><code>gpg --export &gt;~/.gnupg/pubring.gpg\n</code></pre></p> </li> <li> <p>Verify the chart.     <pre><code>helm verify /path/to/product.tgz \n</code></pre></p> </li> </ol> <p>If the verification is successful, the output would be something like:  <pre><code>helm verify ~/Downloads/jira-1.11.0.tgz                                                                         \nSigned by: Atlassian DC Deployments &lt;dc-deployments@atlassian.com&gt;\nUsing Key With Fingerprint: DD1A5B2F7A599129274FB10AD38C66448E19B403\nChart Hash Verified: sha256:ca102cbf416a5c87998d06ba4527b5afc99e1d7d1776317ddd07720251715fde\n</code></pre></p>"},{"location":"userguide/monitoring/PRE_CANNED_CHARTS/","title":"Pre-canned product charts","text":"<p>The instructions outlined on this page provide details on how you can deploy a set of Grafana dashboards specific to your provisioned DC product(s)</p> <p>Pre-requisites</p> <p>JMX metrics, Prometheus and Grafana will all need to be provisioned already to make use of this guide. For instructions on how to do this see, Monitoring with Prometheus</p>"},{"location":"userguide/monitoring/PRE_CANNED_CHARTS/#1-enable-pre-canned-dashboards","title":"1. Enable Pre-canned dashboards","text":"<p>Update the <code>grafana</code> stanza within the deployments <code>values.yaml</code> to enable dashboard creation with the appropriate labels i.e.</p> <pre><code>monitoring:\n  grafana:\n    createDashboards: true\n    dashboardLabels:\n      grafana_dashboard: \"1\"\n</code></pre> <p><code>grafana_dashboard</code> value</p> <p>The value to assign to the label <code>dashboardLabels.grafana_dashboard</code> can be obtained by examining <code>Environment</code> variables for the grafana pod sidecar <code>grafana-sc-dashboard</code>. For instance: <pre><code>kubectl describe pod prometheus-stack-grafana-57dc5589b-2wh98 -n &lt;prometheus-stack-namespace&gt; | grep -A 20 grafana-sc-dashboard | grep -A 5 Environment\n</code></pre> Note: The name of the Grafana pod (<code>prometheus-stack-grafana-57dc5589b-2wh98</code>) may vary slightly between deploys</p> <p>will return something like <pre><code>Environment:\n    METHOD:        WATCH\n    LABEL:         grafana_dashboard\n    LABEL_VALUE:   1\n    FOLDER:        /tmp/dashboards\n    RESOURCE:      both\n</code></pre> In this case we're interested in the value for the label <code>grafana_dashboard</code>, that is <code>1</code>. As such the <code>grafana</code> stanza, above` has been updated with this value. </p> <p>Now perform an upgrade using the updated <code>values.yaml</code>: <pre><code>helm upgrade confluence atlassian-data-center/confluence -f values.yaml --wait --namespace &lt;namespace&gt;\n</code></pre></p>"},{"location":"userguide/monitoring/PRE_CANNED_CHARTS/#2-dashboard-confirmation","title":"2. Dashboard confirmation","text":"<p>Confirm the dashboards are provisioned and working by running (replace pod name with an actual Grafana pod name):</p> <pre><code>kubectl port-forward prometheus-stack-grafana-57dc5589b-2wh98 3000:3000 -n &lt;prometheus-stack-namespace&gt;\n</code></pre> <p>Now go to <code>http://localhost:3000</code> in your local browser. Once logged into Grafana (The default credentials are <code>admin:prom-operator</code>) select Dashboards &gt; Filter by tag &gt; Product name. You should be presented with a list of product specific dashboards that can be viewed, for instance</p> <p></p>"},{"location":"userguide/monitoring/PROMETHEUS/","title":"Monitoring with Prometheus","text":"<p>The instructions outlined on this page provide details on how you can enable Prometheus monitoring on your stack with Grafana</p>"},{"location":"userguide/monitoring/PROMETHEUS/#1-install-prometheus-stack","title":"1. Install Prometheus stack","text":"<p>Note</p> <ul> <li>This approach will also install Grafana</li> <li>For the purposes of this guide the Prometheus stack will be installed with the release name <code>prometheus-stack</code>.</li> </ul> <p>Install kube-prometheus-stack Helm chart</p> <p>Fetch the repo and perform an update <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n</code></pre> Now install the Prometheus stack <pre><code>helm install prometheus-stack prometheus-community/kube-prometheus-stack\n</code></pre></p> <p>Persist Prometheus &amp; Grafana data</p> <p>By default, the Prometheus stack configures Pods to store data using an <code>emptyDir</code> volume, meaning data is not persisted when the Pods are redeployed/restarted. To maintain state, persistent storage for Prometheus and Grafana can be enabled. This can be done by updating the <code>prometheus-stack</code> with the following <code>yaml</code>:</p> Maintain chart and metric state<pre><code>grafana:\n  persistence:\n    enabled: true\n    size: 10Gi\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: 1\nprometheus:\n  prometheusSpec:\n    storageSpec:\n      volumeClaimTemplate:\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 10Gi\n</code></pre> <p>This <code>yaml</code> (added to a file called <code>prometheus-persistence.yaml</code>) can then be used to upgrade the <code>prometheus-stack</code></p> <pre><code>helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack -f prometheus-persistence.yaml --wait --namespace &lt;prometheus-stack-namespace&gt; \n</code></pre>"},{"location":"userguide/monitoring/PROMETHEUS/#2-expose-jmx-metrics","title":"2. Expose JMX metrics","text":"<p>Follow these instructions for details on how to enable and expose <code>JMX</code> for your product via a dedicated <code>Service</code>. </p>"},{"location":"userguide/monitoring/PROMETHEUS/#3-create-a-servicemonitor","title":"3. Create a ServiceMonitor","text":"<p>Now that <code>JMX</code> metrics are exposed, we need a way of scraping them. This will be done using the Prometheus custom resource definition; ServiceMonitor. There are two ways this <code>ServiceMonitor</code> can be provisioned:</p> <ol> <li>Automatically - using <code>helm upgrade</code></li> <li>Manually - deploy a new <code>serviceMonitor</code> CRD</li> </ol> AutomaticallyManually <p>Update the <code>serviceMonitor</code> stanza within the deployments <code>values.yaml</code> and perform a <code>helm upgrade</code>.</p> Automated deployment<pre><code>serviceMonitor:\n  create: true\n  prometheusLabelSelector:\n    release: prometheus-stack\n  scrapeIntervalSeconds: 30\n</code></pre> <p><code>prometheusLabelSelector</code> identification</p> <p>The <code>prometheusLabelSelector.release</code> value will be the release name used for provisioning Prometheus stack in 1. Install Prometheus stack. It can also be identified using the following: <pre><code>kubectl get prometheus/prometheus-stack-kube-prom-prometheus -n &lt;prometheus-stack-namespace&gt; -o jsonpath={'.spec.serviceMonitorSelector.matchLabels'}\n</code></pre></p> <p>Now perform an upgrade using the updated <code>values.yaml</code>: <pre><code>helm upgrade confluence atlassian-data-center/confluence -f values.yaml --wait --namespace &lt;dc-product-namespace&gt;\n</code></pre></p> <p>Alternatively you can manually provision the <code>serviceMontitor</code> by creating a new file called <code>serviceMonitor.yaml</code> with the following:</p> Manual deployment<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: confluence\n  labels:\n    release: prometheus-stack\nspec:\n  endpoints:\n  - interval: 15s\n    path: /metrics\n    port: jmx\n    scheme: http\n  namespaceSelector:\n    any: true\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: confluence\n</code></pre> <p>Particular values to be aware of</p> <p>The above example assumes that:</p> <ul> <li>the DC Helm release name is <code>confluence</code></li> <li><code>labels.release</code> is <code>prometheus-stack</code> (release name used for provisioning Prometheus stack in 1. Install Prometheus stack.</li> </ul> <p>Now provision the <code>serviceMonitor</code>: <pre><code>kubectl apply -f servicemonitor.yaml\n</code></pre></p>"},{"location":"userguide/monitoring/PROMETHEUS/#4-expose-confirm-prometheus-service","title":"4. Expose &amp; confirm Prometheus service","text":"<p>Out of the box Prometheus services are not exposed, the simplest way to do this is to forward the service port (replace pod name with an actual Prometheus pod name): <pre><code>kubectl port-forward prometheus-prometheus-stack-kube-prom-prometheus-0 9090:9090 -n &lt;prometheus-stack-namespace&gt;\n</code></pre></p> <p>Navigate to the URL <code>http://localhost:9090</code> in your browser and then select; Status -&gt; Targets. You should be able to see your product pods as targets.</p>"},{"location":"userguide/monitoring/PROMETHEUS/#5-access-grafana","title":"5. Access Grafana","text":"<p>To access Grafana, run (replace pod name with an actual Grafana pod name):</p> <pre><code>kubectl port-forward prometheus-stack-grafana-57dc5589b-2wh98 3000:3000 -n &lt;prometheus-stack-namespace&gt;\n</code></pre> <p>Grafana details</p> <ul> <li>The name of the Grafana pod (<code>prometheus-stack-grafana-57dc5589b-2wh98</code>) may vary slightly between deploys</li> <li>The default credentials are <code>admin:prom-operator</code> (these can be overridden when deploying kube-prometheus-stack). Alternatively, you may expose grafana service as a <code>LoadBalancer</code> <code>Service</code> type.</li> </ul> <p>and go to <code>http://localhost:3000</code> in your browser.</p> <p>You can then create a new Dashboard and use any of the exported metrics. To get the list of available metrics, run:</p> <pre><code>kubectl port-forward confluence-0 9999:9999 -n &lt;dc-product-namespace&gt; \n</code></pre> <p>and go to <code>http://localhost:9999/metrics</code> in your local browser. You should see a list of available metrics that can be used.</p>"},{"location":"userguide/monitoring/PROMETHEUS/#6-provision-pre-canned-product-dashboards","title":"6. Provision Pre-canned product dashboards","text":"<p>We provide a set of Grafana specific dashboards for each DC product. These can be provisioned by following the Pre-canned product charts guide.</p>"},{"location":"userguide/monitoring/PROMETHEUS/#existing-standalone-prometheus-instance","title":"Existing Standalone Prometheus Instance","text":"<p>JMX service security</p> <p>By default, JMX services are created as ClusterIP types, i.e. they are not available outside the Kubernetes cluster. If your Prometheus instance, is deployed outside the Kubernetes cluster, you will need to expose the <code>JMX</code> service: <pre><code>monitoring:\n  jmxExporterPortType: LoadBalancer\n</code></pre> Make sure you restrict access to the Prometheus endpoint to only authorized CIDR ranges because JMX endpoints are not password protected. Configure your load balancer's network security rules to limit inbound access. </p> <p>See: jmx_exporter does not support authentication to the HTTP  endpoint</p> <p>If you use a standalone Prometheus in Kubernetes, you need to manually create scrape configuration. See: Monitor Jira with Prometheus and Grafana.</p>"},{"location":"userguide/resource_management/JIRA_INDEX_SNAPSHOT/","title":"Creating an initial index snapshot in Jira","text":"<p>These steps should be followed to enable shared index snapshots with Jira:</p> <ol> <li>Log into the Jira instance as the Administrator</li> <li>Go to <code>Settings</code> -&gt; <code>System</code> -&gt; <code>Indexing</code></li> <li>There should be no errors on this page i.e. </li> <li>If there are errors (as seen below) perform a <code>Full re-index</code> before proceeding </li> <li>Once the <code>Full re-index</code> is complete, scroll down to <code>Index Recovery</code> settings visible on the same page </li> <li>Take note of the current settings</li> <li>Temporarily change these values (<code>Edit Settings</code>), as depicted in the screenshot below. The cron expression will create an index snapshot every minute </li> <li>Wait for the snapshot to be created, by checking for an archive in <code>&lt;shared-home&gt;/export/indexsnapshots</code></li> <li>When the snapshot is available, revert the settings noted in <code>step 6</code>, or back to the defaults: </li> <li>Consider keeping the <code>Enable index recovery</code> setting so that it is set to <code>ON</code></li> <li>Proceed with scaling the cluster as necessary</li> </ol>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/","title":"Resource requests and limits","text":"<p>To ensure that Kubernetes appropriately schedules resources, the respective product <code>values.yaml</code> is configured with default <code>cpu</code> and <code>memory</code> resource request values .</p>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#resource-requests","title":"Resource requests","text":"<p>The default resource requests that are used for each product are defined below. </p> <p>Note: these values are geared towards small data sets. For larger enterprise deployments refer to the Data Center infrastructure recommendations.</p> <p>Using the formula below, the <code>memory</code> specific values are derived from the default <code>JVM</code> requirements defined for each product's Docker container.</p> Product CPU Memory Jira <code>2</code> <code>2G</code> Confluence <code>2</code> <code>2G</code> Bitbucket <code>2</code> <code>2G</code> Bamboo <code>2</code> <code>2G</code> Bamboo agent <code>1</code> <code>2G</code> Crowd <code>2</code> <code>1G</code>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#synchrony-container-resources","title":"Synchrony container resources","text":"<p>A Confluence cluster requires a Synchrony container. The default resource requests that are used for the Synchrony container are:</p> <ul> <li><code>cpu: 2</code></li> <li><code>memory: 2.5G</code> </li> </ul>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#memory-request-sizing","title":"Memory request sizing","text":"<p>Request sizing must allow for the size of the product <code>JVM</code>. That means the <code>maximum heap size</code>, <code>minumum heap size</code> and the <code>reserved code cache size</code> (if applicable) plus other JVM overheads, must be considered when defining the request <code>memory</code> size. As a rule of thumb the formula below can be used to deduce the appropriate request memory size. <pre><code>(maxHeap + codeCache) * 1.5\n</code></pre></p>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#resource-limits","title":"Resource limits","text":"<p>Environmental and hardware constraints are different for each deployment, therefore the product's <code>values.yaml</code> does not provide a resource <code>limit</code> definition. Resource usage limits can be defined by updating the commented out the <code>resources.container.limits</code> stanza within the appropriate <code>values.yaml</code>. For example:</p> <pre><code>container:\n  limits:\n    cpu: \"2\"\n    memory: \"4G\"\n  requests:\n    cpu: \"2\" \n    memory: \"2G\"\n</code></pre>"},{"location":"userguide/resource_management/REQUESTS_AND_LIMITS/#init-and-additional-containers","title":"Init and Additional Containers","text":"<p>By default, NFS permission fixer and JMX init containers, as well as Fluentd additional container don't have resources requests and limits. You can set them in <code>values.yaml</code>:</p> <pre><code>volumes:\n  sharedHome:\n    nfsPermissionFixer:\n      resources:\n        requests:\n          cpu: 1\n          memory: 256Mi\n        limits:\n          cpu: 1\n          memory: 256Mi\n\nmonitoring:\n  jmxExporterInitContainer:\n    resources:\n      requests:\n        cpu: 1\n        memory: 256Mi\n      limits:\n        cpu: 1\n        memory: 256Mi  \n\nfluentd:\n  resources:\n    requests:\n      cpu: 1\n      memory: 256Mi\n    limits:\n      cpu: 1\n      memory: 256Mi\n</code></pre>"},{"location":"userguide/resource_management/RESOURCE_SCALING/","title":"Product scaling","text":"<p>For optimum performance and stability the appropriate resource <code>requests</code> and <code>limits</code> should be defined for each pod. The number of pods in the product cluster should also be carefully considered. Kubernetes provides means for horizontal and vertical scaling of the deployed pods within a cluster, these approaches are described below.</p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#horizontal-scaling-adding-pods","title":"Horizontal scaling - adding pods","text":"<p>The Helm charts provision one <code>StatefulSet</code> by default. The number of replicas within this <code>StatefulSet</code> can be altered either declaratively or imperatively. Note that the Ingress must support cookie-based session affinity in order for the products to work correctly in a multi-node configuration.</p> DeclarativelyImperatively <ol> <li>Update <code>values.yaml</code> by modifying the <code>replicaCount</code> appropriately.</li> <li>Apply the patch:   <pre><code>helm upgrade &lt;release&gt; &lt;chart&gt; -f &lt;values file&gt;\n</code></pre></li> </ol> <pre><code>kubectl scale statefulsets &lt;statefulsetset-name&gt; --replicas=n\n</code></pre> <p>Initial cluster size</p> <p>Jira, Confluence, and Crowd all require manual configuration after the first pod is deployed and before scaling up to additional pods, therefore when you deploy the product only one pod (replica) is created. The initial number of pods that should be started at deployment of each product is set in the <code>replicaCount</code> variable found in the values.yaml and should always be kept as 1.</p> <p>Bamboo cluster size</p> <p>Bamboo server currently has limitations relating to clustering, as such, unlike the other products Bamboo server can only be scaled to a maximum of <code>1</code> pod.</p> <p>For details on modifying the <code>cpu</code> and <code>memory</code> requirements of the <code>StatefulSet</code> see section Vertical Scaling below. Additional details on the resource requests and limits used by the <code>StatfulSet</code> can be found in Resource requests and limits.</p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#scaling-jira-safely","title":"Scaling Jira safely","text":"<p>At present there are issues relating to index replication with Jira when immediately scaling up by more than 1 pod at a time. See Jira and horizontal scaling.</p> <p>Before scaling your cluster</p> <p>Make sure there's at least one snapshot file in the <code>&lt;shared-home&gt;/export/indexsnapshots</code> directory. New pods will attempt to use the files in this directory to replicate the index. If there is no snapshot present in  <code>&lt;shared-home&gt;/export/indexsnapshots</code> then create an initial index snapshot</p> <p>Having followed the steps above, and ensured a healthy snapshot index is available, scale the cluster as necessary. Once scaling is complete confirm that the index is still healthy using the approach prescribed in Step 3. If there are still indexing issues then please refer to the guides below for details on how address them:</p> <ul> <li>Unable to perform a background re-index error</li> <li>Troubleshoot index problems in Jira server</li> </ul>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#vertical-scaling-adding-resources","title":"Vertical scaling - adding resources","text":"<p>The resource <code>requests</code> and <code>limits</code> for a <code>StatefulSet</code> can be defined before product deployment or for deployments that are already running within the Kubernetes cluster. Take note that vertical scaling will result in the pod being re-created with the updated values.</p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#prior-to-deployment","title":"Prior to deployment","text":"<p>Before performing a helm install update the appropriate products <code>values.yaml</code> <code>container</code> stanza with the desired <code>requests</code> and <code>limits</code> values i.e.  <pre><code> container: \n  limits:\n    cpu: \"4\"\n    memory: \"4G\"\n  requests:\n    cpu: \"2\"\n    memory: \"2G\"\n</code></pre></p>"},{"location":"userguide/resource_management/RESOURCE_SCALING/#post-deployment","title":"Post deployment","text":"<p>For existing deployments the <code>requests</code> and <code>limits</code> values can be dynamically updated either declaratively or imperatively </p> DeclarativelyImperatively <p>This the preferred approach as it keeps the state of the cluster, and the helm charts themselves in sync.</p> <ol> <li>Update <code>values.yaml</code> appropriately</li> <li>Apply the patch:</li> </ol> <pre><code>helm upgrade &lt;release&gt; &lt;chart&gt; -f &lt;values file&gt;\n</code></pre> <p>Using <code>kubectl edit</code> on the appropriate <code>StatefulSet</code> the respective <code>cpu</code> and <code>memory</code> values can be modified i.e.</p> <pre><code>resources:\n  requests:\n    cpu: \"2\"\n    memory: 2G\n</code></pre> <p>Saving the changes will then result in the existing product pod(s) being re-provisioned with the updated values.</p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/","title":"Helm chart upgrade","text":"<p>Each Helm chart has a default product version that might change in next Helm chart  version. So be aware that if you upgrade the Helm chart, it might lead to upgrading the product as well. This depends on the current and target Helm chart versions. </p> <p>Do you want to upgrade the product to a new version?</p> <p>If you want to upgrade the product version without upgrading the Helm chart then   refer to Product upgrades. </p> <p>Before upgrading the Helm chart, first consider: </p> <ul> <li>the version of the current Helm chart</li> <li>the version of the product running on your Kubernetes cluster</li> <li>the target version of the Helm chart you want to upgrade to</li> <li>the target version of the product you want to upgrade to</li> </ul> <p>You need to know if the target product version is zero-downtime compatible (if it is subject to change).  Based on this information you may need to choose a different upgrade method.</p> <p>Upgrade product strategies</p> <p>There are two options for upgrade:</p> <ul> <li>Normal upgrade: The service will have interruptions during the upgrade.</li> <li>Rolling upgrade: The upgrade will proceed with zero downtime.</li> </ul>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#1-find-the-current-version-of-the-installed-helm-chart","title":"1. Find the current version of the installed Helm chart","text":"<p>To find the current version of Helm chart and the product version run the following command:</p> <pre><code>helm list --namespace &lt;namespace&gt; \n</code></pre> <p>You can see the current Helm chart tag version in the <code>CHART</code> column, and the current product tag version in the <code>APP VERSION</code>  column for each release name. </p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#2-define-the-target-helm-chart-version","title":"2. Define the target Helm chart version","text":"<p>Do you have the Atlassian Helm chart repository locally?</p> <p>Make sure you have the Atlassian Helm chart repository in your local Helm repositories. Run the following command to add them:</p> <pre><code>helm repo add atlassian-data-center \\\n     https://atlassian.github.io/data-center-helm-charts\n</code></pre> <p>Update the Helm chart repository on your local Helm installation:</p> <pre><code>helm repo update\n</code></pre> <p>Target Helm chart version</p> <p>The target Helm chart version must be higher than the current Helm chart version.</p> <p>To see all available Helm chart versions of the specific product run this command:</p> <pre><code>helm search repo atlassian-data-center/&lt;product&gt; --versions\n</code></pre> <p>Select the target Helm chart version. You can find the default application version (target product version tag)   in the <code>APP VERSION</code> column.</p> <p>Upgrading the Helm chart to a MAJOR version is not backward compatible.</p> <p>The Helm chart is semantically versioned. You need to take some extra  steps if you are upgrading the Helm chart to a MAJOR version. Before you proceed, learn about the steps for your   target version in the upgrading section. </p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#3-define-the-upgrade-method","title":"3. Define the upgrade method","text":"<p>Considering the current and target product versions there are different scenarios: </p> <ol> <li>The versions are different, and the target product version is not zero-downtime compatible.  </li> <li>The versions are different, and the target product version is zero-downtime compatible.  </li> <li>The versions are the same.</li> </ol> <p>See the following links to find out if two versions of a product are zero-downtime compatible</p> <ul> <li>Jira: Upgrading Jira with zero downtime </li> <li>Confluence: Upgrading Confluence with zero downtime </li> <li>Bitbucket: Upgrading Bitbucket with zero downtime</li> <li>Bamboo: Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</li> <li>Crowd: Zero downtime upgrades for Crowd are currently not supported.</li> </ul> <p>All supported Jira versions are zero-downtime compatible</p> <p>The minimum supported version of Jira in the Data Center Helm Charts is <code>8.19</code>.   Considering any Jira version later than 8.x is zero-downtime compatible, all supported Jira Data Center versions   are zero-downtime compatible. </p> <p>Based on the scenario follow one of these options in the next step:</p> <ul> <li>Normal upgrade: Upgrade Helm chart when the target product version is not zero-downtime compatible, or you want  to avoid mixed version during the upgrade. In this option the product will have a downtime during the upgrade process. </li> <li>Rolling upgrade: Upgrade Helm chart when the target product version is zero-downtime compatible. This option  will only apply when the target product version is zero-downtime compatible. If you are not sure about this see the links above. </li> <li>No product upgrade: Upgrade the Helm chart with no change in product version. We recommend this method when the target product version is the same as the current product version, or for any other reason you may not want to change the product version but still upgrade the helm chart. </li> </ul>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#4-get-the-current-helm-release-values","title":"4. Get the current Helm release values","text":"<p>--reuse-values argument</p> <p>Do not pass <code>--reuse-values</code> to <code>helm upgrade</code> command because the default values in the new Helm chart version will be ignored which can result in templating errors and upgrade failures.</p> <p>If you have not saved the original values file used during the initial Helm chart installation, you can retrieve user-supplied values:</p> <pre><code>helm get values &lt;release-name&gt; -n &lt;namespace&gt; -o yaml &gt; values.yaml\n</code></pre> <p>Use this file in the argument to <code>helm upgrade</code> command, for example:</p> <pre><code>-f /home/user/values.yaml\n</code></pre>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#5-upgrade-the-helm-chart","title":"5. Upgrade the Helm chart","text":"<p>Tip: Monitor the pods during the upgrade process</p> <p>You can monitor the pod activities during the upgrade by running the following command in a separate terminal:   <pre><code>kubectl get pods --namespace &lt;namespace&gt; --watch\n</code></pre></p> Normal upgradeRolling upgradeUpgrade with no change in the product version"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-downtime","title":"Helm chart upgrade with downtime","text":"<p>You need to use this method to upgrade the Helm chart if:</p> <ul> <li>the target product version is not zero downtime-compatible</li> <li>for any other reason you would prefer to avoid running the cluster in mix mode</li> </ul> <p>Upgrading the Helm chart might change the product version</p> <p>If you want to upgrade the Helm chart to a newer version but don't want to change  the product version then follow the Upgrade with no change in product version tab.</p> <p>The strategy for upgrading the product with downtime is to scale down the cluster to zero nodes and then  start the nodes with new product versions. And finally scale the cluster up to the original number of nodes.  Here are step-by-step instructions for the upgrade process:</p> <ol> <li>Find out the number of nodes in the cluster.     <pre><code>kubectl describe sts &lt;release-name&gt; --namespace &lt;namespace&gt; | grep 'Replicas'\n</code></pre></li> <li> <p>Upgrade the Helm chart.     Replace the product name in the following command:     <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n --version &lt;target-helm-chart-version&gt; \\\n -f &lt;path-to-values-yaml&gt; \\\n --set replicaCount=1 \\\n --wait \\\n --namespace &lt;namespace&gt;\n</code></pre>     The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated      and join the cluster. </p> </li> <li> <p>Scale up the cluster.     After you confirm the new pod is in <code>Running</code> status then scale up the cluster to the same number      of nodes as before the upgrade:      <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/confluence \\\n -f &lt;path-to-values-yaml&gt; \\\n --set replicaCount=&lt;n&gt; \\\n --wait \\\n --namespace &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-zero-downtime","title":"Helm chart upgrade with zero downtime","text":"<p>Upgrade the Helm chart might change the product version</p> <p>If you want to upgrade the Helm chart to newer version but don't want to change  the product version then follow the Upgrade with no change in product version tab.</p> <p>Rolling upgrade is not possible if the cluster has only one node</p> <p>If you have just one node in the cluster then you can't take advantage of the zero-downtime approach. You may   scale up the cluster to at least two nodes before upgrading or there will be a downtime during the upgrade. </p> <p>In order to upgrade the Helm chart when the target product version is different from the current product version,  you can use upgrade with zero downtime to avoid any service interruption during the upgrade. To use this option the target version must be zero-downtime compatible. </p> <p>Make sure the product target version is zero downtime-compatible</p> <p>To ensure you will have a smooth upgrade make sure the product target version is zero-downtime   compatible. If you still aren't sure about this go back to step 3. </p> <p>Here are the step-by-step instructions of the upgrade process. These steps may vary for each product:</p> JiraConfluenceBitbucketBambooCrowd <ol> <li> <p>Put Jira into upgrade mode.      Go to Administration &gt; Applications &gt; Jira upgrades and click Put Jira into upgrade mode.      </p> </li> <li> <p>Run the upgrade using Helm. </p> <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/jira \\\n --version &lt;target-helm-chart-version&gt; \\\n -f &lt;path-to-values-yaml&gt; \\\n --wait \\\n --namespace &lt;namespace&gt;\n</code></pre> </li> <li> <p>Wait for the upgrade to finish.      The pods will be recreated with the updated version, one at a time.</p> <p></p> </li> <li> <p>Finalize the upgrade.     After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p> </li> </ol> <ol> <li> <p>Put Confluence into upgrade mode.       From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode:</p> <p></p> </li> <li> <p>Run the upgrade using Helm.       <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/confluence \\\n    --version &lt;target-helm-chart-version&gt; \\\n    -f &lt;path-to-values-yaml&gt; \\\n    --wait \\\n    --namespace &lt;namespace&gt;\n</code></pre>      Wait until all pods are recreated and are back to <code>Running</code> status. </p> </li> <li> <p>Wait for the upgrade to finish.       The pods will be recreated with the updated version, one at a time.</p> <p></p> </li> <li> <p>Finalize the upgrade.       After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p> </li> </ol> <ol> <li> <p>Put Bitbucket into upgrade mode.      From the admin page click on Rolling Upgrade and set the Bitbucket in Upgrade mode:</p> <p></p> </li> <li> <p>Run the upgrade using Helm. </p> <p><pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/bitbucket \\\n --version &lt;target-helm-chart-version&gt; \\\n -f &lt;path-to-values-yaml&gt; \\\n --wait \\\n --namespace &lt;namespace&gt;\n</code></pre> Wait until all pods are recreated and are back to <code>Running</code> status. </p> </li> <li> <p>Wait for the upgrade to finish.      The pods will be recreated with the updated version, one at a time.</p> <p></p> </li> <li> <p>Finalize the upgrade.      After all the pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p> </li> </ol> <p>Bamboo and zero downtime upgrades</p> <p>Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</p> <p>Crowd and zero downtime upgrades</p> <p>Zero downtime upgrades for Crowd are currently not supported.</p>"},{"location":"userguide/upgrades/HELM_CHART_UPGRADE/#helm-chart-upgrade-with-no-change-in-product-version","title":"Helm chart upgrade with no change in product version","text":"<p>If your target Helm chart has a different product version in comparison with the current product version, and you  still want to keep the current product version unchanged, you should use the following command to upgrade the Helm chart:</p> <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n    --version &lt;helm-chart-target-version&gt; \\\n    -f &lt;path-to-values-yaml&gt; \\\n    --set image.tag=&lt;current-product-tag&gt; \\\n    --wait \\\n    --namespace &lt;namespace&gt;\n</code></pre> <p>However, when the product versions of target and current Helm charts are the same, then you can run the following command to upgrade the Helm chart only:</p> <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n     --version &lt;helm-chart-target-version&gt; \\\n     -f &lt;path-to-values-yaml&gt; \\\n     --wait \\\n     --namespace &lt;namespace&gt;\n</code></pre>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/","title":"Products upgrade","text":"<p>We recommend upgrading the Helm chart rather than upgrading the product directly. However, if you want to upgrade the  product to a specific version that is not listed in the Helm charts, or if you don't want to upgrade Helm chart to a newer version but you still need to upgrade the product version, then you are in a right place.   </p> <p>To upgrade the product to a newer version without upgrading the Helm chart follow these steps:</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-the-tag-of-the-target-image","title":"1. Find the tag of the target image","text":"<p>Go to the Atlassian Docker Hub page of the relevant product to pick a tag that matches your target version.</p> <p>Atlassian Docker Hub page for supported products:</p> <ul> <li>Jira: atlassian/jira-software</li> <li>Confluence: atlassian/confluence</li> <li>Bitbucket: atlassian/bitbucket</li> <li>Bamboo: atlassian/bamboo</li> <li>Bamboo agent: atlassian/bamboo-agent-base</li> <li>Crowd: atlassian/crowd</li> </ul> <p>In the example you're running Jira using the <code>8.13.0-jdk11</code> tag, and you'll be upgrading to <code>8.13.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-define-the-upgrade-strategy","title":"2. Define the upgrade strategy","text":"<p>There are two strategies to upgrade the application:</p> <ul> <li>Normal upgrade: The service will have interruptions during the upgrade.</li> <li>Rolling upgrade: The upgrade will proceed with zero downtime.</li> </ul> <p>You can use rolling upgrade only if the target version is zero-downtime compatible. </p> <p>Can you use the rolling upgrade option?</p> <p>To confirm if you can run a rolling upgrade option, check your current and target product versions in the relevant link:  </p> <ul> <li>Jira: Upgrading Jira with zero downtime </li> <li>Confluence: Upgrading Confluence with zero downtime </li> <li>Bitbucket: Upgrading Bitbucket with zero downtime</li> <li>Bamboo: Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</li> </ul>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-upgrade-the-product","title":"3. Upgrade the product","text":"Normal UpgradeRolling upgrade"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#normal-upgrade","title":"Normal upgrade","text":"<p>The service will have interruptions during the normal upgrade</p> <p>You can use this method to upgrade the Helm chart if:</p> <ul> <li>The target product version is not zero-downtime compatible</li> <li>If you prefer to avoid running the cluster in mix mode</li> </ul> <p>The strategy for normal upgrading is to scale down the cluster to zero nodes, and then   start one node with the new product version. Then scale up the cluster to the original number of nodes.   Here are the step-by-step instructions for the upgrade process:</p> <ol> <li>Find out the current number of nodes.  Run the following command:   <pre><code>kubectl describe sts &lt;release-name&gt; --namespace &lt;namespace&gt; | grep 'Replicas'\n</code></pre></li> <li> <p>Run the upgrade using Helm.  Based on the product you want to upgrade replace the product name in the following command and run:  <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/&lt;product&gt; \\\n    --reuse-values \\\n    --set replicaCount=1 \\\n    --set image.tag=&lt;target-tag&gt; \\\n    --wait \\\n    --namespace &lt;namespace&gt;\n</code></pre>  The cluster will scale down to zero nodes. Then one pod with the target product version will be recreated   and join the cluster. </p> </li> <li> <p>Scale up the cluster.  After you confirm the new pod is in <code>Running</code> status then scale up the cluster to the same number   of nodes as before the upgrade:   <pre><code>helm upgrade &lt;release-name&gt; atlassian-data-center/confluence \\\n    --reuse-values \\\n    --set replicaCount=&lt;n&gt; \\\n    --wait \\\n    --namespace &lt;namespace&gt;\n</code></pre></p> </li> </ol>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#rolling-zero-downtime-upgrade","title":"Rolling (zero downtime) upgrade","text":"<p>Select the product tab to upgrade</p> <p>Upgrading the product with zero downtime is bit different for each product. Please select the product and follow  the steps to complete the rolling upgrade.  </p> JiraConfluenceBitbucketBambooCrowd <p>ATL_FORCE_CFG_UPDATE</p> <p>Make sure <code>additionalEnvironmentVariables.ATL_FORCE_CFG_UPDATE</code> is not excplicitly set to <code>true</code> in Helm values before a ZDU upgrade. Setting it to <code>true</code> and performing a ZDU upgrade will result in build numbers mismatch. </p> <p>Bamboo and zero downtime upgrades</p> <p>Zero downtime upgrades for Bamboo server and Bamboo agents are currently not supported.</p> <p>Crowd and zero downtime upgrades</p> <p>Zero downtime upgrades for Crowd are currently not supported.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#jira-rolling-upgrade","title":"Jira rolling upgrade","text":"<p>Let's say we have Jira version <code>8.19.0</code> deployed to our Kubernetes cluster, and we want to upgrade it to version <code>8.19.1</code>, which we'll call the target version. You can substitute the target version for the one you need, as long as it's newer than the current one.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-tag-of-the-target-image","title":"1. Find tag of the target image.","text":"<p>Go to atlassian/jira-software Docker Hub page to pick a tag that matches your target version.</p> <p>In the example we're running Jira using the <code>8.19.0-jdk11</code> tag, and we'll be upgrading to <code>8.19.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-jira-into-upgrade-mode","title":"2. Put Jira into upgrade mode.","text":"<p>Go to Administration &gt; Applications &gt; Jira upgrades and click Put Jira into upgrade mode.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm","title":"3. Run the upgrade using Helm.","text":"<p>Run the Helm upgrade command with your release name (<code>&lt;release-name&gt;</code>) and the target image from a previous step (<code>&lt;target-tag&gt;</code>). For more details, refer to the Helm documentation.</p> <pre><code>helm upgrade &lt;release-name&gt;  atlassian-data-center/jira \\ \n     --wait \\\n     --reuse-values \\\n     --set image.tag=&lt;target-tag&gt;\n</code></pre> <p>If you used <code>kubectl scale</code> after installing the Helm chart, you'll need to add <code>--set replicaCount=&lt;number-of-jira-nodes&gt;</code> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish","title":"4. Wait for the upgrade to finish.","text":"<p>The pods will be recreated with the updated version, one at a time.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade","title":"5. Finalize the upgrade.","text":"<p>After all pods are active with the new version, click Run upgrade tasks to finalize the upgrade:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#confluence-rolling-upgrade","title":"Confluence rolling upgrade","text":"<p>Let's say we have Confluence version <code>7.12.0</code> deployed to our Kubernetes cluster, and we want to upgrade it to version <code>7.12.1</code>, which we'll call the target version. You can substitute the target version for the one you need, as long as it's newer than the current one.</p> <p>Follow the link to confirm the target version is zero-downtime compatible</p> <pre><code>[Upgrading Confluence with zero downtime](https://confluence.atlassian.com/doc/upgrade-confluence-without-downtime-1027127923.html){.external}\n</code></pre>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-the-tag-of-the-target-image_1","title":"1. Find the tag of the target image.","text":"<p>Go to atlassian/confluence Docker Hub page to pick a tag that matches your target version.</p> <p>In the example we're running Confluence using the <code>7.12.0-jdk11</code> tag, and we'll be upgrading to <code>7.12.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-confluence-into-upgrade-mode","title":"2. Put Confluence into upgrade mode.","text":"<p>From the admin page click on Rolling Upgrade and set the Confluence in Upgrade mode:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm_1","title":"3. Run the upgrade using Helm.","text":"<p>Run the Helm upgrade command with your release name (<code>&lt;release-name&gt;</code>) and the target image from a previous step (<code>&lt;target-tag&gt;</code>). For more details, refer to the Helm documentation.</p> <pre><code>helm upgrade &lt;release-name&gt;  atlassian-data-center/confluence \\\n    --wait \\\n    --reuse-values \\ \n    --set image.tag=&lt;target-tag&gt;\n</code></pre> <p>If you used <code>kubectl scale</code> after installing the Helm chart, you'll need to add <code>--set replicaCount=&lt;number-of-confluence-nodes&gt;</code> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish_1","title":"4. Wait for the upgrade to finish.","text":"<p>The pods will be recreated with the updated version, one at a time.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade_1","title":"5. Finalize the upgrade.","text":"<p>After all the pods are activated with the new version, finalize the upgrade:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#bitbucket-rolling-upgrade","title":"Bitbucket rolling upgrade","text":"<p>Let's say we have Bitbucket version <code>7.12.0</code> deployed to our Kubernetes cluster, and we want to upgrade it to version <code>7.12.1</code>, which we'll call the target version. You can substitute the target version for the one you need, as long as it's newer than the current one.</p> <p>Follow the link to find out if the target version is zero-downtime compatible</p> <pre><code>[Upgrading Bitbucket with zero downtime](https://confluence.atlassian.com/bitbucketserver/upgrade-bitbucket-without-downtime-1038780379.html){.external}\n</code></pre>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#1-find-tag-of-the-target-image_1","title":"1. Find tag of the target image.","text":"<p>Go to atlassian/bitbucket Docker Hub page to pick a tag that matches your target version.</p> <p>In the example we're running Bitbucket using the <code>7.12.0-jdk11</code> tag, and we'll be upgrading to <code>7.12.1-jdk11</code> - our target.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#2-put-bitbucket-into-upgrade-mode","title":"2. Put Bitbucket into upgrade mode.","text":"<p>From the admin page click on Rolling Upgrade and set the Bitbucket to Upgrade mode:</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#3-run-the-upgrade-using-helm_2","title":"3. Run the upgrade using Helm.","text":"<p>Run the Helm upgrade command with your release name (<code>&lt;release-name&gt;</code>) and the target image from a previous step (<code>&lt;target-tag&gt;</code>). For more details, consult the Helm documentation.</p> <pre><code>helm upgrade &lt;release-name&gt;  atlassian-data-center/bitbucket \\\n    --wait \\\n    --reuse-values \\\n    --set image.tag=&lt;target-tag&gt;\n</code></pre> <p>If you used <code>kubectl scale</code> after installing the Helm chart, you'll need to add <code>--set replicaCount=&lt;number-of-bb-nodes&gt;</code> to the command. Otherwise, the deployment will be scaled back to the original number, which most likely is one node.</p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#4-wait-for-the-upgrade-to-finish_2","title":"4. Wait for the upgrade to finish.","text":"<p>The pods will be recreated with the updated version, one at a time.</p> <p></p>"},{"location":"userguide/upgrades/PRODUCTS_UPGRADE/#5-finalize-the-upgrade_2","title":"5. Finalize the upgrade.","text":"<p>After all the pods are active with the new version, finalize the upgrade:</p> <p></p>"}]}