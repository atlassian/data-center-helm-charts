---
# Source: crowd/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: unittest-crowd
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: crowd/templates/config-jvm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: unittest-crowd-jvm-config
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
data:
  additional_jvm_args: >-
    -Dcluster.node.name=${KUBE_POD_NAME}
    -Datlassian.logging.cloud.enabled=false
    -XX:ActiveProcessorCount=2
    -javaagent:/var/atlassian/application-data/crowd/shared/jmx_prometheus_javaagent.jar=9999:/opt/atlassian/jmx/jmx-config.yaml
  max_heap: 768m
  min_heap: 384m
---
# Source: crowd/templates/configmap-jmx-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: unittest-crowd-jmx-config
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
data:
  jmx-config.yaml: |
    rules:
      - pattern: '(java.lang)<type=(\w+)><>(\w+):'
        name: java_lang_$2_$3
        labels:
          product: "crowd"
      - pattern: 'java.lang<type=Memory><HeapMemoryUsage>(\w+)'
        name: java_lang_Memory_HeapMemoryUsage_$1
        labels:
          product: "crowd"
      - pattern: 'java.lang<name=G1 (\w+) Generation, type=GarbageCollector><>(\w+)'
        name: java_lang_G1_$1_Generation_$2
        labels:
          product: "crowd"
      - pattern: '.*'
---
# Source: crowd/templates/configmap-values-analytics.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: unittest-crowd-helm-values
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
data:
  values.yaml: |
    additionalConfigMaps: []
    additionalContainers: []
    additionalFiles: []
    additionalHosts:
    - hostnames:
      - foo.local
      - bar.local
      ip: 127.0.0.1
    additionalInitContainers: []
    additionalLabels: {}
    affinity: {}
    atlassianAnalyticsAndSupport:
      analytics:
        enabled: false
      helmValues:
        enabled: true
    common:
      global: {}
    crowd:
      accessLog:
        enabled: true
        localHomeSubPath: logs
        mountPath: /opt/atlassian/crowd/apache-tomcat/logs
      additionalAnnotations:
        argocd.argoproj.io/sync-wave: "10"
      additionalBundledPlugins: []
      additionalCertificates:
        customCmd: null
        initContainer:
          resources: {}
        secretList: []
        secretName: null
      additionalEnvironmentVariables: []
      additionalJvmArgs: []
      additionalLibraries: []
      additionalPorts: []
      additionalVolumeClaimTemplates: []
      additionalVolumeMounts: []
      containerSecurityContext: {}
      livenessProbe:
        customProbe: {}
        enabled: false
        failureThreshold: 12
        initialDelaySeconds: 60
        periodSeconds: 5
        timeoutSeconds: 1
      ports:
        http: 8095
      postStart:
        command: null
      readinessProbe:
        customProbe: {}
        enabled: true
        failureThreshold: 10
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 1
      resources:
        container:
          requests:
            cpu: "2"
            memory: 1G
        jvm:
          maxHeap: 768m
          minHeap: 384m
      securityContext:
        fsGroup: 2004
      securityContextEnabled: true
      service:
        annotations: {}
        contextPath: /crowd
        loadBalancerIP: null
        port: 80
        sessionAffinity: None
        sessionAffinityConfig:
          clientIP:
            timeoutSeconds: null
        type: ClusterIP
      setPermissions: true
      shutdown:
        command: /shutdown-wait.sh
        terminationGracePeriodSeconds: 30
      startupProbe:
        enabled: false
        failureThreshold: 120
        initialDelaySeconds: 60
        periodSeconds: 5
      tomcatConfig:
        acceptCount: "100"
        accessLogsMaxDays: null
        connectionTimeout: "20000"
        enableLookups: "false"
        generateByHelm: false
        maxHttpHeaderSize: "8192"
        maxThreads: "150"
        mgmtPort: "8020"
        minSpareThreads: "25"
        port: "8095"
        protocol: HTTP/1.1
        proxyInternalIps: null
        proxyName: null
        proxyPort: null
        redirectPort: "8443"
        scheme: null
        secure: null
      topologySpreadConstraints: []
      umask: "0022"
      useHelmReleaseNameAsContainerName: false
    fluentd:
      command: null
      customConfigFile: false
      elasticsearch:
        enabled: true
        hostname: elasticsearch
        indexNamePrefix: crowd
      enabled: false
      extraVolumes: []
      fluentdCustomConfig: {}
      httpPort: 9880
      imageRepo: fluent/fluentd-kubernetes-daemonset
      imageTag: v1.11.5-debian-elasticsearch7-1.2
      resources: {}
    image:
      pullPolicy: IfNotPresent
      repository: atlassian/crowd
      tag: ""
    ingress:
      annotations: {}
      className: nginx
      create: false
      host: null
      https: true
      maxBodySize: 250m
      nginx: true
      openShiftRoute: false
      path: /
      proxyConnectTimeout: 60
      proxyReadTimeout: 60
      proxySendTimeout: 60
      routeHttpHeaders: {}
      tlsSecretName: null
    monitoring:
      exposeJmxMetrics: true
      fetchJmxExporterJar: true
      grafana:
        createDashboards: false
        dashboardAnnotations: {}
        dashboardLabels: {}
      jmxExporterCustomConfig: {}
      jmxExporterCustomJarLocation: null
      jmxExporterImageRepo: bitnami/jmx-exporter
      jmxExporterImageTag: 0.18.0
      jmxExporterInitContainer:
        customSecurityContext: {}
        jmxJarLocation: null
        resources: {}
        runAsRoot: true
      jmxExporterPort: 9999
      jmxExporterPortType: ClusterIP
      jmxServiceAnnotations: {}
      serviceMonitor:
        create: false
        prometheusLabelSelector: {}
        scrapeIntervalSeconds: 30
    nodeSelector: {}
    openshift:
      runWithRestrictedSCC: false
    ordinals:
      enabled: false
      start: 0
    podAnnotations: {}
    podDisruptionBudget:
      annotations: {}
      enabled: false
      labels: {}
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: high
    replicaCount: 1
    serviceAccount:
      annotations: {}
      create: true
      imagePullSecrets: []
      name: null
    terminationGracePeriodSeconds: 30
    testPods:
      affinity: {}
      annotations: {}
      image:
        permissionsTestContainer: debian:stable-slim
        statusTestContainer: alpine:latest
      labels: {}
      nodeSelector: {}
      resources: {}
      schedulerName: null
      tolerations: []
    tolerations: []
    updateStrategy: {}
    volumes:
      additional: []
      localHome:
        customVolume: {}
        mountPath: /var/atlassian/application-data/crowd
        persistentVolumeClaim:
          create: false
          resources:
            requests:
              storage: 1Gi
          storageClassName: null
        persistentVolumeClaimRetentionPolicy:
          whenDeleted: null
          whenScaled: null
      sharedHome:
        customVolume: {}
        mountPath: /var/atlassian/application-data/crowd/shared
        nfsPermissionFixer:
          command: null
          enabled: true
          imageRepo: alpine
          imageTag: latest
          mountPath: /shared-home
          resources: {}
        persistentVolumeClaim:
          accessModes:
          - ReadWriteMany
          create: false
          resources:
            requests:
              storage: 1Gi
          storageClassName: null
        subPath: null
---
# Source: crowd/templates/service-jmx.yaml
apiVersion: v1
kind: Service
metadata:
  name: unittest-crowd-jmx
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 9999
      targetPort: jmx
      appProtocol: http
      name: jmx
  selector:
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
---
# Source: crowd/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: unittest-crowd
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
---
# Source: crowd/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: unittest-crowd
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    argocd.argoproj.io/sync-wave: "10"
spec:
  replicas: 1
  serviceName: unittest-crowd
  selector:
    matchLabels:
      app.kubernetes.io/name: crowd
      app.kubernetes.io/instance: unittest-crowd
  template:
    metadata:
      annotations:
        checksum/config-jvm: fdf627893296baca9c1f8932080860cf429a4b34682274892d2cee0e00396fe4
      labels:
        app.kubernetes.io/name: crowd
        app.kubernetes.io/instance: unittest-crowd
    spec:
      serviceAccountName: unittest-crowd
      terminationGracePeriodSeconds: 30
      hostAliases:
        - hostnames:
          - foo.local
          - bar.local
          ip: 127.0.0.1
      securityContext:
        fsGroup: 2004
      initContainers:
        - name: nfs-permission-fixer
          image: alpine:latest
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0 # make sure we run as root so we get the ability to change the volume permissions
          volumeMounts:
            - name: shared-home
              mountPath: "/shared-home"
          command: ["sh", "-c", "(chgrp 2004 /shared-home; chmod g+w /shared-home)"]
        - name: fetch-jmx-exporter
          image: bitnami/jmx-exporter:0.18.0
          command: ["cp"]
          args: ["/opt/bitnami/jmx-exporter/jmx_prometheus_javaagent.jar", "/var/atlassian/application-data/crowd/shared"]
          securityContext:
            runAsUser: 0
          volumeMounts:
            - mountPath: "/var/atlassian/application-data/crowd/shared"
              name: shared-home
      containers:
        - name: crowd
          image: "atlassian/crowd:6.1.1"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8095
              protocol: TCP
            - name: jmx
              containerPort: 9999
              protocol: TCP
          readinessProbe:
            httpGet:
              port: 8095
              path: /crowd/status
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 1
            failureThreshold: 10
          resources:
            requests:
              cpu: "2"
              memory: 1G
          volumeMounts:
            - name: local-home
              mountPath: "/var/atlassian/application-data/crowd"
            - name: local-home
              mountPath: "/opt/atlassian/crowd/apache-tomcat/logs"
              subPath: "logs"
            - name: shared-home
              mountPath: "/var/atlassian/application-data/crowd/shared"
            - name: helm-values
              mountPath: /opt/atlassian/helm
            - name: jmx-config
              mountPath: /opt/atlassian/jmx
          env:
            - name: KUBE_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ATL_TOMCAT_SCHEME
              value: "https"
            - name: ATL_TOMCAT_SECURE
              value: "true"
            - name: ATL_TOMCAT_CONTEXTPATH
              value: "/crowd"
            - name: ATL_TOMCAT_PORT
              value: "8095"
            - name: ATL_TOMCAT_ACCESS_LOG
              value: "true"
            - name: UMASK
              value: "0022"
            - name: SET_PERMISSIONS
              value: "true"
            - name: ATL_PRODUCT_HOME_SHARED
              value: "/var/atlassian/application-data/crowd/shared"
            - name: JVM_SUPPORT_RECOMMENDED_ARGS
              valueFrom:
                configMapKeyRef:
                  key: additional_jvm_args
                  name: unittest-crowd-jvm-config
            - name: JVM_MINIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: min_heap
                  name: unittest-crowd-jvm-config
            - name: JVM_MAXIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: max_heap
                  name: unittest-crowd-jvm-config
          lifecycle:
            preStop:
              exec:
                command: ["sh", "-c", "/shutdown-wait.sh"]
      priorityClassName: high
      volumes:
        - name: local-home
          emptyDir: {}
        - name: shared-home
          emptyDir: {}
        - name: helm-values
          configMap:
            name: unittest-crowd-helm-values
        - name: jmx-config
          configMap:
            name: unittest-crowd-jmx-config
---
# Source: crowd/templates/tests/test-application-status.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "unittest-crowd-application-status-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
spec:
  containers:
    - name: test
      image: alpine:latest
      env:
        - name: STATUS_URL
          value: "http://unittest-crowd:80/crowd/status"
      command:
        - /bin/sh
        - -ec
        - |
          apk add -q jq curl
          STATUS=$(curl -s "$STATUS_URL")
          echo "Verifying application state is RUNNING or FIRST_RUN: $STATUS"
          echo $STATUS | jq -e '.state|test("RUNNING|FIRST_RUN")'
  restartPolicy: Never
---
# Source: crowd/templates/tests/test-shared-home-permissions.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "unittest-crowd-shared-home-permissions-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: crowd-1.21.4
    app.kubernetes.io/name: crowd
    app.kubernetes.io/instance: unittest-crowd
    app.kubernetes.io/version: "6.1.1"
    app.kubernetes.io/managed-by: Helm
spec:
  containers:
    - name: test
      image: debian:stable-slim
      imagePullPolicy: IfNotPresent
      securityContext:
        # We assume that the UID and GID used by the product images are the same, which in practice they are
        runAsUser: 2004
        runAsGroup: 2004
      volumeMounts:
        - name: shared-home
          mountPath: /shared-home
      command:
        - /bin/sh
        - -ec
        - |
          ls -ld /shared-home
          echo "Creating temporary file in shared home as user $(id -u):$(id -g)"
          touch /shared-home/permissions-test
          ls -l /shared-home/permissions-test
          rm /shared-home/permissions-test
  volumes:
    - name: shared-home
      emptyDir: {}
  restartPolicy: Never
