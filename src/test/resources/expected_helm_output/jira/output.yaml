---
# Source: jira/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: unittest-jira
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
---
# Source: jira/templates/config-jvm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: unittest-jira-jvm-config
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
data:
  additional_jvm_args: >-
    -Datlassian.logging.cloud.enabled=false
    -XX:ActiveProcessorCount=2
    -javaagent:/var/atlassian/application-data/shared-home/jmx_prometheus_javaagent.jar=9999:/opt/atlassian/jmx/jmx-config.yaml
  max_heap: 768m
  min_heap: 384m
  reserved_code_cache: 512m
---
# Source: jira/templates/configmap-jmx-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: unittest-jira-jmx-config
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
data:
  jmx-config.yaml: |
    ---
    startDelaySeconds: 0
    rules:
      - pattern: '(java.lang)<type=(\w+)><>(\w+):'
        name: java_lang_$2_$3
      - pattern: 'java.lang<type=Memory><HeapMemoryUsage>(\w+)'
        name: java_lang_Memory_HeapMemoryUsage_$1
      - pattern: 'java.lang<name=G1 (\w+) Generation, type=GarbageCollector><>(\w+)'
        name: java_lang_G1_$1_Generation_$2
      - pattern: '.*'
    blacklistObjectNames:
      - 'com.atlassian.jira:connectionpool=connections,connection=*,name=BasicDataSource'
---
# Source: jira/templates/configmap-values-analytics.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: unittest-jira-helm-values
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
data:
  values.yaml: |
    additionalConfigMaps: []
    additionalContainers: []
    additionalFiles: []
    additionalHosts:
    - hostnames:
      - foo.local
      - bar.local
      ip: 127.0.0.1
    additionalInitContainers: []
    additionalLabels: {}
    affinity: {}
    atlassianAnalyticsAndSupport:
      analytics:
        enabled: false
      helmValues:
        enabled: true
    common:
      global: {}
    database:
      credentials:
        passwordSecretKey: password
        secretName: null
        usernameSecretKey: username
      driver: null
      type: null
      url: null
    fluentd:
      command: null
      customConfigFile: false
      elasticsearch:
        enabled: true
        hostname: elasticsearch
        indexNamePrefix: jira
      enabled: false
      extraVolumes: []
      fluentdCustomConfig: {}
      httpPort: 9880
      imageRepo: fluent/fluentd-kubernetes-daemonset
      imageTag: v1.11.5-debian-elasticsearch7-1.2
      resources: {}
    image:
      pullPolicy: IfNotPresent
      repository: atlassian/jira-software
      tag: ""
    ingress:
      annotations: {}
      className: nginx
      create: false
      host: null
      https: true
      maxBodySize: 250m
      nginx: true
      openShiftRoute: false
      path: null
      proxyConnectTimeout: 60
      proxyReadTimeout: 60
      proxySendTimeout: 60
      routeHttpHeaders: {}
      tlsSecretName: null
    jira:
      accessLog:
        localHomeSubPath: log
        mountPath: /opt/atlassian/jira/logs
      additionalBundledPlugins: []
      additionalCertificates:
        customCmd: null
        secretName: null
      additionalEnvironmentVariables: []
      additionalJvmArgs: []
      additionalLibraries: []
      additionalPorts: []
      additionalVolumeClaimTemplates: []
      additionalVolumeMounts: []
      clustering:
        enabled: false
      containerSecurityContext: {}
      forceConfigUpdate: false
      livenessProbe:
        customProbe: {}
        enabled: false
        failureThreshold: 12
        initialDelaySeconds: 60
        periodSeconds: 5
        timeoutSeconds: 1
      ports:
        ehcache: 40001
        ehcacheobject: 40011
        http: 8080
      postStart:
        command: null
      readinessProbe:
        customProbe: {}
        enabled: true
        failureThreshold: 10
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 1
      resources:
        container:
          requests:
            cpu: "2"
            memory: 2G
        jvm:
          maxHeap: 768m
          minHeap: 384m
          reservedCodeCache: 512m
      s3Storage:
        avatars:
          bucketName: null
          bucketRegion: null
          endpointOverride: null
      securityContext:
        fsGroup: 2001
      securityContextEnabled: true
      seraphConfig:
        autoLoginCookieAge: "1209600"
        generateByHelm: false
      service:
        annotations: {}
        contextPath: null
        loadBalancerIP: null
        port: 80
        sessionAffinity: None
        sessionAffinityConfig:
          clientIP:
            timeoutSeconds: null
        type: ClusterIP
      setPermissions: true
      shutdown:
        command: /shutdown-wait.sh
        terminationGracePeriodSeconds: 30
      startupProbe:
        enabled: false
        failureThreshold: 120
        initialDelaySeconds: 60
        periodSeconds: 5
      tomcatConfig:
        acceptCount: "10"
        connectionTimeout: "20000"
        customServerXml: ""
        enableLookups: "false"
        generateByHelm: false
        maxHttpHeaderSize: "8192"
        maxThreads: "100"
        mgmtPort: "8005"
        minSpareThreads: "10"
        port: "8080"
        protocol: HTTP/1.1
        proxyName: null
        proxyPort: null
        redirectPort: "8443"
        scheme: null
        secure: null
      topologySpreadConstraints: []
      useHelmReleaseNameAsContainerName: false
    monitoring:
      exposeJmxMetrics: true
      fetchJmxExporterJar: true
      grafana:
        createDashboards: false
        dashboardAnnotations: {}
        dashboardLabels: {}
      jmxExporterCustomConfig: {}
      jmxExporterCustomJarLocation: null
      jmxExporterImageRepo: bitnami/jmx-exporter
      jmxExporterImageTag: 0.18.0
      jmxExporterInitContainer:
        customSecurityContext: {}
        resources: {}
        runAsRoot: true
      jmxExporterPort: 9999
      jmxExporterPortType: ClusterIP
      jmxServiceAnnotations: {}
      serviceMonitor:
        create: false
        prometheusLabelSelector: {}
        scrapeIntervalSeconds: 30
    nodeSelector: {}
    openshift:
      runWithRestrictedSCC: false
    ordinals:
      enabled: false
      start: 0
    podAnnotations: {}
    podDisruptionBudget:
      annotations: {}
      enabled: false
      labels: {}
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: high
    replicaCount: 1
    serviceAccount:
      annotations: {}
      create: true
      eksIrsa:
        roleArn: null
      imagePullSecrets: []
      name: null
    testPods:
      affinity: {}
      annotations: {}
      image:
        permissionsTestContainer: debian:stable-slim
        statusTestContainer: alpine:latest
      labels: {}
      nodeSelector: {}
      resources: {}
      schedulerName: null
      tolerations: []
    tolerations: []
    volumes:
      additional: []
      localHome:
        customVolume: {}
        mountPath: /var/atlassian/application-data/jira
        persistentVolumeClaim:
          create: false
          resources:
            requests:
              storage: 1Gi
          storageClassName: null
        persistentVolumeClaimRetentionPolicy:
          whenDeleted: null
          whenScaled: null
      sharedHome:
        customVolume: {}
        mountPath: /var/atlassian/application-data/shared-home
        nfsPermissionFixer:
          command: null
          enabled: true
          imageRepo: alpine
          imageTag: latest
          mountPath: /shared-home
          resources: {}
        persistentVolumeClaim:
          create: false
          resources:
            requests:
              storage: 1Gi
          storageClassName: null
        subPath: null
---
# Source: jira/templates/service-jmx.yaml
apiVersion: v1
kind: Service
metadata:
  name: unittest-jira-jmx
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  ports:
    - port: 9999
      targetPort: jmx
      name: jmx
  selector:
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
---
# Source: jira/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: unittest-jira
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
  annotations:
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
---
# Source: jira/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: unittest-jira
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  serviceName: unittest-jira
  selector:
    matchLabels:
      app.kubernetes.io/name: jira
      app.kubernetes.io/instance: unittest-jira
  template:
    metadata:
      annotations:
        checksum/config-jvm: f83ece737a22794daa73d86dace03d0e3f9a70fd04ae4df991f7b6d425442781
      labels:
        app.kubernetes.io/name: jira
        app.kubernetes.io/instance: unittest-jira
    spec:
      serviceAccountName: unittest-jira
      terminationGracePeriodSeconds: 30
      hostAliases:
        - hostnames:
          - foo.local
          - bar.local
          ip: 127.0.0.1
      securityContext:
        fsGroup: 2001
      initContainers:
        - name: nfs-permission-fixer
          image: alpine:latest
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0 # make sure we run as root so we get the ability to change the volume permissions
          volumeMounts:
            - name: shared-home
              mountPath: "/shared-home"
          command: ["sh", "-c", "(chgrp 2001 /shared-home; chmod g+w /shared-home)"]
        - name: fetch-jmx-exporter
          image: bitnami/jmx-exporter:0.18.0
          command: ["cp"]
          args: ["/opt/bitnami/jmx-exporter/jmx_prometheus_javaagent.jar", "/var/atlassian/application-data/shared-home"]
          securityContext:
            runAsUser: 0
          volumeMounts:
            - mountPath: "/var/atlassian/application-data/shared-home"
              name: shared-home
      containers:
        - name: jira
          image: "atlassian/jira-software:9.12.7"
          imagePullPolicy: IfNotPresent
          env:
            - name: ATL_TOMCAT_SCHEME
              value: "https"
            - name: ATL_TOMCAT_SECURE
              value: "true"
            - name: ATL_TOMCAT_PORT
              value: "8080"
            - name: SET_PERMISSIONS
              value: "true"
            - name: JIRA_SHARED_HOME
              value: "/var/atlassian/application-data/shared-home"
            - name: JVM_SUPPORT_RECOMMENDED_ARGS
              valueFrom:
                configMapKeyRef:
                  key: additional_jvm_args
                  name: unittest-jira-jvm-config
            - name: JVM_MINIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: min_heap
                  name: unittest-jira-jvm-config
            - name: JVM_MAXIMUM_MEMORY
              valueFrom:
                configMapKeyRef:
                  key: max_heap
                  name: unittest-jira-jvm-config
            - name: JVM_RESERVED_CODE_CACHE_SIZE
              valueFrom:
                configMapKeyRef:
                  key: reserved_code_cache
                  name: unittest-jira-jvm-config
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: ehcache
              containerPort: 40001
              protocol: TCP
            - name: ehcacheobject
              containerPort: 40011
              protocol: TCP
            - name: jmx
              containerPort: 9999
              protocol: TCP
          readinessProbe:
            httpGet:
              port: 8080
              path: /status
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 1
            failureThreshold: 10
          resources:
            requests:
              cpu: "2"
              memory: 2G
          volumeMounts:
            - name: local-home
              mountPath: "/var/atlassian/application-data/jira"
            - name: local-home
              mountPath: "/opt/atlassian/jira/logs"
              subPath: "log"
            - name: shared-home
              mountPath: "/var/atlassian/application-data/shared-home"
            - name: helm-values
              mountPath: /opt/atlassian/helm
            - name: jmx-config
              mountPath: /opt/atlassian/jmx
          lifecycle:
            preStop:
              exec:
                command: ["sh", "-c", "/shutdown-wait.sh"]
      priorityClassName: high
      volumes:
        - name: local-home
          emptyDir: {}
        - name: shared-home
          emptyDir: {}
        - name: helm-values
          configMap:
            name: unittest-jira-helm-values
        - name: jmx-config
          configMap:
            name: unittest-jira-jmx-config
---
# Source: jira/templates/tests/test-application-status.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "unittest-jira-application-status-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
spec:
  containers:
    - name: test
      image: alpine:latest
      imagePullPolicy: IfNotPresent
      env:
        - name: STATUS_URL
          value: "http://unittest-jira:80/status"
      command:
        - /bin/sh
        - -ec
        - |
          apk add -q jq curl
          STATUS=$(curl -s "$STATUS_URL")
          echo "Verifying application state is RUNNING or FIRST_RUN: $STATUS"
          echo $STATUS | jq -e '.state|test("RUNNING|FIRST_RUN")'
  restartPolicy: Never
---
# Source: jira/templates/tests/test-shared-home-permissions.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "unittest-jira-shared-home-permissions-test"
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: jira-1.18.1
    app.kubernetes.io/name: jira
    app.kubernetes.io/instance: unittest-jira
    app.kubernetes.io/version: "9.12.7"
    app.kubernetes.io/managed-by: Helm
spec:
  containers:
    - name: test
      image: debian:stable-slim
      imagePullPolicy: IfNotPresent
      securityContext:
        # We assume that the UID and GID used by the product images are the same, which in practice they are
        runAsUser: 2001
        runAsGroup: 2001
      volumeMounts:
        - name: shared-home
          mountPath: /shared-home
      command:
        - /bin/sh
        - -ec
        - |
          ls -ld /shared-home
          echo "Creating temporary file in shared home as user $(id -u):$(id -g)"
          touch /shared-home/permissions-test
          ls -l /shared-home/permissions-test
          rm /shared-home/permissions-test
  volumes:
    - name: shared-home
      emptyDir: {}
  restartPolicy: Never
