### TEST TEMPLATE ONLY (works on AWS only)
###
### This template serves as an example how to deploy a very simple NFS server providing persistent volumes
### in read write many mode (as all the application nodes require to access the shared home from one persistent volume).
### More visual representation can be found in official kubernetes repository:
### https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs

### For NFS server you should be using fast underlying storage as it will directly impact performance for your Bitbucket
### instance. Please see documentation specific for your kubernetes cluster infrastructure.
#---
#
#kind: StorageClass
#apiVersion: storage.k8s.io/v1
#metadata:
#  name: io1-fast
#  annotations:
#    storageclass.kubernetes.io/is-default-class: "false"
#  labels:
#    build.identifier: CI_PLAN_ID
#    build.component: nfs-server-storage-class
#provisioner: kubernetes.io/aws-ebs
#parameters:
#  type: io1
#  iopsPerGB: "10"
#  encrypted: "true"
#  fsType: ext4


### PersistentVolumeClaim will dynamically create the persistent volume based on the defined storageClassName.
### This will spare you the manual step when you need to create the underlying volume manually and then
### define PersistentVolume for it. You might still prefer that approach as it will give you more control over
### your data
---

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: CI_PLAN_ID-data-pvc
  labels:
    build.identifier: CI_PLAN_ID
    build.component: nfs-server-pvc
spec:
  accessModes:
    - ReadWriteOnce
#  storageClassName: io1-fast # if you want more performant storage, also you need to uncomment StorageClass definition
  resources:
    requests:
      storage: 1Gi # make sure you have enough space for your shared home

---

kind: Service
apiVersion: v1
metadata:
  name: CI_PLAN_ID
  labels:
    build.identifier: CI_PLAN_ID
    build.component: nfs-server-service
spec:
  type: ClusterIP
  ports:
    - name: nfs
      port: 2049
    - name: mountd
      port: 20048
    - name: rpcbind
      port: 111
  selector:
    role: CI_PLAN_ID

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: CI_PLAN_ID
  labels:
    build.identifier: CI_PLAN_ID
    build.component: nfs-server-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      role: CI_PLAN_ID
  template:
    metadata:
      labels:
        role: CI_PLAN_ID
    spec:
      containers:
        - name: CI_PLAN_ID
          image: atlassian/nfs-server-test:latest # this is just a testing image not recommended for any other use case
          imagePullPolicy: Always
          ports:
            - name: nfs
              containerPort: 2049
            - name: mountd
              containerPort: 20048
            - name: rpcbind
              containerPort: 111
          securityContext:
            capabilities:
              add:
                - SYS_ADMIN
                - SYS_MODULE
          volumeMounts:
            - name: CI_PLAN_ID-data-pvc
              mountPath: /srv/nfs
            - name: CI_PLAN_ID-conf
              mountPath: /etc/exports.d/
          resources:
            requests:
              cpu: 1000m
              memory: 512Mi
      terminationGracePeriodSeconds: 60
      volumes:
        - name: CI_PLAN_ID-data-pvc
          persistentVolumeClaim:
            claimName: CI_PLAN_ID-data-pvc
        - name: CI_PLAN_ID-conf
          configMap:
            name: CI_PLAN_ID-conf

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: CI_PLAN_ID-conf
  labels:
    build.identifier: CI_PLAN_ID
    build.component: nfs-server-configmap
data:
  share.exports: |-
    /srv/nfs *(rw,subtree_check,fsid=0,no_root_squash)